{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOIuYDXtIrN69HfVcX2bMGb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 13-1. 들어가며\n","\n","1. 변화의 흐름 - Word Embedding과 Context\n","- 우리는 NLP task를 풀기 위하여 인간의 언어를 컴퓨터의 언어로 바꾸어 주는 작업을 반드시 거쳐야만 합니다(임베딩). 단어가 가지고 있는 의미를 숫자에 녹여내기 위해서 tf-idf, word2vec, fasttext 등 다양한 임베딩 방법들을 공부해 보기도 했지요. 우리가 기존에 배웠던 임베딩 방식은 워드임베딩(Word Embedding)이라고 합니다. 즉, 단어 하나하나를 임베딩하는 방식이지요\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/original_images/01_qSVU9Sq.png)\n","<center>학습이 끝난 word embedding으로 단어들 간의 거리 재보기</center>\n","\n","- 워드 임베딩은 학습이 완료되면 각 단어 하나하나가 특정 임베딩(벡터)을 가지게 됩니다. 그래서 우리는 위와 같이 단어 사이의 거리를 재고, 관계를 파악할 수 있었던 것이죠.\n","\n","혹시...\n","\n","생각해 본 적 있나요?\n","생긴 것은 똑같이 생겼는데 다른 뜻을 가지고 있는 단어들(동음이의어, 다의어 등)은 어떻게 표현될지?\n","\n","없어도 괜찮아요. 이제 생각해 보면 되죠.\n","예시를 보면서 생각해 봅시다.\n","\n","예시 1) bank\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/images/03_MdCqjaS.max-800x600.png)\n","<center></center>\n","\n","예시 2) 차\n","\n","1. 시간이 늦었으니 커피 말고 차나 한 잔 마시자\n","\n","2. 제주도는 버스로 움직이기 힘들어. 차가 있어야 해.\n","\n","3. 축구공을 이쪽으로 차.\n","\n","이제 어떤 문제가 발생하는지 감이 오시나요?\n","\n","워드 임베딩 방법들은, 주변 단어를 학습할 때만 고려하게 됩니다. 다른 모델의 입력으로써 사용할 때(학습된 벡터들을 이용하여 input 단어들을 벡터화시킬 때)는 동음이의어, 다의어와 같은 경우들을 모두 고정된 하나의 벡터로 표현하게 되면서 주변의 단어들을 고려하지 않게 되는 것이죠. 정작 학습할 때는 문맥을 열심히 보더니, 테스트할 때는 현재 단어 한 개만 보는 모양새가 되어버렸습니다.\n","\n","똑똑한 여러분은 '문맥을 고려하는 임베딩(Contextual Embedding)을 만들면 되지 않나?!'라고 생각하실 겁니다.\n","\n","네! 맞습니다! 바로 앞으로 등장하는 모델들은 이러한 문맥(context) 을 잘 반영하는 모델들입니다."],"metadata":{"id":"9sgvThfTZeAZ"}},{"cell_type":"markdown","source":["# 13-2. Transfer Learning과 Language Modeling\n","\n","1. Transfer Learning(전이 학습)\n","- pretrian과 fine-tuning으로 들어본 말 일 것이다. 전이 학습은 특정 환경에서 학습을 마친 신경망(일부 혹은 전부)을 유사하거나 다른 환경에서 사용하는 것을 의미합니다. 이처럼 전이 학습을 이용하게 되면 전윽 데이터로도 성능을 더 좋게 만들어 낼 수 있다는 이점이 있다.\n","\n","2. Language Modeling(언어 모델)\n","- nlp에서 전이 핛브은 보통 언어 모델과 관련이 깊다. 언어 모델은 입력으로 주어진 시퀀스의 다음을 학습하는 과정에서 주어진 시퀀스가 얼마나 자연스러운지를 학습하게 됩니다. 즉 언어 모델은 철수가 밥을 마셨다 인지 철수가 밥을 먹었다 인지를 데이터로부터 학습을 하게 됩니다. 이렇게 학습을 완료한 언어 모델을 언어의 패턴과 규칙을 학습하여 전반적인 언어의 특징을 익히게 됩니다.\n","\n","- nlp에서 바오 이 언어 모델이 사전학습된 모델이 되는 것입니다. 이미 언어의 전반적인 것을 아는 신경망에서 언어와 관련된 문제를 풀게 하는 것이다. 이처럼 주어진 문제(다운스트림 테스크)를 잘 풀기 위해 사전학습된 모델을 재학습시키는 것을 fine-tuning이라고 부른다.\n","\n","3. Transgormer\n","- 최신 NLP의 가장 큰 흐름은 트랜스포머이다. GPT, BERT 등 이후에 만들어지는 모델들은 트랜스포머 기반이기 때문이다. 또한 BERT를 개선하는 수많은 모델이 나오기 시작했다. 따라서 modern NLP라고 하면 트랜스포머를 빼놓을 수 없다.\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/original_images/04_XApPtfX.png)\n","<center>트랜스포머 구조</center>\n","\n","- 트랜스포머는 Encoder-Decoder 구조로, RNN이나 LSTM 등을 사용하지 않고 attention만을 이용한 모델이다. 그래서 비록 LSTM 등 RNN 구조를 사용하지 않지만 번역기 모델 등에 사용하는 Seq2Seq 모델과 구조적으로 동일하다. Encoder-Decoder 모델이라면 입력부터 출력까지의 파이프라인이 이미 고정되어 있어서 이를 이용해 임베딩을 구하거나 전이 학습을 통해 다른 태스크에 활용하기 어려울 것 같다. 그렇다면 transformer 모델이 어떻게 modern NLP의 토대를 이루는 중요한 모델로 발전할 수 있었나?"],"metadata":{"id":"wTrE65GJdMqF"}},{"cell_type":"markdown","source":["# 13-3. ELMO(Embedding from Language Models)\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/images/05_xDakup4.max-800x600.png)\n","<center></center>\n","\n","ELMo는 문맥을 반영한 임베딩을 사전학습한 모델로 구현한 첫 번째 사례이다. 언어 모델을 이용하려 임베딩을 한 것이다. 이름에 포함된 LM이 바로 이 언어 모델을 가리킨다.\n","\n","1. ELMo의 구조\n","- ELMo는 세 가지 요소로 구성되어 있다. 첫 번째는 character-level CNN, 두 번째는 bidirectional LSTM, 마지막으로 ELMo 레이어가 있습니다.\n","\n","(1) character-level CNN\n","- character-level CNN은 입력된 문자들 간의 관계를 파악하고 임베딩 벡터로 변환하는 역할을 합니다. \n","\n","- ELMo는 character-level로 문자를 인식합니다. 좀 더 구체적으로 말한다면, 해당 character의 유니코드 ID를 입력으로 받습니다. 예를 들어, 밥 이라는 단어를 입력으로 받으면 ㅂ ㅏ ㅂ에 해당되는 유니코드 235, 176, 165 세 개의 숫자가 됩니다.\n","\n","- 이렇게 각각 입력받은 단어의 시작과 끝에 해당하는 스페셜 토큰 <BOW>와 <EOW>에 해당하는 유니코드를 앞뒤로 붙여줍니다. 이후 각 유니코드 아이디에 해당하는 행 벡터를 참조하여 붙입니다.(look-up table)\n","\n","- 만들어진 벡터애 (n * 임베딩 차원 수) 사이즈의 필터로 컨볼루션하여 피터맵을 만들고 max-polling하여 하나의 값을 뽑아냅니다. 이러한 작업을 반복하여 사용자가 원하는 크기만큼의 벡터로 만들어냅니다.\n","\n","- 여기서 n은 한 번에 몇 개의 문자들을 함께 볼 것 인가를 의미합니다. n이 2라면 <BOW> ㅂ, ㅂ ㅏ, ㅏ ㅂ, ㅂ <EOW> 이렇게 문자 2개씩 보면서 2개 사이의 관계를 파악하는 CNN이 됩니다.\n","\n","- ELMo의 original 코드에서는 각기 다른 사이즈를 가진 7개의 필터를 이용하여 2048차원의 벡터를 만든다고 합니다.\n","\n","2. bidirectional LSTM\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/original_images/06_EXF2a0T.png)\n","<center>ELMo의 구조</center>\n","\n","- character-level CNN을 통과하여 만들어진 벡터(E_1, E_2, ..., E_NE)들은 bidirectional LSTM을 통과하게 됩니다. 사전학습시에 bidirectional LSTM은 주어진 입력에 다음에 올 단어들을 예측합니다. (bidirectional LM아라고도 불린다.)\n","\n","- bidirectional이라는 것은 양방향으로 학습하는 것을 의미한다. 주어진 입력을 한 번은 순방향으로, 한 번은 역방향으로 각각 2개의 LSTM layer를 통과하게 됩니다. 이렇게 양방향으로 학습하는 것은 모델의 사이즈와 학습에 걸리는 시간이 늘어날 수는 있지만 그만큼 성능이 좋다.\n","\n","- 사전 학습시 순방향과 역방향으로 LSTM을 통과한 히든 벡터들은 이후 소프트맥스를 위해 다음 단어를 예측하게 됩니다. 이때 ELMo는 순방향과 역방향의 벡터를 합치거나 더하지 않습니다. 각각의 독립적인 모델처럼 행동하게 됩니다. 이는 조금만 생각하면 이유를 알 수 있다. 하나는 순방향으로, 하나는 역방향으로 진행하니 cheating(다른 방향의 모델에게 정답을 가르쳐 줌)의 가능성이 있기 때문이죠.\n","\n","- 이렇게 ELMo는 단어를 하나씩 하나씩 슬라이딩하여 다음 단어를 예측하면서 문장 내의 단어와 단어들 사이의 관계를 학습하게 됩니다.\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/images/07_vLCaNHo.max-800x600.png)\n","<center>pretrain을 끝마친 bidirectional LSTM</center>\n","\n","3. ELMO 임베딩 레이어\n","- ELMo 임베딩은 pretrain이 끝나고 finetuning을 하는 과정에서 만들어집니다.\n","\n","- 맨 위 그림에서 Elmo 인형과의 대화처럼 stick이란 단어의 임베딩을 구한다고 가정해 봅시다.\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/images/08_X8OuDsj.max-800x600.png)\n","<center>ELMo embedding을 구하는 과정</center>\n","\n","- 우선 구하려고 하는 토큰에 대한 각 층의 출력값을 모두 가지고 옵니다. 여기서의 각 층의 출력값이라고 하면, 임베딩 벡터(character-level CNN을 통과한 후 나오는 벡터), 각각의 LSTM layer에서의 hidden vector를 의미합니다.\n","\n","- 각 층 1, 2, ..., l, ...1,2,...,l,...마다 가중치 s_1, s_2, ... s_l, ..,를 곱해서 모두 더해줍니다.(weighted sum 혹은 가중합을 해준다고 말할 수 있습니다.)\n","\n","- 마지막으로 다운스트림 태스크의 가중치 Y를 곱하면 ELMo의 임베딩이 됩니다. 여기서 갑자기 등장하는 가중치 s1와 Y는 다운스트임의 태스크별로 fine-tuning 시 학습 되는 값들입니다. 복잡해 보이나, 구하고자 하는 토큰에 대한 각 층의 출력값을 가중합한 것이 ELMo 임베딩입니다.\n","\n","4. ELMo의 이용\n","- 이렇게 구한 ELMo는 임베딩에 이용할 수 있지만, 기존에 학습시켰던 워드임베딩과 같이 사용할 수도 있습니다.\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/original_images/09_gHtb9F2.png)\n","<center>ELMo 의 활용</center>\n","\n","자세한 내용은 아래 논문 참고\n","\n","[Deep contextualized word representations](https://arxiv.org/abs/1802.05365)\n"],"metadata":{"id":"xfaIIn2SfQ8Y"}},{"cell_type":"markdown","source":["# 13-4. GPT(Generative Pre-Training Transformer)\n","\n","GPT는 트랜스포머의 decoder 구조만을 이용하여 만든 네트워크입니다. 트랜스포머의 decoder를 아주 깊고 깊게 쌓아 많은 데이터를 학습 시켜 성능을 높힌 네트워크이다.\n","\n","1. GPT의 구조\n","- Decoder만을 이용한다는 것이 잘 이해가 되지 않을겁니다. 이해를 위하여 Transformer-Decoder 구조를 힌번 더 확인할 필요가있다.\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/original_images/10_egROfLK.png)\n","<center></center>\n","\n","- Decoder는 masked Multi-Head Attention, Multi-Head Attention, Feed Forward Neural Network로 이루어져 있었습니다. 바로 이 구조를 차용하여 Decoder block를 많이 쌓아 올리면 GPT가 되는 것이죠. 이제 GPT를 두 부분으로 나눠서 자세히 보겠습니다.\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/original_images/11_HNGuRhf.png)\n","<center>GPT의 모델 구조</center>\n","\n","Transfomer Decoder Block : Pretraining LM (Unspervised Learning)\n","\n","첫 번째로 볼 부분은 빨간색 박스 안입니다. 사전 학습하는 부분으로 비지도 학습을 하게 됩니다. 위에 트랜스포머의 구조 그대로다.\n","\n","1. Embedding\n","- GPT는 텍스트의 Embedding으로 BPE(Byte-pair Encoding)를 사용하고 있다.\n","\n","- BPE는 모든 단어를 문자들의 집합으로 취급하여 자주 등장하는 문자 쌍을 합치는 subword tokenizeation이다. 처음 보는 단어들일지라도 문자(알파벳)들의 조합으로 나타내어 OOV문제를 해결할 수 있다는 장점이 있다. 기존 트랜스포머와 마찬가지로 포지션 인코딩도 함께 사용한다.\n","\n","2. Masked Multi-Head Attention\n","- Masked Multi-Head Attention은 모든 것을 병렬적으로 처리하는 트랜스포머에게 자기회귀적인 특성을 부여하기 만든 장치입니다. 여기서 자기회귀적이란 훈련 단계에서 디코더에게 정답 문장을 매 스텝 단위로 단어 하나씩 알려주고 그 다음 단어를 예측하게하는 형태로 학습되는 형태라는 것이다. 이는 마지 Seq2Seq 모델에서 디코더가 번역 문장을 생성할 때 time-step을 하나하나 거치듯이 만들어주는 것입니다. 순차처리 방식의 RNN과 달리 정답 문장의 모든 단어를 한꺼번에 입력받는 트랜스포머의 decoder는 학습할 때 현재 자기보다 미래에 생성될 토큰을 보지 못하도록 masking이 필요하게 됩니다.\n","\n","- 사실 이 구조는 예전에 공부한 언어 모델과 같은 구조이다. 다시 한번 되짚어보면, 언어 모델은 비지도 학습을 통해 문장의 자연스러운 순서를 핛습하게 됩니다. 그래서 GPT는 문자 생성에 매우 특화되어 있다. GPT를 연구한 Open AI도 너무나 자연스러운 문장을 만들어내 부작용이 두려워 소스 코드를 공개하지 않았다. 그렇다고해서 요약, 텍스트 분류 등 다른 task들에서 성능이 떨어지는 것은 아니다.\n","\n","Text Prediction & Text classification: finetuning downstream task (Supervised Learning)\n","\n","- 사전 학습이 끝나게 되면 GPT는 다운스트림 task에 맞게 fine-tuning을 하게 됩니다. 바로 파란색 박스 부분에 해당한다. 여기서 우리는 기존에 봐왔던 모들들과 조금 다른 점을 발견할 수 있다. 바로 두개의 Objective가 존재한다는 것이다.\n","\n","- 말 그대로 모델이 두 가지의 문제를 동시에 푸는 것이다. 글자 예측과 분류는 각각 다른 모델들을 이용하여 output을 만들어내는 것이 아니라 한 모델에서 동시에 output을 내는 겁니다.\n","\n","- 논문의 저자들은 이렇게 실제 풀어야 하는 문제인 주요 task와 동시에 보조적으로 또 다른 문제를 풀때 (Auxiliary objective) 주요 task에 대한 정확도가 더 올라갔음을 확인했다고 한다.\n","\n","- 생각해 보면 LM또한, auxiliary로 얻어진 결과라고 생각 할 수 있다. 시퀀스의 다음 나올 단어들을 학습하다 보니 전체적인 어너의 구조를 알게 될 것이다.\n","\n","- 아래는 GPT의 모델 부분을 코드로 작성한 것이다. 전체 코드이기 때문에 다소 복잡해 보인다. FGPT2MainLayer라는 전체 모델 클래스 안에서 TFBlock 레이어 클래스를 반복해서 사용하고 있는 부분을 눈여겨 보고 TFBlock 클래스 안에서 TFAttention, TFMLP 레이어가 사용되는 구조가 위에서 소개한 GPT의 모델 구조 그림에 표현되어 있습니다.\n","\n"],"metadata":{"id":"ikFQq5nnlStB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vy_rZ97wZSRj"},"outputs":[],"source":["class TFAttention(tf.keras.layers.Layer):\n","    def __init__(self, nx, n_ctx, config, scale=False, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n","        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n","        assert n_state % config.n_head == 0\n","        self.n_ctx = n_ctx\n","        self.n_head = config.n_head\n","        self.split_size = n_state\n","        self.scale = scale\n","        self.output_attentions = config.output_attentions\n","\n","        self.c_attn = TFConv1D(n_state * 3, nx, initializer_range=config.initializer_range, name=\"c_attn\")\n","        self.c_proj = TFConv1D(n_state, nx, initializer_range=config.initializer_range, name=\"c_proj\")\n","        self.attn_dropout = tf.keras.layers.Dropout(config.attn_pdrop)\n","        self.resid_dropout = tf.keras.layers.Dropout(config.resid_pdrop)\n","        self.pruned_heads = set()\n","\n","    def prune_heads(self, heads):\n","        pass\n","\n","    @staticmethod\n","    def causal_attention_mask(nd, ns, dtype):\n","        \"\"\"\n","        1-2) masked attention에서 설명한 masking 부분\n","        \"\"\"\n","        i = tf.range(nd)[:, None]\n","        j = tf.range(ns)\n","        m = i >= j - ns + nd\n","        return tf.cast(m, dtype)\n","\n","    def _attn(self, q, k, v, attention_mask, head_mask, output_attentions, training=False):\n","\t\t\t\t\"\"\"\n","\t\t\t\t1-2) attention 계산\n","        q, k, v 의 shape : [batch, heads, sequence, features]\n","\t\t\t\t\"\"\"\n","\n","        w = tf.matmul(q, k, transpose_b=True)\n","        if self.scale:\n","            dk = tf.cast(shape_list(k)[-1], tf.float32)  # scale attention_scores\n","            w = w / tf.math.sqrt(dk)\n","\n","        # w shape : [batch, heads, dst_sequence, src_sequence]\n","        _, _, nd, ns = shape_list(w)\n","        b = self.causal_attention_mask(nd, ns, dtype=w.dtype)\n","        b = tf.reshape(b, [1, 1, nd, ns])\n","        w = w * b - 1e4 * (1 - b)\n","\n","        if attention_mask is not None:\n","            # attention mask 적용\n","            w = w + attention_mask\n","\n","        w = tf.nn.softmax(w, axis=-1)\n","        w = self.attn_dropout(w, training=training)\n","\n","        # Mask heads if we want to\n","        if head_mask is not None:\n","            w = w * head_mask\n","\n","        outputs = [tf.matmul(w, v)]\n","        if output_attentions:\n","            outputs.append(w)\n","        return outputs\n","\n","    def merge_heads(self, x):\n","        x = tf.transpose(x, [0, 2, 1, 3])\n","        x_shape = shape_list(x)\n","        new_x_shape = x_shape[:-2] + [x_shape[-2] * x_shape[-1]]\n","        return tf.reshape(x, new_x_shape)\n","\n","    def split_heads(self, x):\n","        x_shape = shape_list(x)\n","        new_x_shape = x_shape[:-1] + [self.n_head, x_shape[-1] // self.n_head]\n","        x = tf.reshape(x, new_x_shape)\n","        return tf.transpose(x, (0, 2, 1, 3))  # (batch, head, seq_length, head_features)\n","\n","    def call(self, x, layer_past, attention_mask, head_mask, use_cache, output_attentions, training=False):\n","        x = self.c_attn(x)\n","        query, key, value = tf.split(x, 3, axis=2)\n","        query = self.split_heads(query)\n","        key = self.split_heads(key)\n","        value = self.split_heads(value)\n","        if layer_past is not None:\n","            past_key, past_value = tf.unstack(layer_past, axis=0)\n","            key = tf.concat([past_key, key], axis=-2)\n","            value = tf.concat([past_value, value], axis=-2)\n","\n","        # keras serialization을 위한 코드\n","        if use_cache:\n","            present = tf.stack([key, value], axis=0)\n","        else:\n","            present = (None,)\n","\n","        attn_outputs = self._attn(query, key, value, attention_mask, head_mask, output_attentions, training=training)\n","        a = attn_outputs[0]\n","\n","        a = self.merge_heads(a)\n","        a = self.c_proj(a)\n","        a = self.resid_dropout(a, training=training)\n","\n","        outputs = [a, present] + attn_outputs[1:]\n","        return outputs  # a, present, (attentions)\n","\n","\n","class TFMLP(tf.keras.layers.Layer):\n","\"\"\"\n","Transformer의 Decoder Block에서 Feed Foward를 구현해 둔 부분\n","\"\"\"\n","    def __init__(self, n_state, config, **kwargs):\n","        super().__init__(**kwargs)\n","        nx = config.n_embd\n","        self.c_fc = TFConv1D(n_state, nx, initializer_range=config.initializer_range, name=\"c_fc\")\n","        self.c_proj = TFConv1D(nx, n_state, initializer_range=config.initializer_range, name=\"c_proj\")\n","        self.act = get_tf_activation(\"gelu\")\n","        self.dropout = tf.keras.layers.Dropout(config.resid_pdrop)\n","\n","    def call(self, x, training=False):\n","        h = self.act(self.c_fc(x)) # conv1d로 flatten 후 activation 적용\n","        h2 = self.c_proj(h)\n","        h2 = self.dropout(h2, training=training)\n","        return h2\n","\n","\n","class TFBlock(tf.keras.layers.Layer):\n","\"\"\"\n","Transformer의 Decoder Block을 구현해 둔 부분\n","\"\"\"\n","    def __init__(self, n_ctx, config, scale=False, **kwargs):\n","        super().__init__(**kwargs)\n","        nx = config.n_embd\n","        inner_dim = config.n_inner if config.n_inner is not None else 4 * nx\n","        self.ln_1 = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name=\"ln_1\")\n","        self.attn = TFAttention(nx, n_ctx, config, scale, name=\"attn\")\n","        self.ln_2 = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name=\"ln_2\")\n","        self.mlp = TFMLP(inner_dim, config, name=\"mlp\")\n","\n","    def call(self, x, layer_past, attention_mask, head_mask, use_cache, output_attentions, training=False):\n","        a = self.ln_1(x)\n","        output_attn = self.attn(\n","            a, layer_past, attention_mask, head_mask, use_cache, output_attentions, training=training\n","        )\n","        a = output_attn[0]  # output_attn: a, present, (attentions)\n","        x = x + a\n","\n","        m = self.ln_2(x)\n","        m = self.mlp(m, training=training)\n","        x = x + m\n","\n","        outputs = [x] + output_attn[1:]\n","        return outputs  # x, present, (attentions)\n","\n","\n","@keras_serializable\n","class TFGPT2MainLayer(tf.keras.layers.Layer):\n","\"\"\"\n","모델의 전체 구조\n","\"\"\"\n","    config_class = GPT2Config\n","\n","    def __init__(self, config, *inputs, **kwargs):\n","        super().__init__(*inputs, **kwargs)\n","        self.output_attentions = config.output_attentions\n","        self.output_hidden_states = config.output_hidden_states\n","        self.use_cache = config.use_cache\n","        self.return_dict = config.use_return_dict\n","\n","        self.num_hidden_layers = config.n_layer\n","        self.vocab_size = config.vocab_size\n","        self.n_embd = config.n_embd\n","\n","        self.wte = TFSharedEmbeddings(\n","            config.vocab_size, config.hidden_size, initializer_range=config.initializer_range, name=\"wte\"\n","        )\n","        self.wpe = tf.keras.layers.Embedding(\n","            config.n_positions,\n","            config.n_embd,\n","            embeddings_initializer=get_initializer(config.initializer_range),\n","            name=\"wpe\",\n","        )\n","        self.drop = tf.keras.layers.Dropout(config.embd_pdrop)\n","        self.h = [TFBlock(config.n_ctx, config, scale=True, name=\"h_._{}\".format(i)) for i in range(config.n_layer)]\n","        self.ln_f = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_epsilon, name=\"ln_f\")\n","\n","    def get_input_embeddings(self):\n","        return self.wte\n","\n","    def set_input_embeddings(self, value):\n","        self.wte.weight = value\n","        self.wte.vocab_size = self.wte.weight.shape[0]\n","\n","    def _prune_heads(self, heads_to_prune):\n","        \"\"\"\n","        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n","        \"\"\"\n","        raise NotImplementedError\n","\n","    def call(\n","        self,\n","        inputs,\n","        past=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        use_cache=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        training=False,\n","    ):\n","        if isinstance(inputs, (tuple, list)):\n","            input_ids = inputs[0]\n","            past = inputs[1] if len(inputs) > 1 else past\n","            attention_mask = inputs[2] if len(inputs) > 2 else attention_mask\n","            token_type_ids = inputs[3] if len(inputs) > 3 else token_type_ids\n","            position_ids = inputs[4] if len(inputs) > 4 else position_ids\n","            head_mask = inputs[5] if len(inputs) > 5 else head_mask\n","            inputs_embeds = inputs[6] if len(inputs) > 6 else inputs_embeds\n","            use_cache = inputs[7] if len(inputs) > 7 else use_cache\n","            output_attentions = inputs[8] if len(inputs) > 8 else output_attentions\n","            output_hidden_states = inputs[9] if len(inputs) > 9 else output_hidden_states\n","            return_dict = inputs[10] if len(inputs) > 10 else return_dict\n","            assert len(inputs) <= 11, \"Too many inputs.\"\n","        elif isinstance(inputs, (dict, BatchEncoding)):\n","            input_ids = inputs.get(\"input_ids\")\n","            past = inputs.get(\"past\", past)\n","            attention_mask = inputs.get(\"attention_mask\", attention_mask)\n","            token_type_ids = inputs.get(\"token_type_ids\", token_type_ids)\n","            position_ids = inputs.get(\"position_ids\", position_ids)\n","            head_mask = inputs.get(\"head_mask\", head_mask)\n","            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n","            use_cache = inputs.get(\"use_cache\", use_cache)\n","            output_attentions = inputs.get(\"output_attentions\", output_attentions)\n","            output_hidden_states = inputs.get(\"output_hidden_states\", output_hidden_states)\n","            return_dict = inputs.get(\"return_dict\", return_dict)\n","            assert len(inputs) <= 11, \"Too many inputs.\"\n","        else:\n","            input_ids = inputs\n","\n","        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n","        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n","        use_cache = use_cache if use_cache is not None else self.use_cache\n","        return_dict = return_dict if return_dict is not None else self.return_dict\n","\n","        if input_ids is not None and inputs_embeds is not None:\n","            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n","        elif input_ids is not None:\n","            input_shape = shape_list(input_ids)\n","            input_ids = tf.reshape(input_ids, [-1, input_shape[-1]])\n","        elif inputs_embeds is not None:\n","            input_shape = shape_list(inputs_embeds)[:-1]\n","        else:\n","            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n","\n","        if past is None:\n","            past_length = 0\n","            past = [None] * len(self.h)\n","        else:\n","            past_length = shape_list(past[0][0])[-2]\n","        if position_ids is None:\n","            position_ids = tf.range(past_length, input_shape[-1] + past_length, dtype=tf.int32)[tf.newaxis, :]\n","\n","        if attention_mask is not None:\n","            # 3D attention mask 만들기\n","            # Sizes : [batch_size, 1, 1, to_seq_length]\n","            # 3D attention mask를 [batch_size, num_heads, from_seq_length, to_seq_length]로 브로드캐스팅\n","\n","            attention_mask = attention_mask[:, tf.newaxis, tf.newaxis, :]\n","\n","            # attention_mask가 1.0이면 포함, 0.0이면 미포함 하여 attention 계산\n","            attention_mask = tf.cast(attention_mask, tf.float32)\n","            attention_mask = (1.0 - attention_mask) * -10000.0\n","        else:\n","            attention_mask = None\n","\n","        # head_mask가 1.0이면, head를 유지\n","        # attention_probs : shape bsz x n_heads x N x N\n","        # input head_mask : shape [num_heads] 또는 [num_hidden_layers x num_heads]\n","        # head_mask : shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n","        if head_mask is not None:\n","            raise NotImplementedError\n","        else:\n","            head_mask = [None] * self.num_hidden_layers\n","            # head_mask = tf.constant([0] * self.num_hidden_layers)\n","\n","        position_ids = tf.reshape(position_ids, [-1, shape_list(position_ids)[-1]])\n","\n","        if inputs_embeds is None:\n","            inputs_embeds = self.wte(input_ids, mode=\"embedding\")\n","        position_embeds = self.wpe(position_ids)\n","        if token_type_ids is not None:\n","            token_type_ids = tf.reshape(token_type_ids, [-1, shape_list(token_type_ids)[-1]])\n","            token_type_embeds = self.wte(token_type_ids, mode=\"embedding\")\n","        else:\n","            token_type_embeds = 0\n","        hidden_states = inputs_embeds + position_embeds + token_type_embeds\n","        hidden_states = self.drop(hidden_states, training=training)\n","\n","        output_shape = input_shape + [shape_list(hidden_states)[-1]]\n","\n","        presents = () if use_cache else None\n","        all_attentions = () if output_attentions else None\n","        all_hidden_states = () if output_hidden_states else None\n","        for i, (block, layer_past) in enumerate(zip(self.h, past)):\n","            if output_hidden_states:\n","                all_hidden_states = all_hidden_states + (tf.reshape(hidden_states, output_shape),)\n","\n","            outputs = block(\n","                hidden_states,\n","                layer_past,\n","                attention_mask,\n","                head_mask[i],\n","                use_cache,\n","                output_attentions,\n","                training=training,\n","            )\n","\n","            hidden_states, present = outputs[:2]\n","            if use_cache:\n","                presents = presents + (present,)\n","\n","            if output_attentions:\n","                all_attentions = all_attentions + (outputs[2],)\n","\n","        hidden_states = self.ln_f(hidden_states)\n","\n","        hidden_states = tf.reshape(hidden_states, output_shape)\n","        # Add last hidden state\n","        if output_hidden_states:\n","            all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","        if output_attentions:\n","            attention_output_shape = input_shape[:-1] + [-1] + shape_list(all_attentions[0])[-2:]\n","            all_attentions = tuple(tf.reshape(t, attention_output_shape) for t in all_attentions)\n","\n","        if not return_dict:\n","            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_attentions] if v is not None)\n","\n","        return TFBaseModelOutputWithPast(\n","            last_hidden_state=hidden_states,\n","            past_key_values=presents,\n","            hidden_states=all_hidden_states,\n","            attentions=all_attentions,\n","        )"]},{"cell_type":"markdown","source":["Input Transformation\n","\n","Input을 변형시키면 한 개의 모델로 classification, entailment 등등 다양한 문제를 풀 수 있다.\n","\n","- GPT는 언어 모델이기 때문에 사전 학습시에는 문장(단어의 시퀀스들)을 그대로 input을 줍니다. fine-tuning 시에도 똑같이 단어의 시퀀스들을 주면 된다. 이때, input을 아주 조금만 변형시켜 주면 됩니다.\n","\n","- 예를 들어 classification task를 풀기 위해 finetuning을 하게 된다면 <start> <input text> <extract> <class> 이렇게 구성된 데이터셋을 학습시키면 되는 것이죠. GPT는 이 데이터셋에 맞추어서 weight들을 조정하게 될 것입니다.\n","\n","- finetuning이 끝나고 테스트 시에는 <start> <input text> <extract>을 input으로 주면 해당 시퀀스에 뒤이어 나올 토큰 즉 <class>를 생성하게 되는 것이죠.\n","\n","2. GPT vs. GPT2\n","- 이 두 모델의 원리는 같다. GPT의 모델 구조를 그대로 사용하면서 파라미터 사이즈를 10배 정도 키우고 성능을 개선시킨 모델이 바로 GPT2이다.\n","\n","- 성능 개선을 위해서 쓴 테크닉들은 자세히 다루지 않는다. 간단하게 언급하면 이전보더 더욱 정제된 데이터셋을 더욱 많이 사용해서 학습시킨 것과 토크나이저를 더 강력하게 만든 것 등등이 있다. 더 자세한 내용은 아래 논문을 참고한다.\n","\n","[GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)\n","\n","[GPT2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n","\n","3. GPT3\n","- 2020년 6월에 공개되었다. GPT1의 1000배, GPT2의 100배 이상의 파라미터를 갖췄고, 성능도 크게 향상되었습니다. GPT3가 할 수 있는 작업에는 언어 관련 문제 풀이, 랜덤 글짓기, 간단한 사칙 연산, 번역, 간단한 웹코딩, 문장 교정 등이 있습니다.\n","\n","- GPT3 모델은 GPT2와 같은 구조를 가졌지만 모델과 데이터의 크기를 늘리고, 학습 시간을 늘렸습니다. 또한 odified initialization, pre-normalization, reversable tokenization을 적용하고, 트랜스포머 레이어의 attention 패턴에 대해 dense와 locally banded sparse attention을 번갈아 사용했다.\n","\n","- 하지만 GPT를 개발한 OPEN AI는 GPT3 모델을 오픈 소스로 공개하는 것을 원치 않는다. 그 이유는 GPT-3의 성능이 뛰어나 악용의 우려가 있기 때문이다. 하지만 GPT-3를 이용한 API를 [클로즈 베타](https://openai.com/api/)로 공개했다. 그 후 많은 사람들이 GPT-3를 사용하여 다양한 실험을 하였고 그 실험은 이 [블로그](https://uipath.tistory.com/44)에 정리되어있다.\n","\n","[GPT3](https://arxiv.org/pdf/2005.14165.pdf)\n","\n","4. GPT Neo\n","- GPT3를 복제한 [GPT-Neo](https://www.eleuther.ai/projects/gpt-neo/)라는 모델이 EleutherAI라는 비영리 오픈 소스 연구 단체에서 오픈 소스로 공개되었습니다. 모델 뿐 아니라 대규모 데이터과 사전학습된 모델도 공개되었죠. 최근에는 60억 개의 파라미터를 가진 [GPT-J-6B](https://6b.eleuther.ai/)를 공개하기도 하였습니다. GPT3보다 작은 파라미터를 가지고 있지만 GPT3와 유사한 수준의 파라미터 수를 갖는 모델인 GPT-NeoX을 만들고 있다고 하니 기대해 보아도 좋을 것 같습니다.\n","\n","[GPT-Neo](https://github.com/EleutherAI/gpt-neo)\n","\n","[GPT-J-7B](https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b)\n","\n","[GPT-NeoX](https://github.com/EleutherAI/gpt-neox)"],"metadata":{"id":"Ob-76o_wqI8B"}},{"cell_type":"markdown","source":["# 13-5. BERT(Bidirectional Encoder Representations from Transformers)\n","\n","2018년 돌연 혜성처럼 나타나 NLP계를 점령했던 BERT, 성능도 성능이지만 이후로도 많은 연구들에 영감을 주었던 모델입니다.\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/images/13_hSZnKQX.max-800x600.png)\n","<center></center>\n","\n","BERT에 대해 자세히 알아보기 전에 이전까지 배웠던 ELMo, GPT와 비교해 보는 시간을 마련했습니다.\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/images/14_VGS9u7n.max-800x600.png)\n","<center>BERT(좌)와 GPT(우)의 구조 </center>\n","\n","- 먼저 GPT와 비교해 보겠습니다. \n","\n","- 트랜스포머의 decoder를 이용하여 만든 모델인 만큼 GPT는 input을 한 방향으로만 보게됩니다. 다음 단어를 예측해야만 하는 LM의 특징이었다.\n","\n","- 이와는 다르게 BERT는 양방향으로 input을 보고 있는 것을 확인할 수 있습니다.\n","\n","- 그림에서는 보이지 않지만 또 다른 큰 차이점이 존재합니다. BERT는 트랜스포머의 encoder만을 사용한 모델입니다.\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/original_images/15_AHA5nKS.png)\n","<center>ELMo의 구조</center>\n","\n","- 다음은 ELMo와 비교입니다.\n","\n","- ELMo가 bidirection LSTM을 사용하긴 했지만 독립적인 모델처럼 학습하고 마지막 layer에서만 합쳐준다. 따라서 ELMo는 가장 위에 layer만 양방향 정보를 가지고 있다. 이는 모든 layer들이 양방향으로 보는 BERT와 가장 큰 차이점이라고 할 수 있다.\n","\n","- BERT는 이처럼 진짜 양방향이 뭔지 보여주기 위해 만들어진 모델입니다. \n","\n","1. BERT의 구조\n","- 앞서 말했듯이 BERT는 트랜스포머의 encoder 구조를 이용한 모델입니다. 이제부터는 트랜스포머와의 차이점을 위주로 이야기해 보겠습니다.\n","\n","(1) Transformer Encoder Block\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/original_images/16_8aNWN0Q.png)\n","<center>Transformer의 모델 구조</center>\n","\n","1) Embedding(임베딩)\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/images/30.max-800x600.png)\n","<center>ELMo의 구조</center>\n","\n","- BERT는 기존의 트랜스포머와 유사하지만 다른 임베딩 체계를 가지고 있습니다. 세 가지 임베딩을 가지고있다.\n","  - Token Embedding\n","  \n","    우선 Wordpiece을 이용하여 3만 개의 vocabulary를 학습합니다. 학습한 Wordpiece model을 이용하여 token들을 임베딩해줍니다.\n","\n","  - Segment Embedding\n","\n","    BERT는 두 가지 sentence(BERT의 논문에서 나오는 sentence는 문장의 의미보다는 텍스트 덩어리의 의미입니다.)를 입력으로 받기 때문에 이 두 가지 sentence를 구분해야 할 필요가 있습니다. segment embedding은 바로 이를 위해서 존재한다. 모델 입장에서 주르륵 이어진 텍스트들의 덩어리를 나누어주는 역할을 합니다. 그림에서도 [SEP] 토큰으로 구부된 두 sentnece에서 앞 부분은 A, 뒷부분은 B로 나누어 임베딩하는 것을 확인할 수 있다.\n","\n","  - Position Embedding\n","\n","    마지막으로 position embedding입니다. 두 sentence를 나누는 것은 segment embedding에서 해주지만, sentence 안에서의 순서는 아직 모르는 상태입니다. 따라서 문장 내에 절대적인 위치(순서)를 알려주기 위해 필요한 것이 position embedding입니다.\n","\n"," - 여기서 positional encoding과 position embedding가 똑같다고 생각할 수 있으나 그렇지않다. 혼용하여 사용하긴 하지만 엄밀하게는 다른 개념이다. encoding은 ne-hot-encoding(원핫 인코딩)처럼 미리 정해진 값을 주는 것이지만 embedding은 그 값이 정해진 것이 아니라 학습을 통해 습득하는 것을 의미합니다. 다시 말해서, BERT는 학습을 통해 position 정보를 습득한다고 생각하시면 됩니다.\n","\n","- 이렇게 얻은 세 가지 임베딩을 모두 합산하ㅕ BERT의 임베딩이 완성됩니다. 이후 layer normalization솨 dtopout까지 해주면 트랜스포머 첫 블록의 입력이 완성됩니다.\n","\n","- [SEP] 토큰은 위에서 간단하게 설명하고 넘어갔다. 이것들은 special token이라고 해서 실제 단어에는 쓰이지 않지만 특별한 역할들을 가지고 있다. seq2seq 모델에서도 본 적이 있을 것이다. 문장의 시작과 끝을 알려주기 위해 쓰였던 <BOS>,<EOS> 같은 토큰들 말이다.\n","\n","- 그럼 BERT에는 어떤 토큰들이 어떻게 쓰였나?\n"," - [CLS] : sentence의 시작을 알리는 토큰\n"," - [SEP] : sentence의 종결을 알리는 토큰, sentence를 구분하는 데에 사용하기도 함\n"," - [MASK]: 마스크 토큰\n"," - [PAD] : 배치 데이터의 길이를 맞춰주기 위한 토큰\n","\n","2) Activation Function(활성화 함수) : GELU\n","- Feedforward Networks에서 BERT는 ReLU대신 GELU를 사용합니다. 음수 값은 0이 되어버리는 ReLU와는 달리 GELU는 음수에서도 완만한 곡선을 그리면 미분을 가능하게 합니다. GELU를 사용하면 성능이 더욱 좋아지기 때문에 BERT의 저자들은 GELU를 사용했다고 한다.\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/original_images/17_d9apPsw.png)\n","<center>activation function의 비교</center>\n","\n","2. BERT의 학습\n","- BERT는 양방향을 강조한 모델이다. BERT는 학습 방식은 다음과 같다.\n","\n","1) Masked LM(MLM)\n","- 다음 단어를 예측해야만 하는 일반적인 LM은 그 task의 특성상 한 방향일 수 밖에없다. 이와 달리, BERT는 마스크 된 토큰만 맞추면 되는 masked LM(MLM)을 제안했다. 즉, input sequence의 순서에 상관없이 전체 문장을 모두 볼 수 있는 것이다.\n","\n","- NLM을 위해서 BERT는 학습 데이터의 전체에서 15%를 [MASK] 토큰으로 랜덤하게 바꿉니다. 여기서 15%에 해당하는 모든 토큰들을 마스크하는 것이 아니라 80%는 [MASK] 토큰, 10%는 무작위로 랜덤한 토큰으로 바꿔줍니다. 나머지 10%는 원래의 토큰을 그대로 사용한다.\n","\n","- 그 이유는 바로 fine-tuning에 있다. 사전 학습을 끝낸 모델을 fine-tuning할 때에는 input에 [mask] 토큰이 등장하기 않기 때문입니다. 아무래도 fine-tuning시에 [mask] 토큰이 보이지 않는다면, 당연히 성능에 영향을 미치게 될 것이라고 생각한 것이다. 따라서, [mask] 토큰이 아닌 것들도 예측을 하도록 학습하여 문장 자체에 대한 전반적이 이해를 할 수 있도록 해주는 겁니다.\n","\n","- 다시 정리하면, 전체 학습 데이터의 토큰 중 12%(15% 중 80%)는 [mask] 토큰으로, 1.5%(15% 중에서 10%)는 무작위로 랜덤한 토큰으로 대체하고, 1.5%((15% 중에서 10%)는 변경하지 않고 원래의 토큰을 사용하는 겁니다.\n","\n","2) Next Sentence Prediction (NSP)\n","- BERT는 마스크 된 토큰을 맞추는 것과 동시에 또 다른 task를 함께 학습합니다. 바로 Next Sentence Prediction (NSP), 다음 문장인지 확인하기 입니다.\n","\n","다음 예시가 있다.\n","\n","여름의 마지막 해수욕 누가 제일 늦게 바다에서 나왔나\n","\n","그 사람이 바다의 뚜껑 닫지 않고 돌아가\n","\n","그때부터 바다의 뚜껑 열린 채 그대로 있네\n","\n","하라 마스미 「바다의 뚜껑」 중\n","\n","- 한 행을 하나의 sentence라고 가정한다. 그렇다면 '여름의 마지막 해수욕 누가 제일 늦게 바다에서 나왔나' 다음의 sentence는 '그 사람이 바다의 뚜껑 닫지 않고 돌아가'가 될 것입니다. BERT는 이처럼 두 sentence가 연속해서 오는지의 여부를 학습하게 됩니다.\n","\n","- [CLS]여름의 마지막 해수욕 누가 제일 늦게 바다에서 나왔나 [SEP] 그 사람이 바다의 뚜껑 닫지 않고 돌아가[SEP] → TRUE(IsNext)\n","\n","- [CLS]여름의 마지막 해수욕 누가 제일 늦게 바다에서 나왔나 [SEP] 한강에서 자전거 타며 아이스 아메리카노를 마시고 싶다[SEP] → FALSE(NotNext)\n","\n","- NSP를 위해서 BERT의 학습 데이터는 1건당 두 개의 문장으로 구성합니다. 50%의 확률로 TRUE와 FALSE를 부여하도록 만듭니다.\n","\n","- 또한, task가 너무 쉬워지는 것을 방지하기 위해 max_num_tokens라는 것을 정의합니다. 데이터의 90%는 max_num_tokens가 max_sequence_length가 같도록 만들고, 나머지 10%의 데이터는 max_num_tokens가 max_sequence_length보다 짧게 되도록 랜덤으로 정합니다.\n","\n","- 이후, 두 개의 sentence의 단어 총수가 max_num_tokens보다 작아질 때까지 두 sentence 중 단어 수가 많은 쪽의 문장 맨 앞 또는 맨 뒤 단어를 하나씩 제거합니다. 이때 문장 맨 앞의 단어를 선택할지 맨 뒤의 단어를 선택할지는 50%의 확률로 정합니다.\n","\n","- 이렇게 NSP를 학습하게 되면, 문장과 문장 사이의 관계를 학습할 수 있게 됩니다. 문장의 길이를 임의적으로 조정하면서, 짧은 문장에 대해서도 성능이 크게 떨어지지 않게 되며, 문장의 단어들을 랜덤하게 삭제하는 과정에서 문장에서 일부 단어들이 없어져도 그 영향을 크게 받지 않게 됩니다.\n","\n","- 지금까지 본 NLN과 NSP는 따로 학습되는 것이 아니라 동시에 이뤄진다. 따라서 실제 BERT의 학습 데이터셋은 아래와 같은 구조일 것이다.(편의상 토큰을 띄어쓰기 단위로 나누겠습니다)\n","\n","- [CLS]여름의 마지막 [MASK] 누가 제일 늦게 [MASK] 나왔나 [SEP] 그 사람이 바다의 [MASK] 닫지 않고 돌아가[SEP] → Label : TRUE(IsNext)\n","\n","- [CLS]여름의 [MASK] 해수욕 누가 제일 늦게 바다에서 나왔나 [SEP] 한강에서 [MASK] 아이스 아메리카노를 마시고 싶다[SEP] → Label : FALSE(NotNext)\n","\n","3. Fine-tuning Task\n","- BERT 또한 한 모델이 다양한 task들을 수행하기 때문에 input transformation을 이용합니다.\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/images/18_Qdk6eaL.max-800x600.png)\n","<center>BERT의 finetuning</center>\n","\n","- classificastion 같은 경우는 [CLS] 토큰을, QA와 같이 문장이나 단어들이 나와야 하는 경우에는 토큰들의 벡터를 output layer에 넣어 output을 산출해낸다.\n","\n","4. BERT의 모델 코드\n","- Transformer, GPT와 다른 부분을 유심히 보면서 모델을 훑어본다면 더욱 도움이 될 거예요! 메인 모델 구조인 TFBertEncoder 안에 반복적으로 사용되고 있는 TFBertLayer 레이어 구성을 자세히 살펴주세요. 전체 구조를 한눈에 보기에는 복잡하게 느껴지시더라도, 다음 노드에서 단계별로 차례차례 구현해 보면서 좀 더 명확하게 이해하실 수 있으실 겁니다.\n","\n","[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n"],"metadata":{"id":"91qaq66ZuYI7"}},{"cell_type":"code","source":["class TFBertPreTrainingLoss:\n","    \"\"\"\n","    BERT의 경우 Pretraining으로 NSP + MLM 두 가지를 함께 학습하게 됨. 그것을 위한 loss \n","\t\t-100으로 label(logit)이 되어있는 경우 loss 계산 시 제외\n","    \"\"\"\n","\n","    def compute_loss(self, labels, logits):\n","        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n","            from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n","        )\n","\n","        masked_lm_active_loss = tf.not_equal(tf.reshape(labels[\"labels\"], (-1,)), -100)\n","        masked_lm_reduced_logits = tf.boolean_mask(\n","            tf.reshape(logits[0], (-1, shape_list(logits[0])[2])),\n","            masked_lm_active_loss,\n","        )\n","        masked_lm_labels = tf.boolean_mask(tf.reshape(labels[\"labels\"], (-1,)), masked_lm_active_loss)\n","        next_sentence_active_loss = tf.not_equal(tf.reshape(labels[\"next_sentence_label\"], (-1,)), -100)\n","        next_sentence_reduced_logits = tf.boolean_mask(tf.reshape(logits[1], (-1, 2)), next_sentence_active_loss)\n","        next_sentence_label = tf.boolean_mask(\n","            tf.reshape(labels[\"next_sentence_label\"], (-1,)), mask=next_sentence_active_loss\n","        )\n","        masked_lm_loss = loss_fn(masked_lm_labels, masked_lm_reduced_logits)\n","        next_sentence_loss = loss_fn(next_sentence_label, next_sentence_reduced_logits)\n","        masked_lm_loss = tf.reshape(masked_lm_loss, (-1, shape_list(next_sentence_loss)[0]))\n","        masked_lm_loss = tf.reduce_mean(masked_lm_loss, 0)\n","\n","        return masked_lm_loss + next_sentence_loss\n","\n","\n","class TFBertEmbeddings(tf.keras.layers.Layer):\n","    \"\"\"\n","\t\t1-1)에 해당하는 부분으로 3가지 embedding을 만들고 그 embedding을 모두 합산하여 layer normalize와 dropout을 적용\n","\t\t\"\"\"\n","\n","\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.vocab_size = config.vocab_size\n","        self.hidden_size = config.hidden_size\n","        self.initializer_range = config.initializer_range\n","        self.position_embeddings = tf.keras.layers.Embedding(\n","            config.max_position_embeddings,\n","            config.hidden_size,\n","            embeddings_initializer=get_initializer(self.initializer_range),\n","            name=\"position_embeddings\",\n","        )\n","        self.token_type_embeddings = tf.keras.layers.Embedding(\n","            config.type_vocab_size,\n","            config.hidden_size,\n","            embeddings_initializer=get_initializer(self.initializer_range),\n","            name=\"token_type_embeddings\",\n","        )\n","\n","        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n","        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n","\n","    def build(self, input_shape):\n","        \"\"\"shared word embedding layer \"\"\"\n","        with tf.name_scope(\"word_embeddings\"):\n","            # Create and initialize weights. The random normal initializer was chosen\n","            # arbitrarily, and works well.\n","            self.word_embeddings = self.add_weight(\n","                \"weight\",\n","                shape=[self.vocab_size, self.hidden_size],\n","                initializer=get_initializer(self.initializer_range),\n","            )\n","\n","        super().build(input_shape)\n","\n","    def call(\n","        self,\n","        input_ids=None,\n","        position_ids=None,\n","        token_type_ids=None,\n","        inputs_embeds=None,\n","        mode=\"embedding\",\n","        training=False,\n","    ):\n","        \"\"\"\n","        input의 token embeddings\n","        Args:\n","            inputs: int64 tensors (shape [batch_size, length]) 3개를 담은 리스트: (input_ids, position_ids, token_type_ids)\n","            mode: \"embedding\" | \"linear\"\n","        Returns:\n","            outputs: mode == \"embedding\"; output embedding tensor(float32, shape [batch_size, length, embedding_size])\n","\t\t\t\t\t\t\t\t\t\t mode == \"linear\", output linear tensor(float32, shape [batch_size, length, vocab_size])\n","        Raises:\n","            ValueError: if mode is not valid.\n","\t\t\t\t\"\"\"\n","\n","        if mode == \"embedding\":\n","            return self._embedding(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)\n","        elif mode == \"linear\":\n","            return self._linear(input_ids)\n","        else:\n","            raise ValueError(\"mode {} is not valid.\".format(mode))\n","\n","    def _embedding(self, input_ids, position_ids, token_type_ids, inputs_embeds, training=False):\n","        \"\"\"input tensor에 기반하여 임베딩 적용\"\"\"\n","        assert not (input_ids is None and inputs_embeds is None)\n","\n","        if input_ids is not None:\n","            input_shape = shape_list(input_ids)\n","        else:\n","            input_shape = shape_list(inputs_embeds)[:-1]\n","\n","        seq_length = input_shape[1]\n","\n","        if position_ids is None:\n","            position_ids = tf.range(seq_length, dtype=tf.int32)[tf.newaxis, :]\n","\n","        if token_type_ids is None:\n","            token_type_ids = tf.fill(input_shape, 0)\n","\n","        if inputs_embeds is None:\n","            inputs_embeds = tf.gather(self.word_embeddings, input_ids)\n","\n","        position_embeddings = tf.cast(self.position_embeddings(position_ids), inputs_embeds.dtype)\n","        token_type_embeddings = tf.cast(self.token_type_embeddings(token_type_ids), inputs_embeds.dtype)\n","        embeddings = inputs_embeds + position_embeddings + token_type_embeddings\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings, training=training)\n","\n","        return embeddings\n","\n","    def _linear(self, inputs):\n","        \"\"\"\n"," \t\t\t  linear layer를 통해서 input의 logit을 계산\n","        Args:\n","            inputs: float32 tensor (shape [batch_size, length, hidden_size])\n","        Returns:\n","            float32 tensor (shape [batch_size, length, vocab_size])\n","        \"\"\"\n","        batch_size = shape_list(inputs)[0]\n","        length = shape_list(inputs)[1]\n","        x = tf.reshape(inputs, [-1, self.hidden_size])\n","        logits = tf.matmul(x, self.word_embeddings, transpose_b=True)\n","\n","        return tf.reshape(logits, [batch_size, length, self.vocab_size])\n","\n","\n","class TFBertSelfAttention(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        if config.hidden_size % config.num_attention_heads != 0:\n","            raise ValueError(\n","                \"The hidden size (%d) is not a multiple of the number of attention \"\n","                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n","            )\n","\n","        self.num_attention_heads = config.num_attention_heads\n","        assert config.hidden_size % config.num_attention_heads == 0\n","        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n","        self.all_head_size = self.num_attention_heads * self.attention_head_size\n","        self.query = tf.keras.layers.Dense(\n","            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"query\"\n","        )\n","        self.key = tf.keras.layers.Dense(\n","            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"key\"\n","        )\n","        self.value = tf.keras.layers.Dense(\n","            self.all_head_size, kernel_initializer=get_initializer(config.initializer_range), name=\"value\"\n","        )\n","        self.dropout = tf.keras.layers.Dropout(config.attention_probs_dropout_prob)\n","\n","    def transpose_for_scores(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))\n","\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, hidden_states, attention_mask, head_mask, output_attentions, training=False):\n","        batch_size = shape_list(hidden_states)[0]\n","        mixed_query_layer = self.query(hidden_states)\n","        mixed_key_layer = self.key(hidden_states)\n","        mixed_value_layer = self.value(hidden_states)\n","        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n","        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n","        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n","\n","        # \"query\"와 \"key\"의 dot product : raw attention scores\n","        attention_scores = tf.matmul(\n","            query_layer, key_layer, transpose_b=True\n","        )  # (batch size, num_heads, seq_len_q, seq_len_k)\n","        dk = tf.cast(shape_list(key_layer)[-1], attention_scores.dtype)  # scale attention_scores\n","        attention_scores = attention_scores / tf.math.sqrt(dk)\n","\n","        if attention_mask is not None:\n","            attention_scores = attention_scores + attention_mask\n","\n","        # Normalize the attention scores to probabilities.\n","        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n","\n","        attention_probs = self.dropout(attention_probs, training=training)\n","\n","        if head_mask is not None:\n","            attention_probs = attention_probs * head_mask\n","\n","        context_layer = tf.matmul(attention_probs, value_layer)\n","        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\n","        context_layer = tf.reshape(\n","            context_layer, (batch_size, -1, self.all_head_size)\n","        )  # (batch_size, seq_len_q, all_head_size)\n","        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n","\n","        return outputs\n","\n","\n","class TFBertSelfOutput(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.dense = tf.keras.layers.Dense(\n","            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n","        )\n","        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n","        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n","\n","    def call(self, hidden_states, input_tensor, training=False):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.dropout(hidden_states, training=training)\n","        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n","\n","        return hidden_states\n","\n","\n","class TFBertAttention(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.self_attention = TFBertSelfAttention(config, name=\"self\")\n","        self.dense_output = TFBertSelfOutput(config, name=\"output\")\n","\n","    def prune_heads(self, heads):\n","        raise NotImplementedError\n","\n","    def call(self, input_tensor, attention_mask, head_mask, output_attentions, training=False):\n","        self_outputs = self.self_attention(\n","            input_tensor, attention_mask, head_mask, output_attentions, training=training\n","        )\n","        attention_output = self.dense_output(self_outputs[0], input_tensor, training=training)\n","        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n","\n","        return outputs\n","\n","\n","class TFBertIntermediate(tf.keras.layers.Layer):\n","\"\"\"\n","Transformer Block에서의 feedforward\n","\"\"\"\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.dense = tf.keras.layers.Dense(\n","            config.intermediate_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n","        )\n","\n","        if isinstance(config.hidden_act, str):\n","            self.intermediate_act_fn = get_tf_activation(config.hidden_act)\n","        else:\n","            self.intermediate_act_fn = config.hidden_act\n","\n","    def call(self, hidden_states):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.intermediate_act_fn(hidden_states)\n","\n","        return hidden_states\n","\n","\n","class TFBertOutput(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.dense = tf.keras.layers.Dense(\n","            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n","        )\n","        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n","        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n","\n","    def call(self, hidden_states, input_tensor, training=False):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.dropout(hidden_states, training=training)\n","        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n","\n","        return hidden_states\n","\n","\n","class TFBertLayer(tf.keras.layers.Layer):\n","\"\"\"\n","Transformer Encoder Block과 동일한 구조 : Attention,Feedforward,dropout,layer nomalization\n","\"\"\"\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.attention = TFBertAttention(config, name=\"attention\")\n","        self.intermediate = TFBertIntermediate(config, name=\"intermediate\")\n","        self.bert_output = TFBertOutput(config, name=\"output\")\n","\n","    def call(self, hidden_states, attention_mask, head_mask, output_attentions, training=False):\n","        attention_outputs = self.attention(\n","            hidden_states, attention_mask, head_mask, output_attentions, training=training\n","        )\n","        attention_output = attention_outputs[0]\n","        intermediate_output = self.intermediate(attention_output)\n","        layer_output = self.bert_output(intermediate_output, attention_output, training=training)\n","        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\n","\n","        return outputs\n","\n","\n","class TFBertEncoder(tf.keras.layers.Layer):\n","\"\"\"\n","Transformer Encoder Block(코드 상에서 TFBertLayer)를 n_layer만큼 여러개 쌓은 구조\n","\"\"\"\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.layer = [TFBertLayer(config, name=\"layer_._{}\".format(i)) for i in range(config.num_hidden_layers)]\n","\n","    def call(\n","        self,\n","        hidden_states,\n","        attention_mask,\n","        head_mask,\n","        output_attentions,\n","        output_hidden_states,\n","        return_dict,\n","        training=False,\n","    ):\n","        all_hidden_states = () if output_hidden_states else None\n","        all_attentions = () if output_attentions else None\n","\n","        for i, layer_module in enumerate(self.layer):\n","            if output_hidden_states:\n","                all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","            layer_outputs = layer_module(\n","                hidden_states, attention_mask, head_mask[i], output_attentions, training=training\n","            )\n","            hidden_states = layer_outputs[0]\n","\n","            if output_attentions:\n","                all_attentions = all_attentions + (layer_outputs[1],)\n","\n","        # Add last layer\n","        if output_hidden_states:\n","            all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","        if not return_dict:\n","            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\n","\n","        return TFBaseModelOutput(\n","            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_attentions\n","        )\n","\n","\n","class TFBertPooler(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.dense = tf.keras.layers.Dense(\n","            config.hidden_size,\n","            kernel_initializer=get_initializer(config.initializer_range),\n","            activation=\"tanh\",\n","            name=\"dense\",\n","        )\n","\n","    def call(self, hidden_states):\n","        # 첫 번째 토큰의 hidden state를 얻기 위해 pool\n","        first_token_tensor = hidden_states[:, 0]\n","        pooled_output = self.dense(first_token_tensor)\n","\n","        return pooled_output\n","\n","\n","class TFBertPredictionHeadTransform(tf.keras.layers.Layer):\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.dense = tf.keras.layers.Dense(\n","            config.hidden_size, kernel_initializer=get_initializer(config.initializer_range), name=\"dense\"\n","        )\n","\n","        if isinstance(config.hidden_act, str):\n","            self.transform_act_fn = get_tf_activation(config.hidden_act)\n","        else:\n","            self.transform_act_fn = config.hidden_act\n","\n","        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=config.layer_norm_eps, name=\"LayerNorm\")\n","\n","    def call(self, hidden_states):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.transform_act_fn(hidden_states)\n","        hidden_states = self.LayerNorm(hidden_states)\n","\n","        return hidden_states\n","\n","\n","class TFBertLMPredictionHead(tf.keras.layers.Layer):\n","    def __init__(self, config, input_embeddings, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.vocab_size = config.vocab_size\n","        self.transform = TFBertPredictionHeadTransform(config, name=\"transform\")\n","\n","        # input embeddings과 동일한 weight를 가지고 있지만 각각의 token에 대하여 output만 바이어스를 가지고 있음\n","        self.input_embeddings = input_embeddings\n","\n","    def build(self, input_shape):\n","        self.bias = self.add_weight(shape=(self.vocab_size,), initializer=\"zeros\", trainable=True, name=\"bias\")\n","\n","        super().build(input_shape)\n","\n","    def call(self, hidden_states):\n","        hidden_states = self.transform(hidden_states)\n","        hidden_states = self.input_embeddings(hidden_states, mode=\"linear\")\n","        hidden_states = hidden_states + self.bias\n","\n","        return hidden_states\n","\n","\n","class TFBertMLMHead(tf.keras.layers.Layer):\n","\"\"\"\n","2-1)Masked LM을 위한 class\n","\"\"\"\n","    def __init__(self, config, input_embeddings, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.predictions = TFBertLMPredictionHead(config, input_embeddings, name=\"predictions\")\n","\n","    def call(self, sequence_output):\n","        prediction_scores = self.predictions(sequence_output)\n","\n","        return prediction_scores\n","\n","\n","class TFBertNSPHead(tf.keras.layers.Layer):\n","\"\"\"\n","2-2)NSP를 위한 class\n","\"\"\"\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.seq_relationship = tf.keras.layers.Dense(\n","            2, kernel_initializer=get_initializer(config.initializer_range), name=\"seq_relationship\"\n","        )\n","\n","    def call(self, pooled_output):\n","        seq_relationship_score = self.seq_relationship(pooled_output)\n","\n","        return seq_relationship_score\n","\n","\n","@keras_serializable\n","class TFBertMainLayer(tf.keras.layers.Layer):\n","\"\"\"\n","모델의 전체 구조\n","\"\"\"\n","    config_class = BertConfig\n","\n","    def __init__(self, config, **kwargs):\n","        super().__init__(**kwargs)\n","\n","        self.num_hidden_layers = config.num_hidden_layers\n","        self.initializer_range = config.initializer_range\n","        self.output_attentions = config.output_attentions\n","        self.output_hidden_states = config.output_hidden_states\n","        self.return_dict = config.use_return_dict\n","        self.embeddings = TFBertEmbeddings(config, name=\"embeddings\")\n","        self.encoder = TFBertEncoder(config, name=\"encoder\")\n","        self.pooler = TFBertPooler(config, name=\"pooler\")\n","\n","    def get_input_embeddings(self):\n","        return self.embeddings\n","\n","    def set_input_embeddings(self, value):\n","        self.embeddings.word_embeddings = value\n","        self.embeddings.vocab_size = value.shape[0]\n","\n","    def _prune_heads(self, heads_to_prune):\n","        \"\"\"\n","        heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n","        \"\"\"\n","        raise NotImplementedError\n","\n","    def call(\n","        self,\n","        inputs,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        training=False,\n","    ):\n","        if isinstance(inputs, (tuple, list)):\n","            input_ids = inputs[0]\n","            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask\n","            token_type_ids = inputs[2] if len(inputs) > 2 else token_type_ids\n","            position_ids = inputs[3] if len(inputs) > 3 else position_ids\n","            head_mask = inputs[4] if len(inputs) > 4 else head_mask\n","            inputs_embeds = inputs[5] if len(inputs) > 5 else inputs_embeds\n","            output_attentions = inputs[6] if len(inputs) > 6 else output_attentions\n","            output_hidden_states = inputs[7] if len(inputs) > 7 else output_hidden_states\n","            return_dict = inputs[8] if len(inputs) > 8 else return_dict\n","            assert len(inputs) <= 9, \"Too many inputs.\"\n","        elif isinstance(inputs, (dict, BatchEncoding)):\n","            input_ids = inputs.get(\"input_ids\")\n","            attention_mask = inputs.get(\"attention_mask\", attention_mask)\n","            token_type_ids = inputs.get(\"token_type_ids\", token_type_ids)\n","            position_ids = inputs.get(\"position_ids\", position_ids)\n","            head_mask = inputs.get(\"head_mask\", head_mask)\n","            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n","            output_attentions = inputs.get(\"output_attentions\", output_attentions)\n","            output_hidden_states = inputs.get(\"output_hidden_states\", output_hidden_states)\n","            return_dict = inputs.get(\"return_dict\", return_dict)\n","            assert len(inputs) <= 9, \"Too many inputs.\"\n","        else:\n","            input_ids = inputs\n","\n","        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n","        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n","        return_dict = return_dict if return_dict is not None else self.return_dict\n","\n","        if input_ids is not None and inputs_embeds is not None:\n","            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n","        elif input_ids is not None:\n","            input_shape = shape_list(input_ids)\n","        elif inputs_embeds is not None:\n","            input_shape = shape_list(inputs_embeds)[:-1]\n","        else:\n","            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n","\n","        if attention_mask is None:\n","            attention_mask = tf.fill(input_shape, 1)\n","\n","        if token_type_ids is None:\n","            token_type_ids = tf.fill(input_shape, 0)\n","\n","        embedding_output = self.embeddings(input_ids, position_ids, token_type_ids, inputs_embeds, training=training)\n","\n","\t\t\t\t# 3D attention mask 만들기\n","        # Sizes : [batch_size, 1, 1, to_seq_length]\n","        # 3D attention mask를 [batch_size, num_heads, from_seq_length, to_seq_length]로 브로드캐스팅\n","\n","        extended_attention_mask = attention_mask[:, tf.newaxis, tf.newaxis, :]\n","\n","\t\t\t\t# attention_mask가 1.0이면 포함, 0.0이면 미포함 하여 attention 계산\n","        extended_attention_mask = tf.cast(extended_attention_mask, embedding_output.dtype)\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","\n","        # head_mask가 1.0이면, head를 유지\n","        # attention_probs : shape bsz x n_heads x N x N\n","        # input head_mask : shape [num_heads] 또는 [num_hidden_layers x num_heads]\n","        # head_mask : shape [num_hidden_layers x batch x num_heads x seq_length x seq_length\n","        if head_mask is not None:\n","            raise NotImplementedError\n","        else:\n","            head_mask = [None] * self.num_hidden_layers\n","            # head_mask = tf.constant([0] * self.num_hidden_layers)\n","\n","        encoder_outputs = self.encoder(\n","            embedding_output,\n","            extended_attention_mask,\n","            head_mask,\n","            output_attentions,\n","            output_hidden_states,\n","            return_dict,\n","            training=training,\n","        )\n","\n","        sequence_output = encoder_outputs[0]\n","        pooled_output = self.pooler(sequence_output)\n","\n","        if not return_dict:\n","            return (\n","                sequence_output,\n","                pooled_output,\n","            ) + encoder_outputs[1:]\n","\n","        return TFBaseModelOutputWithPooling(\n","            last_hidden_state=sequence_output,\n","            pooler_output=pooled_output,\n","            hidden_states=encoder_outputs.hidden_states,\n","            attentions=encoder_outputs.attentions,\n","        )"],"metadata":{"id":"Fa6uRzh859y6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 13-6. Transformer-XL(Transformer Extra Long)\n","\n","- transformer-XL은 기존의 언어 모델과 트랜스포머가 가지고 있던 한계점을 개선한 모델입니다. 트랜스포머 이전의 LM에서도 늘 한계점으로 꼽히던 context를 반영하기가 트랜스포머에서도 문제로 떠오릅니다.\n","비교적 짧은 문장에서의 context는 잘 학습했는데, sequence가 길어질수록 그 상관관계(long-term dependency)가 점점 떨어진다는 것이 문제였죠. 주제에 대해서 잘 말하다가 갑자기 다른 이야기를 한다던가 하는 문제 말이에요.\n","\n","transformer-XL은 직관적인 그 이름에서도 드러나듯이 좀 더 긴 context를 어떻게 담을 것인가에 대해 고민한 모델입니다.\n","\n","1. transformer-XL의 구조\n","- context를 중점으로 트랜스포머를 다시 본다.\n","\n","1) Vanilla Transformer LMs\n","- 트랜스포머는 max_seq_length가 정해져 있습니다. 즉, 모델이 감당할 수 있을 만큼 텍스트를 잘라서 학습하고, 학습한 이후부터 다시 일정 길이만큼 잘라서 학습을 하게 되죠. 이때, 이전 segment에서 학습했던 context는 무시되고, 지금 학습을 하고 있는 segment 안에서만 context를 고려하게 됩니다. 다시 말해서 segment1과 segment2는 전혀 공유하는 context가 없이 단절되었다(context fragmentation)는 말입니다. 분명 사람이 볼 때는 이어지는 흐름인데 말이다.\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/original_images/19_RTNDWar.png)\n","<center>vanilla transformer LM의 학습 단계 </center>\n","\n","- 테스트 시 또 다른 문제가 등장하게 되죠. 바로 슬라이딩을 하면서 생기는 문제입니다. 모델은 일정 길이의 context를 보고 한 단어를 예측합니다. 그다음에 딱 한 개만큼만 슬라이딩하여 새로운 context를 만들고 다시 연산하여 하나의 단어를 예측합니다.\n","\n","이렇게 하면 이전 context를 조금씩이나마 유지할 수 있을지 모르지만 연산에 드는 비용이 엄청나겠죠?\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/images/20_87zCAda.max-800x600.png)\n","<center>vanilla transformer LM의 테스트 단계 </center>\n","\n","2) Segment-level recurrence with state reuse\n","- 이러한 문제들을 해결하기 위해 저자들은 recurrence 메커니즘을 도입합니다. 학습 시에 이전 segment에서 계산했었던 hidden state를 사용하는 것이죠. 이를 통해 context fragmentation을 해결하고 long-term dependency를 유지할 수 있게 됩니다.\n","\n","- RNN의 원리를 떠올리면 쉽게 와닿으실 겁니다. 그러나 RNN과는 달리, transformer-XL은 다음 layer에서 이전 layer의 hidden state를 재사용하게 됩니다.\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/images/21_35Nt3rH.max-800x600.png)\n","<center>recurrence를 도입한 구조의 학습 단계</center>\n","\n","- 단, 이때 이전 segment들의 정보를 가진 hidden state들의 gradient는 더 이상 변하지 않도록 고정을 시킵니다.(메모리에 있던 값을 불러온다고 하여 cache라고 부르기도 합니다)\n","\n","![](https://d3s0tskafalll9.cloudfront.net/media/original_images/22_vr7wEs2.png)\n","<center>recurrence를 도입한 구조의 테스트 단계</center>\n","\n","- 트랜스포머 구조의 LM이 테스트 시 가지고 있던 문제도 recurrence 메커니즘을 이용하면 쉽게 해결할 수 있습니다. 이미 계산한 hidden state의 정보를 메모리에 가지고 있다가 cache를 하기 때문에 계속해서 반복하여 똑같은 연산을 할 필요가 없어집니다. 이 덕분에 속도도 더욱 빨라질 수 있습니다.\n","\n","3) Relative Positional Encodings\n","- \n","\n"],"metadata":{"id":"vIC11oPO6CsN"}}]}