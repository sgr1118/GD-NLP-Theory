{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMchJS60f/5Xq/AHRDF2nd4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgr1118/GD-NLP-Theory/blob/main/GD5_Word_Embedding(220101).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5-1. 벡터화\n",
        "\n",
        "1. BOG / DTM\n",
        "\n",
        "(1) BOG\n",
        "\n",
        "[bog 영상](https://youtu.be/dKYFfUtij_U)\n",
        "\n",
        "Q1. Out of Vocabulary 문제란 어떤 문제를 말하나요?\n",
        "- 단어장(사전)에 없는 단어에 대처할 수 없어 난감한 문제\n",
        "\n",
        "(2) DTM\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/GN-3-L-1.max-800x600.jpg)\n",
        "<center>DTM</center>\n",
        "\n",
        "DTM의 행을 문서 벡터, 열을 단어 벡터라고 볼 수 있습니다. 이때, DTM의 문서 벡터나 단어 벡터는 대부분의 값이 0이라는 특징을 가지고 있는데, 이런 벡터를 희소 벡터(sparse vector) 라고 합니다.\n",
        "\n",
        "2. TF-IDF\n",
        "\n",
        "[TF-IDF 영상](https://youtu.be/meEchvkdB1U)\n",
        "\n",
        "3. 원-핫 인코딩\n",
        "- 모든 단어의 관계를 독립적으로 정의한다. 단어장에 있는 모든 단어에 대해서 1 ~ N까지 고유한 정수를 부여한다. 이 정수는 단어장에 있는 각 단어의 일종의 인덱스 역할을 합니다. 각 단어는 V차원의 벡터로 표현되는데, 해당 단어의 인덱스 위치만 1이고 나머지는 전부 0의 값을 가지는 벡터가 됩니다.\n",
        "\n",
        "- 문서 1 : 강아지, 고양이, 강아지\n",
        "- 문서 2 : 애교, 고양이\n",
        "- 문서 3 : 컴퓨터, 노트북\n",
        "\n",
        "이 문서로 단어장을 만들면 크기는 5가 된다. 강아지 1번, 고양이 2번, 컴퓨터 3번, 애교 4번, 노트북 5번\n",
        "\n",
        "숫자 부여에 정해진 규칙은 없지만 관례적으로는 빈도수가 높은 단어들부터 낮은 숫자를 부여해요. 가장 많이 등장하는 단어는 1부터 부여하는 식이죠. 이렇게 하면 아주 큰 정수가 부여된 단어는 빈도수가 아주 낮은 단어라는 뜻이고, 그 단어는 중요하지 않은 단어일 확률이 높습니다. 그러므로 추가적인 전처리로 정수가 아주 큰 숫자는 그냥 단어장에서 제거해버리는 선택을 할 수도 있습니다.\n",
        "\n",
        "- 강아지 : [1, 0, 0, 0, 0]\n",
        "- 고양이 : [0, 1, 0, 0, 0]\n",
        "- 애교 : [0, 0, 1, 0, 0]\n",
        "- 컴퓨터 : [0, 0, 0, 1, 0]\n",
        "- 노트북 : [0, 0, 0, 0, 1]"
      ],
      "metadata": {
        "id": "Hf3DA8k74TOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5-2. 벡터화 실습: 원-핫 인코딩 구현해보기\n",
        "\n",
        "한국어 형태소 konlpy 설치하기"
      ],
      "metadata": {
        "id": "6pIHNLOa_vdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s https://raw.githubusercontent.com/teddylee777/machine-learning/master/99-Misc/01-Colab/mecab-colab.sh | bash"
      ],
      "metadata": {
        "id": "HPFZ10FbATAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcneiDGE3qQo"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리되지 않은 문장\n",
        "text = \"임금님 귀는 당나귀 귀! 임금님 귀는 당나귀 귀! 실컷~ 소리치고 나니 속이 확 뚫려 살 것 같았어.\"\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nsg_-ulrBTwC",
        "outputId": "e8fba49c-f7c2-451d-d467-4ed94e441183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'임금님 귀는 당나귀 귀! 임금님 귀는 당나귀 귀! 실컷~ 소리치고 나니 속이 확 뚫려 살 것 같았어.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2. 전처리 이야기\n",
        "\n",
        "이 텍스트에는' !, ~, . '와 같은 각종 특수문자들이 있습니다. 상황에 따라 다르겠지만, 대개 이런 특수문자들은 자연어 처리에서 큰 의미를 가지지 못합니다. 여기서는 정규 표현식을 사용하여 특수문자들을 제거하고자 합니다.\n",
        "\n",
        "한글과 공백을 제외하고 특수문자만 제거하고 싶다면 어떤 정규 표현식을 사용하면 될까요? 이를 수행하기 위해서는 정규 표현식으로 한국어의 범위를 지정할 수 있어야 합니다. 일반적으로 자음의 범위는 'ㄱ ~ ㅎ', 모음의 범위는 'ㅏ ~ ㅣ'와 같이 지정할 수 있습니다. 해당 범위 내에 어떤 자음과 모음이 속하는지 알고 싶다면 아래의 링크를 참고\n",
        "\n",
        "[Hangul Compatibility Jamo](https://www.unicode.org/charts/PDF/U3130.pdf)\n",
        "\n",
        "또한 완성형 한글의 범위는 '가 ~ 힣'과 같이 사용합니다. 해당 범위 내에 포함된 음절들은 아래의 링크에서 확인할 수 있습니다.\n",
        "\n",
        "[Hangul Syllables](https://www.unicode.org/charts/PDF/UAC00.pdf)\n",
        "\n",
        "한글, 공백을 제외한 모든 문자를 표현하는 regex : [^ㄱ-ㅎㅏ-ㅣ가-힣 ]"
      ],
      "metadata": {
        "id": "L7TvxJssBXPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 정규표현식을 사용한 전처리\n",
        "\n",
        "reg = re.compile('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]') # 가-힣 뒤에 띄어쓰기 포함되있으니 주의!\n",
        "text = reg.sub('', text)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PFGoVifBxgK",
        "outputId": "81a17ddc-d09d-41aa-ea45-e9349c501632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임금님 귀는 당나귀 귀 임금님 귀는 당나귀 귀 실컷 소리치고 나니 속이 확 뚫려 살 것 같았어\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3. 토큰화 이야기\n",
        "\n",
        "단어장을 구성하기 위해서는 단어장의 원소인 토큰(token)이라는 단위를 정해줄 필요가 있습니다. 그리고 한국어는 주로 형태소 분석기를 통해서 토큰 단위를 나눠줍니다. 여기서는 KoNLPy에 내장된 Okt 형태소 분석기를 사용해보겠습니다.\n",
        "\n",
        "[KoNLPy Okt](https://konlpy.org/en/latest/api/konlpy.tag/#okt-class)\n"
      ],
      "metadata": {
        "id": "zY_VNAS0CmDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "okt=Okt()\n",
        "tokens = okt.morphs(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgImaH8FDJJf",
        "outputId": "eba1f855-30bb-4411-dc44-a8bb5094e6ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['임금님', '귀', '는', '당나귀', '귀', '임금님', '귀', '는', '당나귀', '귀', '실컷', '소리', '치고', '나니', '속이', '확', '뚫려', '살', '것', '같았어']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4. 단어장 만들기\n",
        "\n",
        "이제 이 토큰들을 가지고 단어장을 만들어 볼 텐데요. 여기서는 빈도수가 높은 단어일수록 낮은 정수를 부여하려고 합니다. 빈도수가 높은 순서대로 낮은 정수를 부여하기 위해서 각 단어의 빈도수를 카운트 할 필요가 있겠죠? 여기서는 파이썬의 Counter 서브클래스를 사용해서 단어의 빈도를 카운트해 보겠습니다\n",
        "\n",
        "[파이썬 collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter)"
      ],
      "metadata": {
        "id": "h9v1-Od0DSpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Counter(tokens)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrVQh2M3DjKE",
        "outputId": "8368784a-6202-4dbe-bea8-5a1d75a54f23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'귀': 4, '임금님': 2, '는': 2, '당나귀': 2, '실컷': 1, '소리': 1, '치고': 1, '나니': 1, '속이': 1, '확': 1, '뚫려': 1, '살': 1, '것': 1, '같았어': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어가 키(key)로, 단어에 대한 빈도수가 값(value)으로 저장되어 있네요. vocab에 단어를 입력하면 빈도수를 리턴합니다. '임금님'이 몇 번 등장했는지 빈도수를 출력해 볼까요?"
      ],
      "metadata": {
        "id": "feNnjwdwD_l9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab['임금님']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RtCuH2CDo9t",
        "outputId": "3f9c6a3b-2158-472e-c749-778d7e765634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'임금님'이란 단어가 총 2번 등장했네요. most_common()는 상위 빈도수를 가진 단어를 주어진 수만큼 리턴합니다. 이를 사용하여 등장 빈도수가 높은 단어들을 원하는 개수만큼 얻을 수 있습니다. 등장 빈도 수 상위 5개의 단어만 단어장으로 저장해 볼게요.\n",
        "\n"
      ],
      "metadata": {
        "id": "SspPkktBEfp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size) # 등장 빈도수가 높은 상위 5개의 단어만 저장\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE5tKLjyEs1F",
        "outputId": "ae956328-fbc6-4cc3-846a-a281d8768e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('귀', 4), ('임금님', 2), ('는', 2), ('당나귀', 2), ('실컷', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "빈도수 상위 5개의 단어만 남아 있는 것을 확인할 수 있습니다! 이제 높은 빈도수를 가진 단어일수록 낮은 정수 인덱스를 부여해 볼까요?"
      ],
      "metadata": {
        "id": "c3yIZ40kE2_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
        "print(word2idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcTC027xE37e",
        "outputId": "5f687543-3de1-49bb-863e-4336dc21e77c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'귀': 1, '임금님': 2, '는': 3, '당나귀': 4, '실컷': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: 원-핫 벡터 만들기\n",
        "\n",
        "이제 원-핫 인코딩을 하는 함수를 만들어 각 단어를 원-핫 벡터로 만들어볼게요. 아래의 함수는 특정 단어와 단어장을 입력하면 해당 단어의 원-핫 벡터를 리턴하는 함수에요."
      ],
      "metadata": {
        "id": "oT6n4V63FPoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoding(word, word2index):\n",
        "  one_hot_vector = [0] * (len(word2index))\n",
        "  index = word2index[word]\n",
        "  one_hot_vector[index - 1] = 1\n",
        "  return one_hot_vector"
      ],
      "metadata": {
        "id": "mS3BceNhFVUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_encoding('임금님', word2idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5TtC6qBFscr",
        "outputId": "c161f9ac-6985-40f7-fad7-7e7ccc037544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "케라스를 통한 원-핫 인코딩(one-hot encoding)\n",
        "\n",
        "원-핫 인코딩을 지원하는 패키지는 여러 가지가 있는데 여기서는 텐서플로의 케라스 API를 사용해볼 거에요. 우선 필요한 도구들을 임포트 해 보죠. 여기서는 단어장을 만드는 역할을 해주는 케라스 Tokenizer와 원-핫 인코딩을 위한 도구인 to_categorical을 사용합니다."
      ],
      "metadata": {
        "id": "KS7e2c-qGFO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "kytsNOFSGIFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = [['강아지', '고양이', '강아지'],['애교', '고양이'], ['컴퓨터', '노트북']]\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ISL59kuGKjh",
        "outputId": "e1766fe2-89bd-4e08-a9f5-d0cbfaa01184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['강아지', '고양이', '강아지'], ['애교', '고양이'], ['컴퓨터', '노트북']]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = Tokenizer()\n",
        "t.fit_on_texts(text)\n",
        "print(t.word_index) # 각 단어에 대한 인코딩 결과 출력."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnyFApo8GSSl",
        "outputId": "44755de9-f287-48e5-cfbf-c4d6a6650cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'강아지': 1, '고양이': 2, '애교': 3, '컴퓨터': 4, '노트북': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어장 크기 저장\n",
        "# vocab_size에 +1을 해주는 이유는 케라스 토크나이저는 각 단어에 고유한 정수를 부여할 때\n",
        "# 숫자 1부터 부여하지만 실제로 자연어 처리를 할 때는 특별 토큰으로 0번 단어로 단어장에 추가로 사용하는 경우가 많기 때문입니다.\n",
        "# 주로 0번은 패딩(padding) 작업을 위한 패딩 토큰으로 사용되는데, 여기서는 0번 단어도 고려해주는 것이 좋다는 정도로만 이해하고 1을 더해서 단어장의 크기를 저장하겠습니다. 이제 vocab_size는 6이 되었습니다.\n",
        "\n",
        "vocab_size = len(t.word_index) + 1\n"
      ],
      "metadata": {
        "id": "AQQ8ZocJGhj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "케라스 토크나이저에 단어장이 저장되었으므로, 단어장에 속한 단어들로 구성된 텍스트 시퀀스는 케라스 토크나이저를 통해 정수 시퀀스로 변환할 수 있습니다."
      ],
      "metadata": {
        "id": "QqzAfb-qG87X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub_text = ['강아지', '고양이', '강아지', '컴퓨터']\n",
        "encoded = t.texts_to_sequences([sub_text])\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xkIndU_G-r4",
        "outputId": "4d499847-a815-47a2-b530-555c720da3c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 1, 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 결과는 텍스트 시퀀스가 정수 시퀀스로 변환되는 것을 보여줍니다. 앞에서 강아지는 1번, 고양이는 2번, 컴퓨터는 4번으로 정수가 부여되었습니다. 이렇게 변환된 정수 시퀀스는 to_categorical()을 사용해 원-핫 벡터의 시퀀스로 변환할 수 있습니다."
      ],
      "metadata": {
        "id": "vOTCoLS9HJhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot = to_categorical(encoded, num_classes= vocab_size)\n",
        "print(one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d8885NcHLEt",
        "outputId": "8bb0debd-bf17-4b32-cbeb-0584cd2fc303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5-3. 워드 임베딩"
      ],
      "metadata": {
        "id": "RUr407IeHp1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 희소 벡터(Sparse Vector)의 문제점\n",
        "\n",
        "DTM, TF-IDF, 원-핫 벡터는 단어장의 크기에 영향을 받는 희소 벡터(sparse vector) 라는 특징을 가지고 있습니다. 원-핫 인코딩의 경우 단어장의 크기가 30,000이라면, 30,000개의 모든 단어 벡터는 각각 30,000 차원의 벡터가 되죠. 그런데 이 벡터들은 하나의 원소만 1이고 29,999개의 원소가 0의 값을 가집니다. \n",
        "\n",
        "희소 벡터에는 차원의 저주(curse of dimensionality) 라는 문제가 있습니다. 차원의 저주를 이해하기 위해서 직관적인 이야기로 시작해 볼게요. 같은 정보를 저차원과 고차원에 각각 표현한다고 해 보죠. 저차원에서는 정보의 밀도가 상대적으로 커지지만, 고차원에서는 정보가 흩어지며 밀도가 작아질 거예요. 아래의 그림은 이를 시각화하여 보여주고 있습니다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/seukeurinsyas_2021-09-09_15-30-49.png)\n",
        "<center>차원의 저주</center>\n",
        "\n",
        "정보 밀도가 작아지는 것, 즉 차원이 커지는 것과 머신 러닝 모델의 성능에는 어떤 연관 관계가 있습니다. 차원의 저주에 대해서는 아래 참고자료를 통해 개념을 정리해 봅시다.\n",
        "\n",
        "[차원의 저주 보충 설명](http://thesciencelife.com/archives/1001)\n",
        "\n",
        "Q2. 데이터의 차원이 커지면 더 많은 정보를 담을 수 있게 되므로 무조건 ML 모델의 성능이 올라간다! (O/X)\n",
        "- X, 차원이 커질수록 훨씬 더 많은 데이터를 가지고도 높은 성능에 이르지 못한다는 점입니다.\n",
        "\n",
        "원-핫 벡터가 가지는 문제점은 또 있습니다. 사람은 직관적으로 '강아지'와 '고양이'라는 두 단어의 의미적 유사성이 '강아지'와 '컴퓨터'라는 두 단어의 의미적 유사성보다는 높다고 판단할 수 있습니다. '강아지'와 '고양이'는 귀여운 애완동물이고, '컴퓨터'는 데이터를 처리하는 전자기기로 분류할 수 있기 때문입니다. 하지만 원-핫 벡터는 이를 반영하지 못합니다.\n",
        "\n",
        "벡터 간 유사도를 구하는 방법으로는 대표적으로 내적(inner product)이 있습니다. 임의의 두개의 원-핫 벡터 간 내적(inner product)을 구해보면, 대부분 서로 직교(orthogonal)하여 그 값은 0입니다. 이는 거의 모든 원-핫 벡터의 상호 유사도가 0임을 의미하며. 결국 원-핫 벡터를 통해서는 단어 벡터 간 유사도를 구할 수 없음을 의미합니다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/gn-3-l-3-1.max-800x600.jpg)\n",
        "<center>벡터 직교</center>\n",
        "\n",
        "기계가 단어 벡터 간 유사도를 구할 수 없다는 점은 오랫동안 자연어 처리의 걸림돌이었습니다. 어떤 사람이 '톺아보다'라는 생소한 단어를 배웠고, '톺아보다'가 '샅샅이 살펴보다'와 유사한 의미임을 학습했다고 해보죠. 그 사람은 이제 '발표 자료를 살펴보다'라는 문장을 '발표 자료를 톺아보다'라는 문장으로 어려움 없이 수정할 수 있습니다. 비록 '발표 자료를 톺아보다'라는 예문을 어디서 읽은 적이 없더라도, 의미적 유사성에 대한 이해를 통해 새로운 자연어를 생성한 셈입니다. 하지만 단어 벡터 간 유사도를 구할 수 없는 기계는 이와 같이 관측하지 못한 문장에 대해서 유연하게 대처할 수 없었습니다.\n",
        "\n",
        "이에 대한 대안으로 '기계가 단어장 크기보다 적은 차원의 밀집 벡터(dense vector)를 학습'하는 워드 임베딩(word embedding) 이 제안되었습니다. 이를 통해 얻는 밀집 벡터는 각 차원이 0과 1이 아닌 다양한 실숫값을 가지며, 이 밀집 벡터를 임베딩 벡터(embedding vector) 라고 합니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oGyLLxV6HswF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 워드 임베딩(Word Embedding)\n",
        "\n",
        "워드 임베딩에서도 한 단어를 벡터로 바꿉니다. 그런데 그 벡터의 길이를 일정하게 정해줍니다. 더 많은 단어가 있다고 해서 벡터의 길이가 길어지지 않습니다. 여기서 일반적으로 벡터의 길이가 단어장 크기보다 매우 작기 때문에 각 벡터 값에 정보가 축약되어야 하고 결국 밀집 벡터(dense vector)가 됩니다.\n",
        "\n",
        "밀집 벡터는 희소 벡터가 가진 특성을 반대로 표현하고 싶어서 만들어진 말인데요. 대부분 값이 0인 희소 벡터와는 반대로 밀집 벡터에서는 대부분 값이 0이 아닙니다. 또 희소 벡터에서는 각 벡터 값의 의미가 True/False나 갯수처럼 단순했다면 밀집 벡터에서는 각 벡터 값의 의미가 파악하기 어려울 정도로 많은 의미를 함축하고 있죠.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/gn-3-l-3-2.max-800x600.jpg)\n",
        "<center></center>\n",
        "\n",
        "또 워드 임베딩에서는 단어가 갖는 특성을 계산할 수 있는 방법이 제안됩니다. 단어 사이의 관계나 문장에서 단어가 갖는 특징을 수식으로 나타내고 계산해서 정확한 숫자로 나타내도록 하는 거죠. 이 값이 클수록(또는 작을 수록) 적합한 표현이라고 객관적으로 판단할 수 있게 말이에요. 여기서 사용되는 방법이나 수식은 꽤 다양합니다. 저마다 더 훌륭한 방법을 생각해 내는 것이죠.\n",
        "\n",
        "앞서 말한 내적을 활용하는 방법을 예로 들어도 좋습니다. 비슷한 의미를 가진 두 단어의 내적이 클수록 잘 변환된 벡터라고 판단하는 거죠. 그러면 비슷한 단어의 내적은 커지고 반대되는 단어의 내적은 작아지도록 만드는 식을 통해 벡터를 계산해 낼 수 있습니다. 이렇게 알아보기 쉬운 단순한 방법을 쓰면 좋겠지만 안타깝게도 실제로 벡터 변환에 이용되는 방법들은 주로 통계학에 기반을 둔 복잡한 식을 활용합니다.\n",
        "\n",
        "또, 시간이 흘러 이제는 인공 신경망을 이용한 방법이 많이 사용되는데요. 인공 신경망을 학습해가는 과정을 이용해 벡터의 값을 조정해 가는 방법입니다. 우선 무작정 단어의 적합성이나 유사도를 계산해 보고 그 값이 커져야 하는지 작아져야 하는지 따져봅니다. 그런 후 단어 벡터의 값을 조금씩 바꿔 갑니다. 마치 딥러닝에서 손실 값을 이용해 가중치를 바꿔 가듯이 단어 벡터의 값을 조금씩 바꿔 가는 거죠. 이렇게 반복해 학습이 끝나면 단어가 들어가야 할 위치나 의미에 맞게 단어 벡터의 값이 결정됩니다.\n",
        "\n",
        "예를 들면 아래처럼 단어 벡터가 결정이 될 수 있겠죠.\n",
        "\n",
        "//      [둥근,빨간,단맛,신맛]\n",
        "사과  : [0.8, 0.7, 0.7, 0.1] // 0.8만큼 둥글고, 0.7만큼 빨갛고, 0.7만큼 달고, 0.1만큼 신 것은 사과다\n",
        "\n",
        "바나나: [0.1, 0.0, 0.8. 0.0] // 0.1만큼 둥글고, 0.0만큼 빨갛고, 0.8만큼 달고, 0.0만큼 신 것은 바나나다\n",
        "\n",
        "귤    : [0.7, 0.5, 0.6, 0.5] // 0.7만큼 둥글고, 0.5만큼 빨갛고, 0.6만큼 달고, 0.5만큼 신 것은 귤이다\n",
        "\n",
        "이건 예시일 뿐이니 이렇다고 생각하면 절대 안됩니다. 임베딩 방법마다 벡터 값이 결정되는 방법도 다르고 각 벡터 값의 의미도 파악하기란 불가능합니다.\n",
        "\n",
        "결론적으로 워드 임베딩에서 중요한 것은 두 가지 입니다.\n",
        "\n",
        "- 한 단어를 길이가 비교적 짧은 밀집 벡터로 나타낸다.\n",
        "- 그런데 이 밀집 벡터는 단어가 갖는 의미나 단어 간의 관계 등을 어떤 식으로든 내포하고 있다.\n",
        "\n",
        "이렇게 만들어진 밀집 벡터를 임베딩 벡터라고 말하는데요. 지금까지 이야기를 보면 임베딩 벡터라는 결과물에 대해서만 이야기했습니다. 이제 어떻게 임베딩 벡터를 만들지 살펴볼 차례인데요. 워드 임베딩 방법에는 여러가지가 있기 때문에 지금까지 설명하지 않았던 것 뿐이에요.\n",
        "\n",
        "앞으로 워드 임베딩을 수행하는 여러 알고리즘을 살펴보게 될 것입니다.\n",
        "\n",
        "\n",
        "Q3. 임베딩 벡터의 값은 어떤 방식을 통해서 얻어지나요? (힌트 : 원-핫 벡터의 값은 각 단어에 정수를 맵핑해주고, 이로부터 인코딩한다는 점에 표현 방식이 '수동'이라고 볼 수 있습니다.)\n",
        "- 훈련 데이터로부터 어떤 모델을 학습하는 과정에서 '자동'으로 얻어지는데, 주로 언어 모델(Language Model)을 학습하는 가운데 얻어짐.\n",
        "\n",
        "[A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
        "\n",
        "워드 임베딩은 2003년 요슈아 벤지오(Yoshua Bengio) 교수가 NPLM(Neural Probabilistic Language Model) 이란 모델을 통해 제안했습니다. 하지만 당시 이 모델은 학습 속도가 지나치게 느리다는 단점이 있었고, 2013년 구글은 NPLM을 개선하여 정밀도와 속도를 향상시킨 Word2Vec 을 제안했죠. Word2Vec 이후로 FastText 나 GloVe 등과 같은 임베딩 방법이 추가로 제안되었답니다. 여기서는 방금 언급한 이 세 가지 방법에 대해서 배워보겠습니다. 우선 Word2Vec에 대해서 알아볼까요?"
      ],
      "metadata": {
        "id": "gKz3lXirLbPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5-4. Word2Vec (1) 분포 가설\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/word2vec.png)\n",
        "<center>Korean Word2Vec</center>\n",
        "\n",
        "[Korean Word2Vec](https://word2vec.kr/search/)\n",
        "\n",
        "[Word2Vec 영상](https://youtu.be/sY4YyacSsLc)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ikdHG4v4PFSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 분포 가설(Distributional Hypothesis)\n",
        "\n",
        "Word2Vec은 앞서 말했듯이 단어를 벡터로 표현하는 방법의 일종으로 저차원으로 이루어져 있고, 단어의 의미를 여러 차원에 분산하여 표현한 벡터입니다.\n",
        "\n",
        "Word2Vec의 핵심 아이디어는 분포 가설(distributional hypothesis) 을 따릅니다. 이 가설은 언어학자 존 루퍼트 퍼스(John Rupert Firth)의 다음 인용으로 설명됩니다.\n",
        "\n",
        "You shall know a word by the company it keeps(곁에 오는 단어들을 보면 그 단어를 알 수 있다).\n",
        "\n",
        "이 인용의 뜻은 어떤 단어들의 의미를 보려면 주변 단어들을 보라는 의미를 내포하고있다. \n",
        "\n",
        "- 분포 가설 : 비슷한 문맥에서 같이 등장하는 경향이 있는 단어들은 비슷한 의미를 가진다.\n",
        "\n",
        "분포 가설에 따르는 Word2Vec은 같이 등장하는 경향이 적은 단어들에 비해 '강아지', '애교, '귀여운'과 같은 단어들을 상대적으로 유사도가 높은 벡터로 만듭니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "ax62wHStPu1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5-5. Word2Vec (2) CBoW\n",
        "\n",
        "Word2Vec에는 크게 CBoW와 Skip-gram라는 두 가지 방법이 있습니다. CBoW는 주변에 있는 단어들을 통해 중간에 있는 단어들을 예측하는 방법입니다. 반대로, Skip-Gram은 중간에 있는 단어로 주변 단어들을 예측하는 방법입니다.\n"
      ],
      "metadata": {
        "id": "LU8hJafeQemQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. CBoW(Continuous Bag of words)\n",
        "\n",
        "- 예문 : \"I like natural language processing.\"\n",
        "\n",
        "이런 예문이 있다고 할 때, Cbow는 중간에 있는 단어를 예측하는 방법이므로 {\"i\", \"like\", \"language\", \"processing\"}으로부터 'natural'을 예측하는 것이 CBoW가 하는 일이다.\n",
        "\n",
        "이때 예측해야하는 단어를 중심 단어라고 하고, 예측에 사용되는 단어들을 주변 단어하고 한다.\n",
        "\n",
        "중심 단어를 예측하기 위해 앞, 뒤로 몇 개의 단어를 볼지를 결정를 결정했다면, 그 범위를 윈도우(window) 라고 합니다.\n",
        "\n",
        "만약 윈도우 크기가 1이고, 예측하고자 하는 중심 단어가 'language'라면 앞의 한 단어인 'natural'과 뒤의 한 단어인 'processing'을 참고합니다. 윈도우 크기가 m일 때, 중심 단어를 예측하기 위해 참고하는 주변 단어의 개수는 2m이다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/Untitled_23.png)\n",
        "<center>슬라이딩 윈도우</center>\n",
        "\n",
        "윈도우 크기를 정했다면, 윈도우를 계속 움직여서 주변 단어와 중심 단어를 바꿔가며 학습을 위한 데이터 셋을 만들 수 있는데, 이 방법을 슬라이딩 윈도우(sliding window) 라고 합니다. 위의 그림은 윈도우 크기가 1일 때, 하나의 샘플 문장에 대해 데이터셋을 만드는 과정을 보여줍니다. 위 그림에서 슬라이딩 윈도우를 처음부터 끝까지 마친다면 다음과 같은 데이터셋을 얻을 수 있습니다.\n",
        "\n",
        "\n",
        "- 아래 데이터셋의 형식은 ((주변 단어의 셋), 중심 단어)임을 가정한다.\n",
        "- ((like), I), ((I, natural), like), ((like, language), natural), ((natural, processing), language), ((language), processing)\n",
        "\n",
        "이렇게 선택된 데이터셋에서 단어 각각은 원-핫 인코딩되어 원-핫 벡터가 되고, 원-핫 벡터가 CBoW나 Skip-gram의 입력이 됩니다. CBoW를 시각화하면 아래처럼 됩니다. \n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/Untitled_24.max-800x600.png)\n",
        "<center>CBoW</center>\n",
        "\n",
        "위의 그림은 원-핫 벡터로 변환된 다수의 주변 단어를 이용해 원-핫 벡터로 변환된 중심 단어를 예측할 때의 CBoW의 동작 메커니즘을 보여주고 있습니다. 윈도우 크기가 m이라면 2m개의 주변 단어를 이용해 1개의 중심 단어를 예측하는 과정에서 두 개의 가중치 행렬(matrix)을 학습하는 것이 목적이죠.\n",
        "\n",
        "그림에서 주황색 사각형이 첫 번째 가중치 행렬 WW, 초록색 사각형이 두 번째 가중치 행렬 W'W \n",
        "′\n",
        " 입니다. 두 개의 가중치 행렬이 있다는 것을 인공 신경망 구조에서 보면 CBoW는 입력층, 은닉층, 출력층 이렇게 3개의 층으로만 구성된 인공 신경망이라는 의미입니다. 사실 Word2Vec은 은닉층이 1개라서 딥 러닝이라기보다는 얕은 신경망(Shallow Neural Network) 을 학습한다고 볼 수 있습니다.\n",
        "\n",
        "CBoW 신경망 구조에서 주변 단어 각각의 원-핫 벡터는 입력층에 위치하고 중심 단어의 원-핫 벡터가 위치한 곳은 출력층이라고 볼 수 있습니다. CBoW에서 사실 입력층과 출력층의 크기는 단어 집합의 크기인 VV로 이미 고정되어 있습니다. (원-핫 벡터로 표현되었기 때문이겠죠?) 하지만 은닉층의 크기는 사용자가 정의해주는 하이퍼파라미터인데요. 여기서는 은닉층의 크기를 NN이라고 해보겠습니다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/Untitled_25.max-800x600.png)\n",
        "<center>입력층에서 은닉층으로 가는 과정</center>\n",
        "\n",
        "주변 단어로 선택된 각각의 원-핫 벡터는 첫 번째 가중치 행렬과 곱해지게 됩니다. 이때 가중치 행렬의 크기는 (V × N)입니다. 그런데 원-핫 벡터는 각 단어의 정수 인덱스 i에 해당되는 위치에만 1의 값을 가지므로, 원-핫 벡터와 가중치 행렬과의 곱은 가중치 행렬의 i 위치에 있는 행을 그대로 가져오는 것과 동일 합니다.\n",
        "\n",
        "이를 마치 테이블에서 값을 그대로 룩업(lookup)해오는 것과 같다고 하여 룩업 테이블(lookup table) 이라고 합니다. 위의 그림은 가중치 행렬의 크기가 (5 × 4)일 때의 룩업 테이블을 보여줍니다. 위의 그림에서는 단어장의 크기가 5, 은닉층의 크기가 4일 때를 가정하고 있습니다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/cbow.png)\n",
        "<center>은닉층 연산</center>\n",
        "\n",
        "룩업 테이블을 거쳐서 생긴 2m개의 주변 단어 벡터들은 각각 N의 크기를 가집니다. CBoW에서는 이 벡터들을 모두 합하거나, 평균을 구한 값 을 최종 은닉층의 결과로 합니다. 그러면 최종 은닉층의 결과도 N차원의 벡터가 되겠죠. 이게 은닉층 연산의 전부입니다. Word2Vec에서는 은닉층에서 활성화 함수나 편향(bias)을 더하는 연산을 하지 않습니다.\n",
        "\n",
        "Word2Vec에서의 은닉층은 활성화 함수가 존재하지 않고, 단순히 가중치 행렬과의 곱셈만을 수행하기에 기존 신경망의 은닉층과 구분 지어 투사층(projection layer) 이라고도 합니다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/Untitled_26.png)\n",
        "<center>출력층 연산</center>\n",
        "\n",
        "은닉층에서 생성된 N차원의 벡터는 두 번째 가중치 행렬과 곱해집니다. 이 가중치 행렬의 크기는 (N × V)이므로, 곱셈의 결과로 나오는 벡터의 차원은 VV입니다. 출력층은 활성화 함수로 소프트맥스 함수를 사용하므로 이 VV차원의 벡터는 활성화 함수를 거쳐 모든 차원의 총합이 1이 되는 벡터로 변경됩니다.\n",
        "\n",
        "CBoW는 이 출력층의 벡터를 중심 단어의 원-핫 벡터와의 손실(loss)을 최소화 하도록 학습시킵니다. 이 과정에서 첫 번째 가중치 행렬 WW와 두 번째 가중치 행렬 W'가 업데이트되는데, 학습이 다 되었다면 N차원의 크기를 갖는 W의 행이나 W'의 열로부터 어떤 것을 임베딩 벡터로 사용할지를 결정하면 됩니다. 때로는 W와 W'의 평균치를 임베딩 벡터로 선택하기도 합니다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JVF_36a6Qo0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5-6. Word2Vec (3) Skip-gram과 Negative Sampling"
      ],
      "metadata": {
        "id": "bbnnLTCRV7BN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Skip-gram\n",
        "\n",
        "Skip-gram은 중심 단어로부터 주변 단어를 예측한다는 것이 다른 점이다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/Untitled_27.png)\n",
        "<center>슬라이딩 윈도우</center>\n",
        "\n",
        "앞서 봤던 CBoW에서 얻은 샘플 수는 5였다. 그런데 Skip-gram은 데이터셋 구성부터 다릅니다. 중심 단어로부터 주변 단어 각각을 예측하기 때문입니다. 위 그림으로부터 얻을 수 있는 Skip-gram의 데이터셋은 아래와 같다.\n",
        "\n",
        "- 아래 데이터셋의 형식은 (중심 단어, 주변 단어)임을 가정합니다.\n",
        "- (i, like) (like, I), (like, natural), (natural, like), (natural, language), (language, natural), (language, processing), (processing, language)\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/Untitled_28.max-800x600.png)\n",
        "<center>Skip-gram 과정</center>\n",
        "\n",
        "중심 단어로부터 주변 단어를 예측한다는 점, 그리고 이로 인해 중간에 은닉층에서 다수의 벡터의 덧셈과 평균을 구하는 과정이 없어졌다 는 점만 제외하면 CBoW와 메커니즘 자체는 동일합니다. skip-gram도 CBoW와 마찬가지로 학습 후에 가중치 행렬 WW의 행 또는 W'의 열로부터 임베딩 벡터를 얻을 수 있습니다.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_7LygRRBWCoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 네거티브 샘플링(negative sampling)\n",
        "\n",
        "대체적으로 Word2Vec를 사용할 때는 SGNS(Skip-Gram with Negative Sampling) 을 사용합니다. 즉 Skip-gram을 사용하면서 네거티브 샘플링(Negative Sampling) 이란 방법도 사용한다는 것이다. 앞서 배운 Word2Vec의 구조는 연산량이 지나치게 많아 실제로 사용하기 어렵다.\n",
        "\n",
        "Skip-gram의 학습 과정을 살펴봅시다. 모델 구조는 단순해 보이지만 복잡한 과정을 거칩니다. 출력층에서 소프트맥스 함수를 통과한 V 차원의 벡터와 레이블에 해당되는 V차원의 주변 단어의 원-핫 벡터와의 오차를 구하고, 역전파를 통해 모든 단어에 대한 임베딩 벡터을 조정합니다. 그 단어가 중심 단어나 주변 단어와 전혀 상관없는 단어라도요. 만약 단어장의 크기가 수십, 수백만에 달한다면 이 작업은 너무너무 느립니다! Output을 위한 소프트맥스(Softmax) 함수의 분모항이 수백만에 달하는 것만 생각해 보아도 충분히 예상할 수 있습니다.\n",
        "\n",
        "하지만 지금 집중하고 있는 중심 단어와 주변 단어가 '사과', '딸기'와 같이 과일과 관련된 단어라면, '필통', '연필'이라는 연관 관계가 없는 단어들의 임베딩 값을 굳이 업데이트할 필요가 없겠죠? 그래서 네거티브 샘플링은 연산량을 줄이기 위해서 소프트맥스 함수를 사용한 V개 중 1개를 고르는 다중 클래스 분류 문제 를 시그모이드 함수를 사용한 이진 분류 문제 로 바꾸기로 합니다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/Untitled_29.max-800x600.png)\n",
        "<center>Skip-gram</center>\n",
        "\n",
        "기존의 skip-gram은 위의 그림과 같이 중심 단어로부터 주변 단어를 예측하는 방식이었습니다. 그런데 네거티브 샘플링을 사용하면 이렇게 바뀝니다!\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/Untitled_29.max-800x600.png)\n",
        "<center>Negative Sampling</center>\n",
        "\n",
        "중심 단어와 주변 단어를 입력값으로 받아 이 두 단어가 정말로 이웃 관계(실제로 중심 단어와 주변 단어의 관계면)면 1을 또는 0을 출력하는 문제로 바꾸는 것\n",
        "\n",
        "즉 기존의 다중 분류 문제에서 이진 분류 문제로 바뀌는 것이다.\n",
        "\n",
        "기존 Skip-gram이 데이터셋 만드는 방식을 복습해보자.\n",
        "\n",
        "- 예문 : Thou shalt not make a machine in the likeness of a human mind\n",
        "\n",
        "윈도우 크기가 2일 때, 위 예문으로부터 슬라이딩 윈도우를 통해서 만들어지는 Skip-gram의 데이터 셋은 아래의 그림과 같다. skip-gram 방식이기에 input word는 중심 단어, target word는 주변 단어를 의미한다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/Untitled_31.max-800x600.png)\n",
        "<center>skip-gram의 데이터셋</center>\n",
        "\n",
        "만들어진 데이터셋에 새롭게 레이블을 달아준다면 다음과 같이 진행된다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/Untitled_32.max-800x600.png)\n",
        "<center>정상적인 데이터셋에는 1이라는 레이블을 달아줍니다.</center>\n",
        "\n",
        "위 그림에서 좌측은 기존의 데이터셋, 우측은 기존 데이터셋에 1로 레이블링을 해주었다. 1로 레이블링했다는 것은 0으로 레이블 해주는 경우도고 있다는 것이다. 0으로 레이블링 해주는 경우는 실제로 이웃 관계가 아닌 경우이다.\n",
        "\n",
        "랜덤으로 단어장에 있는 아무 단어나 가져와 target word로 하는 거짓 데이터셋을 만들고 0으로 레이블링을 해주는 것 이죠! 거짓(negative) 데이터셋을 만들기 때문에 이 방법이 네거티브 샘플링 이라 불립니다. 아래의 그림은 거짓 데이터셋을 만드는 과정을 보여줍니다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/Untitled_33.max-800x600.png)\n",
        "<center>거짓 데이터셋을 만드는 과정</center>\n",
        "\n",
        "이렇게 완성된 데이터셋으로 학습하면 Word2Vec은 더 이상 다중 클래스 분류 문제가 아니라 이진 분류 문제로 간주할 수 있습니다. 중심 단어와 주변 단어를 내적하고, 출력층의 시그모이드 함수를 지나게 하여 1 또는 0의 레이블로부터 오차를 구해서 역전파를 수행합니다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/Untitled_34.max-800x600.png)\n",
        "<center>내적, 시그모이드 함수, 오차, 역전파</center>\n",
        "\n",
        "이런 학습 방식은 기존의 소프트맥스 함수를 사용했던 방식보다 상당량의 연산량을 줄일 수 있는 효과를 가지고 있습니다! 이 아이디어는 엄청난 연산량을 필요로 했던 Word2Vec의 학습이 가능케 했던 핵심적인 아이디어 중 하나입니다. 다양한 분야에 손쉽게 응용할 수 있는 매력적인 아이디어이므로 잘 알아둡시다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dGpVY0L3X00p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5-7. Word2Vec (4) 영어 Word2Vec 실습과 OOV 문제"
      ],
      "metadata": {
        "id": "dUnGoLfxeWH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 영어 Word2Vec 실습\n",
        "\n",
        "영어 데이터를 다운로드받아 직접 Word2Vec을 훈련시켜보겠습니다. Word2Vec을 별도로 구현할 필요없이 파이썬의 gensim 패키지를 통해 이미 구현된 Word2Vec 모델을 사용할 수 있습니다. 여기서 사용할 훈련 데이터는 NLTK에서 제공하는 코퍼스이며, gensim 패키지는 토픽 모델링을 위한 NLP 패키지입니다."
      ],
      "metadata": {
        "id": "AT6kGhhqewWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK에 내장된 코퍼스 다운로드\n",
        "import nltk\n",
        "nltk.download('abc')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcZSQWy-etvK",
        "outputId": "e796e1db-535a-4efe-e3a3-4052183f2773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/abc.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK의 코퍼스를 불러와 corpus라는 변수에 저장\n",
        "from nltk.corpus import abc\n",
        "corpus = abc.sents()"
      ],
      "metadata": {
        "id": "onP-QvKyfSKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 코퍼스 출력\n",
        "print(corpus[:3])\n",
        "print(len(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE1aVwZhfdww",
        "outputId": "b288627c-dd9d-4d2f-834d-fe9f6482ace0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['PM', 'denies', 'knowledge', 'of', 'AWB', 'kickbacks', 'The', 'Prime', 'Minister', 'has', 'denied', 'he', 'knew', 'AWB', 'was', 'paying', 'kickbacks', 'to', 'Iraq', 'despite', 'writing', 'to', 'the', 'wheat', 'exporter', 'asking', 'to', 'be', 'kept', 'fully', 'informed', 'on', 'Iraq', 'wheat', 'sales', '.'], ['Letters', 'from', 'John', 'Howard', 'and', 'Deputy', 'Prime', 'Minister', 'Mark', 'Vaile', 'to', 'AWB', 'have', 'been', 'released', 'by', 'the', 'Cole', 'inquiry', 'into', 'the', 'oil', 'for', 'food', 'program', '.'], ['In', 'one', 'of', 'the', 'letters', 'Mr', 'Howard', 'asks', 'AWB', 'managing', 'director', 'Andrew', 'Lindberg', 'to', 'remain', 'in', 'close', 'contact', 'with', 'the', 'Government', 'on', 'Iraq', 'wheat', 'sales', '.']]\n",
            "29059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 라이브러리 환경 불러오기\n",
        "\n",
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "my_path = '/content/notebooks'\n",
        "# Colab Notebooks 안에 my_env 폴더에 패키지 저장\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks/my_env', my_path)\n",
        "sys.path.insert(0, my_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joJFOdK2e1dm",
        "outputId": "b0242876-47b2-401c-c465-6e1a21915a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gensim을 사용하여 Word2Vec 학습하기\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(sentences = corpus, vector_size = 100, window = 5, min_count = 5, workers = 4, sg = 0)"
      ],
      "metadata": {
        "id": "HawO9eZkfPeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 코드에서 파라미터의 의미\n",
        "\n",
        "- vector size = 학습 후 임베딩 벡터의 차원\n",
        "- window = 컨텍스트 윈도우 크기\n",
        "- min_count = 단어 최소 빈도수 제한 (빈도가 적은 단어들은 학습하지 않아요.)\n",
        "- workers = 학습을 위한 프로세스 수\n",
        "- sg = 0은 CBoW, 1은 Skip-gram.\n",
        "\n",
        "아주 잠깐의 기다림 끝에 Word2Vec의 학습이 완료됩니다. Word2Vec는 입력한 단어에 대해서 가장 코사인 유사도가 높은 단어들을 출력하는 model.wv.most_similar를 지원합니다. 'man'과 가장 유사한 단어들은 어떤 단어들일까요?\n"
      ],
      "metadata": {
        "id": "No1ntYDFgNVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_result = model.wv.most_similar(\"man\")\n",
        "print(model_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z5tX7GtgbMI",
        "outputId": "e9e15fb0-8682-47f4-d6af-5bb8eadba48e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('woman', 0.9233596324920654), ('skull', 0.9109037518501282), ('Bang', 0.905676543712616), ('asteroid', 0.9052280187606812), ('third', 0.901983916759491), ('baby', 0.8994574546813965), ('dog', 0.8985607028007507), ('bought', 0.8975837826728821), ('rally', 0.8911102414131165), ('dinosaur', 0.8889824748039246)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "일반적으로 더 많은 훈련 데이터를 사용하면 사용할수록 더 좋은 성능을 얻을 수 있답니다. 자, 이렇게 공들여 학습한 모델을 저장해두었다가 필요할 때 로드하면 더욱 좋겠죠? 이번에는 모델을 저장하고 로드하는 방법을 배워보겠습니다."
      ],
      "metadata": {
        "id": "y_SmOv9ngfG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장하고 로드하기\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model.wv.save_word2vec_format('/content/drive/MyDrive/Aiffel/GD_theory/w2v')\n",
        "loaded_model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Aiffel/GD_theory/w2v')"
      ],
      "metadata": {
        "id": "h12ntYmFggWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 다시 테스트해보기\n",
        "\n",
        "model_result = loaded_model.most_similar(\"man\")\n",
        "print(model_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO8RK808hnOX",
        "outputId": "db9f4c67-f1d8-4f2c-fd2d-9b9b3510d296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('woman', 0.9233596324920654), ('skull', 0.9109037518501282), ('Bang', 0.905676543712616), ('asteroid', 0.9052280187606812), ('third', 0.901983916759491), ('baby', 0.8994574546813965), ('dog', 0.8985607028007507), ('bought', 0.8975837826728821), ('rally', 0.8911102414131165), ('dinosaur', 0.8889824748039246)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Word2Vec의 OOV 문제\n",
        "\n",
        "Word2Vec은 Bag of words 학습 과정에서 언급되었던 문제인 사전에 없는 단어(Out Of Vocabuary) 문제를 그대로 가지고 있습니다. 다시 말해, 사전에 없는 단어에 대해서 Word2Vec은 임베딩 벡터값을 얻을 수 없습니다. "
      ],
      "metadata": {
        "id": "3LMFGzbNhsag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OOV((Out Of Vocabuary) 문제 확인\n",
        "\n",
        "loaded_model.most_similar('overacting')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "0Z53QnEZh1np",
        "outputId": "4dba257a-b5b6-43d8-8112-9554978dbf31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-c27f7f78ab96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# OOV((Out Of Vocabuary) 문제 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overacting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/notebooks/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m         all_keys = [\n\u001b[1;32m    844\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/notebooks/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'overacting' not present in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어장에 없는 단어를 넣으면 코드는 에러를 일으키고 'Key 'overacting' not present in vocabulary'라는 KeyError가 출력된다. \n",
        "\n",
        "그리고 오타가 있는 데이터를 넣어도 위와 같은 오류메시지가 출력된다."
      ],
      "metadata": {
        "id": "ogAn4tAGh8z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 오타가 있는 단어 넣기\n",
        "loaded_model.most_similar('memorry')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "LqaagUKSiViK",
        "outputId": "f103f052-3975-4b50-e5cb-514679475b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-1ee1295916c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 오타가 있는 단어 넣기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'memorry'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/notebooks/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m         all_keys = [\n\u001b[1;32m    844\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/notebooks/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'memorry' not present in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5-8. 임베딩 벡터의 시각화\n",
        "\n",
        "이번에는 구글이 공개한 임베딩 벡터의 시각화 오픈소스인 임베딩 프로젝터(embedding projector) 를 사용해서 임베딩 벡터들을 시각화해 보겠습니다. 임베딩 프로젝터를 통해서 어떤 임베딩 벡터들이 가까운 거리에 군집이 되어 있고, 특정 임베딩 벡터와 유클리드 거리나 코사인 유사도가 높은지 확인할 수 있습니다!"
      ],
      "metadata": {
        "id": "TSMntvwDixEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 필요한 파일 만들기\n",
        "\n",
        "임베딩 프로젝터를 통해서 임베딩 벡터를 시각화하기 위해서는 이미 저장된 모델이 필요해요. 이미 저장된 모델로부터 벡터값이 저장된 파일과 메타파일을 얻어야 하거든요. 앞서 'w2v'란 이름으로 모델을 저장했었죠? 아래 커맨드를 실행해 보겠습니다.\n",
        "\n",
        "[참고: gensim scripts.word2vec2tensor](https://radimrehurek.com/gensim/models/word2vec.html)"
      ],
      "metadata": {
        "id": "wrxtsSeSi-6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# w2v 메타데이터 및 텐서데이터 생성\n",
        "\n",
        "!python -m gensim.scripts.word2vec2tensor --input /content/drive/MyDrive/Aiffel/GD_theory/w2v --output /content/drive/MyDrive/Aiffel/GD_theory/w2v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hByOB12Di8m_",
        "outputId": "b7b9428a-f234-449d-80b7-8844b1f6bb2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-10-02 03:32:57,248 - word2vec2tensor - INFO - running /usr/local/lib/python3.7/dist-packages/gensim/scripts/word2vec2tensor.py --input /content/drive/MyDrive/Aiffel/GD_theory/w2v --output /content/drive/MyDrive/Aiffel/GD_theory/w2v\n",
            "2022-10-02 03:32:57,248 - utils_any2vec - INFO - loading projection weights from /content/drive/MyDrive/Aiffel/GD_theory/w2v\n",
            "2022-10-02 03:32:58,161 - utils_any2vec - INFO - loaded (10363, 100) matrix from /content/drive/MyDrive/Aiffel/GD_theory/w2v\n",
            "2022-10-02 03:32:59,600 - word2vec2tensor - INFO - 2D tensor file saved to /content/drive/MyDrive/Aiffel/GD_theory/w2v_tensor.tsv\n",
            "2022-10-02 03:32:59,601 - word2vec2tensor - INFO - Tensor metadata file saved to /content/drive/MyDrive/Aiffel/GD_theory/w2v_metadata.tsv\n",
            "2022-10-02 03:32:59,603 - word2vec2tensor - INFO - finished running word2vec2tensor.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 임베딩 프로젝터에 tsv 파일 업로드하기\n",
        "\n",
        "[Embedding Projector](https://projector.tensorflow.org/)\n",
        "\n",
        "임베딩 프로젝터 사이트는 좌측 상단을 통해 데이터를 업로드하고, 시각화 결과를 중앙에서 볼 수 있으며, 우측에서 거리나 유사도에 대한 파라미터를 조작할 수 있는 구조입니다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/Untitled_35.png)\n",
        "<center></center>\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/Untitled_36.max-800x600.png)\n",
        "<center></center>\n",
        "\n",
        "설명을 읽어보면 Step 1에는 각각의 벡터값이 저장된 tsv 파일을 업로드하고, Step2에는 메타 데이터의 tsv 파일을 업로드하라고 되어 있습니다. 다시 말해 각 임베딩 벡터의 이름을 말합니다.\n",
        "\n",
        "위에 있는 Choose file 버튼을 누르고 w2v_tensor.tsv 파일을 업로드하고, 아래에 있는 Choose file 버튼을 누르고 w2v_metadata.tsv 파일을 업로드합니다. 두 파일을 업로드하면 임베딩 프로젝터에 학습했던 워드 임베딩 모델이 시각화됩니다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/Untitled_37.max-800x600.png)\n",
        "<center></center>\n",
        "\n",
        "우측에 Search 버튼 또는 그래프의 포인트를 클릭해 원하는 단어를 선택하고, neighbors에 몇 개까지의 이웃을 검색할지 선택합니다. distance에서 COSINE 또는 EUCLIDEAN을 통해서 거리 측정 메트릭을 코사인 유사도로 할 것인지, 유클리드 거리로 할 것인지 선택할 수 있습니다. 그 외에도 중앙에 있는 화면에 마우스를 통해서 화면을 돌려보며 임베딩 벡터들의 군집을 확인해보세요!"
      ],
      "metadata": {
        "id": "FFtWa9BtjcRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5-9. FastText\n",
        "\n",
        "페이스북에서 개발한 FastText는 Word2Vec 이후에 등장한 워드 임베딩 방법으로, 메커니즘 자체는 Word2Vec을 그대로 따르고 있지만, 문자 단위 n-gram(character-level n-gram) 표현을 학습한다는 점에서 다릅니다. Word2Vec은 단어를 더 이상 깨질 수 없는 단위로 구분하는 반면, FastText는 단어 내부의 내부 단어(subwords)들을 학습한다는 아이디어를 가지고 있습니다.\n",
        "\n",
        "FastText의 n-gram에서 n은 단어들이 얼마나 분리되는지 결정하는 하이퍼파라미터입니다. n을 3으로 잡은 트라이그램(tri-gram)의 경우, 단어 \"partial\"은 'par', 'art', 'rti', 'tia', 'ial'로 분리하고 이들을 벡터로 만듭니다. 더 정확히는 시작과 끝을 의미하는 <, >를 도입하여 <pa, par, art, rti, tia, ial, al>라는 6개의 내부 단어(subword) 토큰을 벡터로 만듭니다. 여기에 추가적으로 하나를 더 벡터화하는데, 기존 단어에 <, 와 >를 붙인 토큰 <partial>입니다.\n",
        "\n",
        "즉 n = 3인 경우, FastText는 단어 partial에 대해 임베딩되는 n-gram 토큰들은 다음과 같습니다\n",
        "\n",
        "# n = 3인 경우\n",
        "<pa, par, art, rti, tia, ial, al>, <partial>\n",
        "\n",
        "실제 사용할 때는 n의 최솟값과 최댓값으로 범위를 설정할 수 있는데, gensim 패키지에서는 기본값으로 각각 3과 6으로 설정되어 있습니다. 다시 말해 최솟값 = 3, 최댓값 = 6인 경우라면, 단어 \"partial\"에 대해서 FastText는 아래 내부 단어들을 벡터화합니다.\n",
        "\n",
        "# n = 3 ~ 6인 경우\n",
        "<pa, par, art, rti, ita, ial, al>, <par, arti, rtia, tial, ial>, <part, ...중략... , <partial>\n",
        "\n",
        "여기서 내부 단어들을 벡터화한다는 의미는 저 단어들 각각에 대해서 Word2Vec을 수행한다는 의미입니다. 최종적으로 이렇게 벡터화된 n-gram 벡터들의 총합을 해당 단어의 벡터로 취합니다\n",
        "\n",
        "# 각 원소는 벡터임을 가정함\n",
        "partial = <pa + par + art + rti + ita + ial + al> + <par + arti + rtia + tial + ial> + <part + ...중략...  + <partial>\n",
        "\n",
        "1. FastText의 학습 방법\n",
        "\n",
        "사실 FastText의 학습 방식은 Word2Vec와 크게 다르지 않습니다. FastText도 Word2Vec과 마찬가지로 네거티브 샘플링을 사용하여 학습합니다. \"(중심 단어, 주변 단어)\"의 쌍을 가지고 이 쌍이 포지티브인지 네거티브인지 예측을 진행하는 것이다. 다만, Word2Vec과 다른 점은 학습 과정에서 중심 단어에 속한 문자 단위 n-gram 단어 벡터들을 모두 업데이트한다는 점입니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZN3g9Utzkwyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. OOV와 오타에 대한 대응\n",
        "\n",
        "FastText는 Word2Vec과 달리 OOV와 오타에 강건하다(robust)는 특징이 있다. 이는 단어장에 없는 단어라도, 해당 단어의 n-gram이 다른 단어에 존재하면 이로부터 벡터값을 얻는다는 원리에 기인한다."
      ],
      "metadata": {
        "id": "_MOBdJSInBOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FastText 라이브러리 불러오기\n",
        "\n",
        "from gensim.models import FastText\n",
        "\n",
        "fasttext_model = FastText(corpus, window=5, min_count=5, workers=4, sg=1)"
      ],
      "metadata": {
        "id": "M3VmAs7nnTLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이전에 오류를 일으켰던 단어 넣어보기\n",
        "\n",
        "fasttext_model.wv.most_similar('overacting')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_aUOHfInfuX",
        "outputId": "4412e6cf-58c9-4a4b-e56d-309df5ce7c5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('extracting', 0.9440352916717529),\n",
              " ('lifting', 0.9377554059028625),\n",
              " ('attracting', 0.9313859343528748),\n",
              " ('fluctuating', 0.9305419921875),\n",
              " ('contracting', 0.9289496541023254),\n",
              " ('climbing', 0.9288549423217773),\n",
              " ('emptying', 0.9277371168136597),\n",
              " ('circulating', 0.9274292588233948),\n",
              " ('shifting', 0.9272541999816895),\n",
              " ('debilitating', 0.9255145192146301)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 이전에 오류를 일으켰던 단어 넣어보기\n",
        "\n",
        "fasttext_model.wv.most_similar('memoryy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3gIeu7wnjGV",
        "outputId": "5ba3e3a9-d171-4647-96b2-d7e1e2e0eb62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('memory', 0.9509990215301514),\n",
              " ('musical', 0.884731113910675),\n",
              " ('music', 0.870802640914917),\n",
              " ('interactive', 0.863287627696991),\n",
              " ('intelligence', 0.8622024655342102),\n",
              " ('intermediate', 0.8600305318832397),\n",
              " ('basic', 0.8584713339805603),\n",
              " ('cognitive', 0.8578050136566162),\n",
              " ('mechanism', 0.8570135235786438),\n",
              " ('mechanisms', 0.8550618886947632)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 한국어에서의 FastText\n",
        "\n",
        "한국어도 당연히 FastText 방식으로 학습시킬 수 있습니다. 영어의 경우 문자, 즉 알파벳 단위가 n-gram이었다면 한국어의 경우에는 음절 단위라고 볼 수 있겠네요.\n",
        "\n",
        "(1) 음절 단위 FastText\n",
        "n = 3일때, 단어 '텐서플로우'의 트라이그램 벡터들은 어떻게 구성될까요?\n",
        "\n",
        "<텐서, 텐서플, 서플로, 플로우, 로우>, <텐서플로우>\n",
        "한국어에서 FastText가 빛을 발하는 것은 사실 음절 단위라기보다는 자소 단위인 경우입니다. 한국어의 자소를 각각의 문자로 간주한 경우에 FastText는 꽤 잘 동작한다고 알려져 있습니다.\n",
        "\n",
        "(2) 자소 단위 FastText\n",
        "단어에 대해서 초성, 중성, 종성을 분리한다고 하고, 종성이 존재하지 않는 경우에는 _라는 토큰을 대신 사용한다면 어떨까요? n = 3일 때, 단어 '텐서플로우'는 다음과 같이 트라이그램 벡터들로 분리할 수 있습니다. <ㅌㅔ,ㅌㅔㄴ,ㅔㄴㅅ,ㄴㅅㅓ,ㅅㅓ_, ...중략... >\n",
        "\n",
        "한국어에 FastText가 어떻게 적용될 수 있었는지에 대해서 아래의 글을 읽어보고, 퀴즈를 풀어보도록 하자.\n",
        "\n",
        "[한국어를 위한 어휘 임베딩의 개발 -2-](https://brunch.co.kr/@learning/8)\n",
        "\n",
        "Q5. 프랑스와 같은 고유 명사는 분해해도 이득을 볼 수 없는데 그 이유는 무엇일까요?\n",
        "- 어휘를 분해해도 의미적인 특징을 잡아낼 수 없기 때문이다.\n",
        "\n",
        "- 한국어 어휘를 자모 수준으로 분리하여 학습하는 것은 일부 어휘의 특성을 학습하는 데 큰 도움이 되지 않거나 가끔은 성능의 저하를 일으키기 때문입니다."
      ],
      "metadata": {
        "id": "jbhIm3ywnkvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5-10. GloVe\n",
        "\n",
        "글로브(Global Vectors for Word Representation, GloVe) 는 2014년에 미국 스탠포드 대학에서 개발한 워드 임베딩 방법론입니다. 워드 임베딩의 두 가지 접근 방법인 카운트 기반과 예측 기반 두 가지 방법을 모두 사용했다 는 것이 특징입니다.\n",
        "\n",
        "카운트 기반 방법이란 단어의 빈도를 수치화하여 표현하는 방법이다. 이런 방법은 단어 간 유사도를 반영할 수 없을 뿐만 아니라, 대부분의 값이 0인 희소 표현이라는 특징이 있다. DTM을 차원 축소하여 밀집 표현으로 임베딩 하는 바법이 LSA(Latent Semantic Analysis)\n"
      ],
      "metadata": {
        "id": "l_o-IV5RwCpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 잠재 의미 분석(LSA, Latent Semantic Analysis)\n",
        "\n",
        "[SVD와 PCA, 그리고 잠재의미분석(LSA)](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/06/pcasvdlsa/)\n",
        "\n",
        "[LSA 영상](https://youtu.be/GVPTGq53H5I)\n",
        "\n",
        "Q6. 행렬 A에 대해서 특이값 분해의 변형인 thin SVD, compact SVD, Truncated SVD를 각각 사용하였을 때, 이 3개의 특이값 분해 중 행렬 A를 복원할 수 있는 특이값 분해는 무엇일까요?\n",
        "- thin SVD, compact SVD 두 개는 행렬 A를 복원할 수 있다\n",
        "\n",
        "LSA를 요약하면 DTM에 특잇값 분해를 사용하여 잠재된 의미를 이끌어내는 방법론 입니다. 그 결과의 행벡터를 사용해서 임베딩 벡터를 얻을 수도 있다. LSA는 단어를 카운트해서 만든 DTM을 입력으로 하므로 카운트 기반의 임베딩 방법이라고 볼 수 있는데, 이 방법은 몇 가지 한계가 있다.\n",
        "\n",
        "(1) 차원 축소의 특성으로 인해 새로운 단어가 추가되면 다시 DTM을 만들어 새로 차원 축소를 해야 한다.\n",
        "\n",
        "(2) 단어 벡터간 유사도를 계산하는 측면에서 Word2Vec보다 성능이 떨어진다\n",
        "\n",
        "반면, LSA와 대조되는 방법으로 예측 기반의 방법 은 Word2Vec과 같은 방법을 말합니다. Word2Vec은 인공 신경망이 예측한 값으로부터 실제 레이블과의 오차를 구하고, 손실 함수를 통해서 인공 신경망을 학습하는 방식이었죠. \n",
        "\n",
        "GloVe 연구진은 Word2Vec의 경우에는 LSA보다 단어 벡터 간 유사도를 구하는 능력은 뛰어나지만, LSA처럼 코퍼스의 전체적인 통계 정보를 활용하지는 못한다는 점을 한계로 지적했습니다. \n",
        "\n",
        "그리고는 카운트 기반과 예측 기반을 모두 사용하여 Word2Vec보다 더 나은 임베딩 방법을 제안하였는데, GloVe가 그 주인공입니다. \n",
        "\n",
        "하지만 경험적으로 봤을 때, GloVe가 Word2Vec보다 반드시 뛰어나다고 장담하기는 어렵고, Word2Vec에 거의 준하는 성능을 보여준다고 평가되고 있습니다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Tm5QjimZxEtQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 윈도우 기반 동시 등장 행렬(Window based Co-occurrence Matrix)\n",
        "\n",
        "GloVe를 이해하기 위해서는 윈도우 기반 동시 등장 행렬의 정의를 이해하여야 한다.\n",
        "\n",
        "다음과 같은 예문이있다.\n",
        "\n",
        "- I like deep learning.\n",
        "- I like NLP.\n",
        "- I enjoy flying.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/Cap_2020-03-12_20-48-35-686.max-800x600.png)\n",
        "<center>윈도우 크기 1인 동시 등장 행렬</center>\n",
        "\n",
        "윈도우 기반 동시 등장 행렬은 행과 열을 전체 단어장(vocabulary)의 단어들로 구성하고, 어떤 i 단어의 윈도우 크기(window Size) 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬 입니다. 위의 경우에는 윈도우 크기를 1로 하였습니다. 이러한 동시 등장 행렬은 전치(transpose)해도 동일한 행렬이 된다는 특징을 가지고 있습니다."
      ],
      "metadata": {
        "id": "rOzXTzp1yTlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 동시 등장 확률(Co-occurrence Probability)\n",
        "\n",
        "동시 등장 행렬에 대해서 이해했다면, 동시 등장 확률에 대해서 이해해봅시다. 동시 등장 확률 P (k|i)P(k∣i) 는 동시 등장 행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고, 특정 단어 i가 등장했을 때 어떤 단어 k가 등장한 횟수를 카운트하여 계산한 조건부 확률입니다. 이때 i를 중심 단어(center word), k를 주변 단어(context word) 라고 합니다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/Untitled_38.max-800x600.png)\n",
        "<center>동시 등장 확률표</center>\n",
        "\n",
        "위 동시 등장 확률 표는 실제 GloVe 논문에 제시되었던 표입니다. 위의 표를 통해 알 수 있는 사실은 ice가 등장했을 때 solid가 등장할 확률은 large인 반면, steam이 등장했을 때 solid가 등장할 확률은 small이라는 점입니다. 이는 solid는 '단단한'이라는 의미를 가졌으니까 '증기'라는 의미를 가지는 steam보다는 당연히 '얼음'이라는 의미를 가지는 ice라는 단어와 더 자주 등장하기 때문입니다.\n",
        "\n",
        "{P(solid | ice)} / {P(solid | steam)} ​를 계산한 값은 1보다는 매우 큰 값입니다. 그 이유는 P(solid | ice)P(solid∣ice)의 값은 크고, P(solid | steam)P(solid∣steam)의 값은 작기 때문입니다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k5xaFrnZzb6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. GloVe의 손실 함수 설계하기\n",
        "\n",
        "GloVe는 동시 등장 행렬로부터 계산된 동시 등장 확률을 이용해 손실 함수를 설계합니다. 동시 등장 행렬을 사용하고 있으니 코퍼스의 전체적인 통계 정보를 활용하는 '카운트 기반'의 방법론이면서, 손실 함수를 통해 모델을 학습시키므로 '예측 기반'의 방법론이라고 할 수 있는 것이죠.\n",
        "\n",
        "GloVe의 아이디어 한 줄 요약\n",
        "- 중심 단어 벡터와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 빈도의 로그값이 되도록 만드는 것\n",
        "\n",
        "다시 표현하면\n",
        "- 전체 코퍼스에서 동시 등장 빈도의 로그값과 중심 단어 벡터와 주변 단어 벡터의 내적값의 차이가 최소화되도록 두 벡터의 값을 학습하는 것\n",
        "\n",
        "사실 내적이라는 것은 두 단어의 유사도를 측정하는 메트릭 중 하나이기 때문에, 이를 동시 등장 확률 또는 빈도와 연관 지어서 값을 학습한다는 것이 GloVe의 아이디어라고 볼 수 있겠습니다.\n",
        "\n",
        "손실 함수를 이해하기 위해서 GloVe의 변수들을 다음과 같이 정의합니다\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/Untitled_39.png)\n",
        "<center>GloVe 변수 설명</center>\n",
        "\n",
        "이때, GloVe의 손실 함수는 다음과 같습니다.\n",
        "\n",
        "![GloVe 손실함수.PNG](data:image/png;base64,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)\n",
        "\n",
        "위 수식에서 우측의 괄호를 보면 중심 단어와 주변 단어 벡터의 내적이 동시 등장 빈도의 로그값과의 차이를 줄이도록 설계되었음을 볼 수 있습니다. \n",
        "\n",
        "중심 단어 i가 등장했을 때, 윈도우 내 주변 단어 j가 등장하는 횟수인 Xij를 입력으로 하는 함수 f(Xij)항의 의도는 가중치를 주기 위한 것이다.\n",
        "\n",
        "GloVe의 연구진은 동시 등장 행렬에서 동시 등장 빈도의 값 f(Xik)이 굉장히 낮은 경우에는 거의 도움이 되지 않는 정보라고 판단했습니다. 그래서 이에 대한 가중치를 주기 위해서 GloVe 연구진이 선택한 것은 바로 f(Xik)의 값에 영향을 받는 가중치 함수(Weighting function) 를 도입하는 것입니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZET1DuB50GKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. pre-trained GloVe 모델 실습\n",
        "\n",
        "위에서도 언급했듯이 GloVe는 2014년에 개발되었고, 2015년에 1.2 버전이 나온 이후로는 관리되지 않고 있기 때문에 최신 버전의 python에서는 GloVe를 설치하는 것이 불가능합니다. 따라서 pre-trained된 GloVe 모델을 불러와서 간단한 실습을 해보겠습니다.\n",
        "\n",
        "GloVe에는 4개의 데이터셋을 사용해 pre-trained된 word vector들이 있습니다.\n",
        "\n",
        "- Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download)\n",
        "- Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download)\n",
        "- Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 300d vectors, 822 MB download)\n",
        "- Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 200d vectors, 1.42 GB download)\n",
        "\n",
        "gensim을 이용해 pre-train된 Glove 모델을 간단히 불러올 수 있습니다. 여기서는 'Wikipedia 2014 + Gigaword 5' 데이터셋으로 학습된 모델을 불러오겠습니다.\n"
      ],
      "metadata": {
        "id": "MNqH-m4a2n07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "glove_model = api.load('glove-wiki-gigaword-50') # glove vectors 다운로드\n",
        "glove_model.most_similar('dog') # 'dog'과 비슷한 단어 찾기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9BYf-ye2-iV",
        "outputId": "fe38fbdf-d76d-4f35-f736-ec0b41acad1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cat', 0.9218004941940308),\n",
              " ('dogs', 0.8513158559799194),\n",
              " ('horse', 0.7907583713531494),\n",
              " ('puppy', 0.7754920721054077),\n",
              " ('pet', 0.7724708318710327),\n",
              " ('rabbit', 0.7720814347267151),\n",
              " ('pig', 0.7490062117576599),\n",
              " ('snake', 0.7399188876152039),\n",
              " ('baby', 0.7395570278167725),\n",
              " ('bite', 0.7387937307357788)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_model.most_similar('memoryy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "DcEHBF-B3b6G",
        "outputId": "227f3d07-e30e-4057-aeac-2b162d67cec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ebdc0ddda607>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mglove_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'memoryy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/notebooks/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m         all_keys = [\n\u001b[1;32m    844\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/notebooks/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    517\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'memoryy' not present in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe는 Word2Vec과 같이 OOV 문제를 가지고 있어서 'memoryy'라는 단어는 인식하지 못합니다. 또한 pre-trained GloVe 모델은 한글이나 알파벳 대문자가 포함된 데이터셋으로 학습하지 않았기 때문에 알파벳 소문자만 인식한다는 사실에 유의해야한다."
      ],
      "metadata": {
        "id": "6QaWnDO_3dS3"
      }
    }
  ]
}