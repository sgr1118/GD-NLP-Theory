{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM6Flb9mmTG12/Sjp7g8VNL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgr1118/GD-NLP-Theory/blob/main/GD7_Seq2Seq_and_Attention(221007).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7-1. 들어가며\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/GN-4-L-1.max-800x600.jpg)\n",
        "<center></center>\n",
        "\n",
        "문장을 토큰화하고 의미를 부여하는 과정은 멋지고 중요한 일이지만 조금은 지루할 수 있죠. 그 과정을 견디고 여기까지 오신 여러분, 고생하셨습니다!\n",
        "\n",
        "이번 코스에서는 언어 모델이 발전해 온 과정에 대해 개략적으로 공부하고, NLP의 큰 흐름 중 하나인 Sequence to Sequence(Seq2seq) 에 대해 살펴볼 것입니다. 이를 발전시키기 위한 기법이자 지금은 없어선 안 될 중요한 메커니즘인 Attention에 대해서도 최초의 아이디어가 생겨난 시점에서부터 자세히 살펴보려고 합니다. 부디 즐거운 시간이 되시길 바랍니다!\n",
        "\n",
        "-----------------------------------------------------------\n",
        "\n",
        "1. 학습 목표\n",
        "\n",
        "- 언어 모델이 발전해 온 과정을 개략적으로 파악한다.\n",
        "- 기존 RNN 기법이 번역에서 보인 한계를 파악하고, 이를 개선한 Seq2seq를 이해한다.\n",
        "- Seq2seq를 발전시킨 Attention에 대해 알아본다.\n",
        "\n",
        "-----------------------------------------------------------\n",
        "2. 학습 내용\n",
        "- 우리가 만드는 언어 모델\n",
        "- Sequence to Sequence 문제\n",
        "- Sequence to Sequence 구현\n",
        "- Attention! (1) Bahdanau Attention\n",
        "- Attention! (2) Luong Attention\n",
        "- 미래의 기법들\n",
        "- 마무리\n"
      ],
      "metadata": {
        "id": "QRqJQAj5zE9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7-2. 우리가 만드는 언어 모델\n",
        "\n",
        "언어 모델(Language Model)이란, 주어진 단어들을 보고 다음 단어를 맞추는 모델입니다. 더 자세하게는, 단어의 시퀀스를 보고 다음 단어에 확률을 할당 하는 모델이다.\n",
        "\n",
        "좀 더 수학적으로 표현하자면, 언어 모델은 n-1개의 단어 시퀀스 w1,...,wn-1가 주어졌을 때, n번쨰 단어 wn으로 무엇이 올지를 예측하는 확률 모델로 표현됩니다. 파라미터 θ로 모델링하는 언어 모델을 다음과 같이 표현할 수 있습니다.\n",
        "\n",
        "![캡처2.PNG](data:image/png;base64,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)\n",
        "\n",
        "하지만 나중에는 꼭 시퀀스 형태의 Next Token Prediction 언어모델이 아니더라도, 주변 단어를 보고 중심 단어를 예측하는 형태로 언어모델을 구성하는 것도 보시게 될 것입니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zSbVyxfIztYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 통계적 언어 모델 (Statistical Language Model)\n",
        "\n",
        "딥러닝이 등장하기 이전엔 통계적 언어 모델(Statistical Language Model) 의 사용이 지배적이었습니다. 대표적으로 2000년대 초반까지 구글이나 네이버의 번역기는 모두 통계적 언어 모델을 기반으로 하고 있었죠. 통계적 언어 모델의 이모저모를 배우기 위해 아래 웹페이지를 방문해 보세요.\n",
        "\n",
        "[언어모델](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/09/16/LM/)\n",
        "\n",
        "Q1. 위 글에서 설명하는 언어 모델은 충분한 데이터가 없다면 범용적인 모델을 구축하기 어렵습니다. 그 이유에 대해 간단히 적어보세요.\n",
        "- 한 번도 본 적 없는(학습 데이터에 존재하지 않는) 단어 / 문장에 대해서는 확률을 부여할 수 없다. 따라서 언어 모델이 아우르는 범위를 넓히기 위해 다양한 단어를 포함하는 데이터가 필요하다. (양까지 충분하다면 정의하는 확률이 일반적이므로 더욱 좋다.)\n",
        "\n",
        "2. 신경망 언어 모델 (Neural Network Language Model)\n",
        "\n",
        "통계적 언어 모델의 단점을 개선한 것이 우리가 배울 신경망 언어 모델(Neural Network Language Model, 이하 NNLM) 입니다. NNLM의 시초는 Feed-Forward 신경망 언어 모델인데, 지금의 Embedding 레이어의 아이디어인 모델입니다. 아래 웹피이지를 참고하자\n",
        "\n",
        "[피드 포워드 신경망 언어 모델(NNLM)](https://wikidocs.net/45609)\n",
        "\n",
        "Q2. 위 글에서 설명하는 희소문제(sparsity problem)에 대해 간단히 설명해 보세요.\n",
        "- 희소 문제란, 모델이 충분한 데이터를 관측하지 못하면 언어를 정확히 모델링 할 수 없는 문제를 말한다. 한 번도 관측하지 못한 데이터에 대해선 0에 확률을 부여한다는 것에서 문제가 발생한다.\n",
        "\n",
        "Q3. 원-핫 입력이 투사층을 거쳐가는 것은 해당 행을 읽어오는 것과 동일하다는 데에서 붙여진 이름은 무엇인가요?\n",
        "- 룩 업 테이블(Lookup Table)\n",
        "\n",
        "Q4. 출력 층에서 사용되는 활성함수 Softmax는 어떤 의미를 갖나요?\n",
        "- 소프트맥스 함수를 거쳐 나온 0-1 사이 값은 해당 인덱스에 대한 확률을 의미한다.\n",
        "\n",
        "단어를 어떤 Embedding으로 표현할 수 있다는 것은 어떤 이점을 가지고있나? 단순 유사도 비교를 넘어서는 활용 사례를 아래 영상에서 확인\n",
        "\n",
        "[유튜브 영상](https://youtu.be/gUMvBRI-WGo)\n",
        "\n",
        "각 단어를 일련의 Embedding 벡터로 표현한 후, 이전의 몇 개 단어를 활용해 다음 단어를 예측하는 것은 분명 많은 문제를 해결했습니다. 특히 단어 간의 유사도를 표현할 수 있게 되어 문장의 유창성이 높아진 것은 혁신적이었죠!\n",
        "\n",
        "하지만 예측에 정해진 개수의 단어만 참고한다는 분명한 한계가 있었습니다. 예를 들어 번역문을 생성하려면 문장이 짧을 수도, 길 수도 있으니, n개의 단어를 참고하기보다는 \"몇 개 단어가 들어와도 문장 단위로 처리한다!\"는 종류의 모델링이 필요하게 되었죠. 그렇게 고안된 것이 바로 여러분들이 잘 알고 계신 순환 신경망(Recurrent Neural Network, 이하 RNN)을 활용한 언어 모델 입니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "SulrVPFh-4j-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7-3. Sequence to Sequence 문제\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/GN-4-L-2.jpg)\n",
        "<center>NNLM의 구조</center>\n",
        "\n",
        "다시 한번 설명하면, 여러 개의 단어(Embedding)를 합쳐(Concatenate) 고정된 크기의 Weight를 Linear로 처리하는 방식은 유연성에 한계가 있었습니다. 단어의 개수에 무관하게 처리할 수 있는 네트워크가 필요했고, 그것은 곧 RNN의 고안으로 이어졌습니다. RNN은 고정된 크기의 Weight가 선언되는 것은 동일하지만 입력을 순차적으로 \"적립\"하는 방식을 채택함으로써 유동적인 크기의 입력을 처리할 수 있었습니다.\n",
        "\n",
        "\"적립\"이라는 표현을 일반적으로 사용하지는 않습니다만, 필자가 생각하기에 RNN의 동작 방식에 잘 어울린다고 생각해 선택했습니다. 기억, 누적, 압축 등으로 대체할 수 있으며 가장 와닿는 방향으로 이해하세요!\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/GN-4-L-3.jpg)\n",
        "<center>RNN 학습 구조 비유</center>\n",
        "\n",
        "단어가 자체적으로 의미를 가질 수 있는 Embedding을 도입하고, 입력의 유연성을 위해 RNN도 적용했는데, 아직도 해결할 문제가 있다. 대표적으로 RNN에는 두 가지 문제점이 꼽히곤 합니다.\n",
        "\n",
        "1. 하나의 Weight에 입력을 적립하다 보니 입력이 길어질수록 이전 입력에 대한 정보가 소실되는 기울기 소실(Vanishing Gradient) 문제가 있습니다. 위의 그림을 보면 각 입력마다 정보를 색으로 확인할 수 있습니다. 첫 입력인 What(남색)의 정보가 마지막 입력인 ?에 다다라서는 거의 희석된 모습을 보여주고 있죠. 이 문제는 LSTM을 고안함으로써 개선되었습니다.\n",
        "\n",
        "[LSTM 이해하기](https://dgkim5360.tistory.com/entry/understanding-long-short-term-memory-lstm-kr)\n",
        "\n",
        "2. 단어 단위로 입력과 출력을 순환하는 RNN 구조는 문장 생성엔 적합할지언정 번역에 사용하기는 어렵다는 문제가 있습니다. 나는 점심을 먹는다 라는 문장을 영문으로 번역하자면 목표 문장은 I eat lunch 가 될 것인데, 과정을 순차적으로 생각하면 eat 이라는 단어를 만들 때는 먹는다 에 대한 정보가 없습니다. 각 언어별로 어순이 다르기 때문입니다.\n",
        "\n",
        "나는 -> I\n",
        "\n",
        "나는 점심을 -> I lunch\n",
        "\n",
        "나는 점심을 먹는다 -> I lunch eat(?)\n",
        "\n",
        "심지어 입력의 길이와 번역의 길이가 같다는 보장도 없죠. 번역에 있어서는 문장을 다 읽고 번역하는, 즉 문장 전체를 보고 나서 생성하는 구조가 필요했습니다. 이에 2014년, 구글이 Sequence to Sequence(Seq2Seq) 구조를 제안합니다.\n",
        "\n",
        "[seq2seq 논문](https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf)\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/GN-4-L-4.max-800x600.jpg)\n",
        "<center>seq2seq 구조</center>\n",
        "\n",
        "[seq2seq 보충 내용](https://reniew.github.io/35/)\n",
        "\n",
        "Q5. seq2seq의 저자들은 단순한 RNN 대신 LSTM을 사용하고자 하였습니다. 그 이유를 간단히 서술하고 논문에서 해당 내용이 포함된 문단을 찾아보세요.\n",
        "- 기존의 RNN은 기울기 소실문제를 가지고있다. 단순한 RNN은 긴 입력에 대한 정보를 학습시키기 어렵기 때문이다.\n",
        "\n",
        "- A simple strategy for general sequence learning is to map the input sequence to a fixed-sized vector using one RNN, and then to map the vector to the target sequence with another RNN (this approach has also been taken by Cho et al. 5). While it could work in principle since the RNN is provided with all the relevant information, it would be difficult to train the RNNs due to the resulting long term dependencies 14, 4 (figure 1) 16, 15. However, the Long Short-Term Memory (LSTM) 16 is known to learn problems with long range temporal dependencies, so an LSTM may succeed in this setting.\n",
        "\n",
        "Q6. 위의 <식 1>을 보고 각 x와 y, v가 무엇을 의미하는지 설명하세요. 그리고 v의 정의를 논문에서 찾아보세요.\n",
        "- x = input sequence\n",
        "- y = output sequence\n",
        "- v = 인코더의 입력 x에 대한 고정된 크기의 Representation Vector 아웃풋으로서, 디코더의 입력으로 사용됨\n",
        "\n",
        "- The LSTM computes this conditional probability by first obtaining the fixed-dimensional representation v of the input sequence (x1, . . . , xT ) given by the last hidden state of the LSTM\n",
        "\n",
        "Q7. 논문의 2. The Model의 마지막 문단은 입력과 출력의 \"establish communication\"에 대해 설명하고 있습니다. 이에 대해 이해한 대로 설명해보세요.\n",
        "- 입력 abc와 출력 def에 대해 a->d, b->e, c->f의 관계를 가진다면 입력을 cba로 뒤집어도 관련 있는 단어 사이의 평균 거리는 같게 유지된다. 그런데 a와 d의 거리는 매우 가까워지고, b와 e의 거리는 상당히 가까워진다. 이런 변형은 SGD가 입출력 간의 구조적 관계를 파악하는 것을 용이하게 하고 LSTM에 대해 성능을 Boost하는 효과가 있다. 특히 논문의 3.3 Reversing the Source Sentences 두 번째 문단을 보면, 단어 사이의 평균 거리는 유지되면서 문장의 앞 부분 단어의 거리가 줄어들기 때문에 학습 효과가 높아지는 것으로 추측하고 있다.\n",
        "\n",
        "Q8. Encoder은 입력 문장의 모든 단어들을 순차적으로 입력받고 모든 단어를 압축한 단 하나의 [    ]를 만듭니다. 빈칸에 들어갈 단어는 무엇인가요?\n",
        "- 컨텍스트 벡터(Context Vector)\n",
        "\n",
        "Q9. 문장의 시작과 끝에 붙는 특수한 토큰들이 있습니다. 만약 그 토큰들이 없다면 어떻게 될까요? 시작 토큰이 없는 경우와 끝 토큰이 없는 경우를 나누어 적어봅시다.\n",
        "- 시작 토큰이 없는 경우, Decoder의 첫 입력으로 전달할 단어가 없어져 번역을 진행할 수 없다. 만일 랜덤한 단어를 입력으로 넣는 경우, 번역의 성능에 문제가 생긴다.\n",
        "\n",
        "- 끝 토근이 없는 경우, 문장의 끝을 알릴 수 없어 단어를 무한정 생성하게 된다. 온점이나 느낌표 등을 끝 토큰으로 사용하기엔 불안정한 성능을 보일 것이다.\n",
        "\n",
        "Q10. 글 말미에 이번 코스의 스포일러가 담겨있네요! 문맥을 더 잘 반영하는 벡터를 생성하는 메커니즘이 있는데, 이름이 뭔가요?\n",
        "- 어텐션 메커니즘(Attention)\n"
      ],
      "metadata": {
        "id": "v8c7mUaJEHSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7-4. Sequence to Sequence 구현\n",
        "\n",
        "이제 Sequence to Sequence를 TensorFlow로 구현해보죠. 일단은 데이터를 직접 다루기보다는 차원 수를 확인하는 실습을 해보겠습니다. RNN 계통의 레이어들은 입력값과 반환값이 설정에 따라 각양각색입니다. 이번 구현에서는 입력으로 Embedding된 단어만 전달하고 (Hidden State는 전달하지 않습니다), 출력은 Encoder와 Decoder 별로 상이하므로 각각 설명을 첨부하겠습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "lUcT2z8T2VP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. LSTM Encoder"
      ],
      "metadata": {
        "id": "M-t7BdkB20Lt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuZYJEq_y10a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.LSTM(enc_units) # return_sequences 매개변수를 기본값 False로 전달\n",
        "\n",
        "  def call(self, x):\n",
        "    print('입력 Shape :', x.shape)\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    print('Embedding Layer를 거친 Shape : ', x.shape)\n",
        "\n",
        "    output = self.lstm(x)\n",
        "    print('LSTM Layer의 Output Shape : ', output.shape)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding 레이어를 단어 사이즈와 Embedding 차원에 대해 선언을 한 후, 논문에서 소개한 대로 tf.keras.layers.LSTM(enc_units)으로 LSTM을 정의합니다. TensorFlow 속 LSTM 모듈의 기본 반환 값은 최종 State 값이므로 return_sequences 나 return_state 값은 따로 조정하지 않습니다 (기본: False). 즉, 우리가 정의해 준 Encoder 클래스의 반환 값이 곧 컨텍스트 벡터(Context Vector) 가 되는 겁니다. 추가적인 옵션이 궁금하시다면 아래의 TensorFlow LSTM 공식 문서를 참조하시면 좋습니다.\n",
        "\n",
        "[tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)"
      ],
      "metadata": {
        "id": "wTopWJ8e6AI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 30000\n",
        "emb_size = 256\n",
        "lstm_size = 512\n",
        "batch_size = 1\n",
        "sample_seq_len = 3\n",
        "\n",
        "print(\"Vocab Size: {0}\".format(vocab_size))\n",
        "print(\"Embedidng Size: {0}\".format(emb_size))\n",
        "print(\"LSTM Size: {0}\".format(lstm_size))\n",
        "print(\"Batch Size: {0}\".format(batch_size))\n",
        "print(\"Sample Sequence Length: {0}\\n\".format(sample_seq_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjuJ2YFS6I_9",
        "outputId": "b8e1a5ab-ddad-4d33-9896-646ed3d20cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size: 30000\n",
            "Embedidng Size: 256\n",
            "LSTM Size: 512\n",
            "Batch Size: 1\n",
            "Sample Sequence Length: 3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_size, emb_size, lstm_size)\n",
        "sample_input = tf.zeros((batch_size, sample_seq_len))\n",
        "\n",
        "sample_output = encoder(sample_input) # 컨텍스트 벡터로 사용할 인코더 LSTM의 최종 State값"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jzz6VcJ76NL3",
        "outputId": "34829c1c-3484-479f-f2e4-73bbc6be4a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 Shape : (1, 3)\n",
            "Embedding Layer를 거친 Shape :  (1, 3, 256)\n",
            "LSTM Layer의 Output Shape :  (1, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/GN-4-L-6.max-800x600.jpg)\n",
        "<center>예시 코드와 동일한 Shape를 가지는 Encoder 구조</center>\n",
        "\n",
        "반환 값인 LSTM의 최종 State 값을 Decoder에게 전달해 주면 되겠죠?\n"
      ],
      "metadata": {
        "id": "Xvv9KURU6kqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. LSTM Decoder"
      ],
      "metadata": {
        "id": "MUxAIYSn6yvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder 구현에 사용된 변수들을 이어 사용함에 유의!\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.LSTM(dec_units,\n",
        "                                     return_sequences = True) # return_sequences 매개변수를 기본값 True로 전달\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.softmax = tf.keras.layers.Softmax(axis = -1)\n",
        "\n",
        "  def call(self, x, context_v): # 디코더의 입력 x와 인코더의 컨텍스트 벡터를 인자로 받는다.\n",
        "    print('입력 Shape :', x.shape)\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    print('Embedding Layer를 거친 Shape : ', x.shape)\n",
        "\n",
        "    context_v = tf.repeat(tf.expand_dims(context_v, axis = 1),\n",
        "                          repeats = x.shape[1], axis =1)\n",
        "\n",
        "    x = tf.concat([x, context_v], axis = -1) # 컨텍스트 벡터를 concat 해준다\n",
        "    print(\"Context Vector가 더해진 Shape:\", x.shape)\n",
        "    \n",
        "    x = self.lstm(x)\n",
        "    print(\"LSTM Layer의 Output Shape:\", x.shape)\n",
        "\n",
        "    output = self.fc(x)\n",
        "    print(\"Decoder 최종 Output Shape:\", output.shape)\n",
        "\n",
        "    return self.softmax(output)"
      ],
      "metadata": {
        "id": "O-BEuPYm7B01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder는 Encoder와 구조적으로 유사하지만 결과물을 생성해야 하므로 Fully Connected 레이어가 추가되었고, 출력값을 확률로 변환해 주는 Softmax 함수도 추가되었습니다 (Softmax는 모델 내부에 포함시키지 않아도 훈련 과정에서 포함시키는 방법도 있습니다). 그리고 Decoder가 매 스텝 생성하는 출력은 우리가 원하는 번역 결과에 해당하므로 LSTM 레이어의 return_sequences 변수를 True로 설정하여 State 값이 아닌 Sequence 값을 출력으로 받습니다."
      ],
      "metadata": {
        "id": "IMsQrWkj8lt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Vocab Size: {0}\".format(vocab_size))\n",
        "print(\"Embedidng Size: {0}\".format(emb_size))\n",
        "print(\"LSTM Size: {0}\".format(lstm_size))\n",
        "print(\"Batch Size: {0}\".format(batch_size))\n",
        "print(\"Sample Sequence Length: {0}\\n\".format(sample_seq_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YfdjuJL9BIB",
        "outputId": "dd97e626-6a0e-4e90-e683-11cfdec78542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size: 30000\n",
            "Embedidng Size: 256\n",
            "LSTM Size: 512\n",
            "Batch Size: 1\n",
            "Sample Sequence Length: 3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(vocab_size, emb_size, lstm_size)\n",
        "sample_input = tf.zeros((batch_size, sample_seq_len))\n",
        "\n",
        "dec_output = decoder(sample_input, sample_output)  # Decoder.call(x, context_v) 을 호출"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUYR4y3s9CKb",
        "outputId": "c605218f-6e99-40f1-b066-7ef04827c2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 Shape : (1, 3)\n",
            "Embedding Layer를 거친 Shape :  (1, 3, 256)\n",
            "Context Vector가 더해진 Shape: (1, 3, 768)\n",
            "LSTM Layer의 Output Shape: (1, 3, 512)\n",
            "Decoder 최종 Output Shape: (1, 3, 30000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/GN-4-L-7.max-800x600.jpg)\n",
        "<center>예시 코드와 동일한 Shape를 가지는 Seq2seq 구조</center>\n",
        "\n",
        "![seq2se1.PNG](data:image/png;base64,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)\n",
        "\n",
        "Encoder가 생성한 컨텍스트 벡터 v 를 Embedding 레이어를 거친 y 값에 Concatnate하여 위 수식을 비로소 만족하게 됩니다. 우리가 Seq2seq를 완성한 거죠!"
      ],
      "metadata": {
        "id": "vS-LVd4-9EV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7-5. Attention! (1) Bahdanau Attention\n",
        "\n",
        "혁신적이었던 Seq2Seq은 Encoder-Decoder 구조라는 딥러닝 모델의 큰 틀을 제시했고, 지금까지도 그 구조는 널리 활용되고 있습니다. 하지만 그것만으로 기계 번역이 완벽했다면 우리는 이미 외국어에 대한 두려움이 사라졌겠죠?\n",
        "\n",
        "Seq2Seq 역시 여느 기법처럼 한계점이 존재했으며 이를 발전시키려는 시도가 아주 많았습니다. 가장 대표적인 방법이 바로 Attention 메커니즘입니다! 이번 스텝에서는 Attention 메커니즘의 이모저모를 살펴보자\n",
        "\n",
        "1. Bahdanau Attention\n",
        "\n",
        "Bahdanau는 seq2seq의 컨텍스트 벡터가 고정된 길이로 정보를 압축하는 것이 손실을 야기한다고 주장하였다. 즉 짧은 문장에 대해서는 괜찮을지 모르겠으나 문장이 길어질수록 성능이 저하된다는 것이다.\n",
        "\n",
        "이에 그느 Encoder의 최종 State 값만을 사용하는 기존의 방식이 아닌, 매 스텝의 Hidden State를 활용해 컨텍스트 벡터를 구축하는 어텐션 매커니즘을 제안한다.\n",
        "\n",
        "아래는 Bahdanau Attention 논문이다. 반드시 읽어봐야 할 논문 중 하나이다. 나중에 반드시 읽어보자\n",
        "\n",
        "[Bahdanau Attention 논문](https://arxiv.org/pdf/1409.0473.pdf)\n",
        "\n",
        "[Bahdanau Attention 정리글](https://lovit.github.io/machine%20learning/2019/03/17/attention_in_nlp/)\n",
        "\n",
        "Q11. 블로그 저자의 사견에 따르면 모델의 성능 향상 외에 Attention을 활용할 수 있는 방법이 하나 있습니다. 부산물이라고 표현된 그 방법은 무엇인가요?\n",
        "- attention weight matrix를 이용하여 모델의 작동 방식에 대한 시각화는 모델의 안정성을 점검하고, 모델이 의도와 다르게 작동할 때 그 원인을 찾는데 이용될 수 있습니다. \n",
        "\n",
        "Q12. Sequence Labeling과 Sequence to Sequence의 예시를 하나씩 적고, 두 Task의 차이점에 대해 간단히 설명해봅시다\n",
        "- Sequence Labeling : 품사 판별, 입력되는 단어열과 출력되는 품사열의 길이가 같습니다.\n",
        "- Sequence to Sequence : 기계번역\n",
        "- 차이점 : Labeling은 입력 단어열과 품사열의 길이가 같지만 seq2seq는 다를 수 있다.\n",
        "\n",
        "2. seq2seq과 attn-seq2seq, 뭐가 다른가?\n",
        "\n",
        "어텐션이 있고 없고는 다음과 같이 다르다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/GN-4-L-attn.max-800x600.png)\n",
        "<center></center>\n",
        "\n",
        "위 두식에서 다른 점은 context vector c에 첨자 i가 다르다. 이런 차이가 발생한 이유는 다음가 같다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/GN-4-L-attn_exp.max-800x600.png)\n",
        "<center></center>\n",
        "\n",
        "위 그림은 Bahdanau 논문 원문의 3.LEARNING TO ALIGN AND TRANSLATE의 내용을 바탕으로 재구성한 것입니다. 위 그림의 왼쪽 부분은 X_j를 입력으로, y_i를 출력으로 하는 인코더-디코더 부분을 도식화한 것입니다. 여기서 유의해야 할 점은 ii는 디코더의 인덱스, jj는 인코더의 인덱스라는 점입니다.\n",
        "\n",
        "그렇다면 context vector c에 첨자 i가 붙는다는 의미는 다음과 같다.\n",
        "\n",
        "인코더가 X를 해석한 context c_i는 디코더의 포지션 i에 따라 다르게 표현(represent)되어야 한다.\n",
        "\n",
        "seq2seq의 인코더가 해석한 context는 디코더의 포지션i에 무관하게 항상 일정했다. 그러나 attention이 더해지면서 달라진다.\n",
        "\n",
        "'나는 밥을 먹었다'라는 한글 문장을 'I ate food'로 번역한다고 가정한다. 영어 단어의 첫 단어 'I'를 만들어야 할 때 인코더가 한글 문장을 해석한 컨텍스트 벡터에서는 '나는'이 강조되어야 하고, 영어 문장의 세 번째 'lunch'를 만들어야 할 때 인코더의 컨텍스트 벡터에서는 '밥을'이 강조되어야 한다는 것이다.\n",
        "\n",
        "디코더가 현재 시점 i에서 보기에 인코더의 어느 부분 j가 중요한가? 이 가중치가 바로 attention인 것이다.\n",
        "\n",
        "얼마나 강조되어야 하는지를 나타내는 가중치를 계산하는 식은 다음과 같다. 위의 식에서 αij가 바로 인코더의 j번째 hidden state h_j가 얼마나 강조되어야 할지를 결정하는 가중치 역할을 합니다. 이 가중치는 다시 디코더의 직전 스텝의 hidden state s \n",
        "i−1와 h_j의 유사도가 높을수록 높아지게 되어 있습니다.\n",
        "\n",
        "![캡처3.PNG](data:image/png;base64,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)\n",
        "\n",
        "Q13. 가중치의 합이 1이 되는 위 식이 성립하는 것은 어떤 함수의 특성 때문인데요, 입력값을 확률값으로 변환해주는 그 함수는 무엇인가요?\n",
        "- softmax\n",
        "\n",
        "Attention이라는 기법이 어떤 의미를 가지는지 정리가 조금 되셨나요? 우리는 Bahdanau의 기법을 조금 더 알고 싶으니 그에 포커싱한 멋진 시각화 글을 소개해드립니다.\n",
        "\n",
        "[Attn: Illustrated Attention](https://eda-ai-lab.tistory.com/157)\n",
        "\n",
        "주의❗ 위 시각화 글의 1 단계 : 모든 인코더 hidden state의 점수 얻기 부분에서 두 Hidden State의 평가 함수에 내적을 사용했는데요. 일반적인 attention 설명을 위해 내적을 사용했을 뿐, Bahdanau attention에서 실제 평가 함수는 아래와 같이 특정 벡터 공간으로 매핑된 두 Hidden State의 합을 사용합니다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/GN-4-L-9-1.max-800x600.png)\n",
        "<center></center>\n",
        "\n",
        "Q14. 위 시각화 글의 2단계에서, Softmax의 결괏값은 Encoder에 매 스텝마다 입력으로 들어온 각 단어의 독립적인 비중이라고 생각해도 좋은가요? (Hint: RNN의 Hidden State는 어떤 과정을 통해 생기죠?)\n",
        "- 독립적이지 않다. RNN은 단어의 정보를 순차적으로 적립하여 Hidden State를 구축하기 때문에 순방향 RNN의 첫 단어는 그 단어의 정보만을 담았을지언정 그 이후 스텝들은 거쳐온 모든 단어의 정보를 포함하고 있다.\n",
        "\n",
        "Q15. 위 시각화 글의 4단계에서, Hidden State에 Softmax 값을 곱하여 Alignment 값을 얻었습니다. 해당 값을 모두 더하여 최종적인 컨텍스트 벡터를 얻는데요, 다 더해지면 값이 모호해지지 않을까요? 이는 어떤 의미를 갖나요? 간단히 설명하고 기존 Seq2seq의 고정 크기의 컨텍스트 벡터와 비교해봅시다. (Hint: Word2Vec의 연산을 기억하나요?)\n",
        "- 컨텍스트 벡터가 핵심 단어(비중이 큰 단어)에 가장 근접하게 다가서되, 주변 단어에도 각각의 비중만큼 영향을 받아 문장을 적합한 위치에 매핑되게 한다.\n",
        "\n",
        "- 기존 RNN의 최종 스텝을 컨텍스트 벡터로 쓰던 방식은 항상 고정된 비중을 사용(마지막에 등장한 단어가 큰 비중)하는 셈이므로 Bahdanau의 방식보다 문맥 정보를 유연하게 반영하지 못한다.\n",
        "\n",
        "Q16. Bahdanau 방식에서 생성된 컨텍스트 벡터는 어떤 방식으로 Decoder에서 사용되나요?\n",
        "- Decoder의 이전 Hidden State와 Concatenate하여 새로운 Hidden State로 정의된다.\n",
        "\n",
        "Bahdanau가 제안한 Attention은 하나의 Baseline이 되어 지금도 여러 기법을 시험하는 데에 멋진 중심점이 되어주고 있습니다. 멋진 기술들의 공통점은 구현한 것을 봤을 때도 이해가 정말 잘 된다는 것이죠! 여기까지의 개념설명이 명확히 와닿지 않았더라도, 구현을 살펴보며 복습을 해봅시다!\n"
      ],
      "metadata": {
        "id": "McKUgpJA9Wyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BahdanauAttention 구현\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W_decoder = tf.keras.layers.Dense(units)\n",
        "    self.W_encoder = tf.keras.layers.Dense(units)\n",
        "    self.W_combine = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, H_encoder, H_decoder):\n",
        "    print('[ H_encoder ] Shape : ',  H_encoder.shape)\n",
        "\n",
        "    H_encoder = self.W_encoder(H_encoder)\n",
        "    print(\"[ W_encoder X H_encoder ] Shape:\", H_encoder.shape)\n",
        "\n",
        "    print(\"\\n[ H_decoder ] Shape:\", H_decoder.shape)\n",
        "    H_decoder = tf.expand_dims(H_decoder, 1)\n",
        "    H_decoder = self.W_decoder(H_decoder)\n",
        "\n",
        "    print(\"[ W_decoder X H_decoder ] Shape:\", H_decoder.shape)\n",
        "                               \n",
        "    score = self.W_combine(tf.nn.tanh(H_decoder + H_encoder))\n",
        "    print(\"[ Score_alignment ] Shape:\", score.shape)\n",
        "\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    print(\"\\n최종 Weight:\\n\", attention_weights.numpy())\n",
        "\n",
        "    context_vector = attention_weights * H_decoder\n",
        "    context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "W_size = 100\n",
        "\n",
        "print(\"Hidden State를 {0}차원으로 Mapping\\n\".format(W_size))\n",
        "\n",
        "attention = BahdanauAttention(W_size)\n",
        "\n",
        "enc_state = tf.random.uniform((1, 10, 512))\n",
        "dec_state = tf.random.uniform((1, 512))\n",
        "\n",
        "_ = attention(enc_state, dec_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8N8mQ2KMl4O",
        "outputId": "e55af06c-6bd8-40c1-a4c7-bf46d8cce139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden State를 100차원으로 Mapping\n",
            "\n",
            "[ H_encoder ] Shape :  (1, 10, 512)\n",
            "[ W_encoder X H_encoder ] Shape: (1, 10, 100)\n",
            "\n",
            "[ H_decoder ] Shape: (1, 512)\n",
            "[ W_decoder X H_decoder ] Shape: (1, 1, 100)\n",
            "[ Score_alignment ] Shape: (1, 10, 1)\n",
            "\n",
            "최종 Weight:\n",
            " [[[0.11162146]\n",
            "  [0.07901648]\n",
            "  [0.07644609]\n",
            "  [0.14229028]\n",
            "  [0.11833052]\n",
            "  [0.07512707]\n",
            "  [0.1513459 ]\n",
            "  [0.06177875]\n",
            "  [0.07978389]\n",
            "  [0.10425954]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder의 모든 스텝에 대한 Hidden State를 100차원의 벡터 공간으로 매핑 (1, 10, 100) 하고, Decoder의 현재 스텝에 대한 Hidden State 역시 100차원의 벡터 공간으로 매핑 (1, 1, 100)해 두 State의 합으로 정의된 Score (1, 10, 1) 를 구하는 모습입니다. Softmax를 거쳐 나온 값은 0-1 사이의 값으로 각 단어가 차지하는 비중을 의미하겠죠? 예시에서는 랜덤한 값을 사용했기 때문에 비중이 비슷비슷하지만 실제 단어로 적용시켜보면 유사한 단어에 높은 비중을 할당하게 된답니다! 그것을 시각화하면 아래와 같은 그림을 보실 수 있습니다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/original_images/GN-4-L-9.jpg)\n",
        "<center>Attention Map]</center>"
      ],
      "metadata": {
        "id": "oMW7vEf_Pd8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7-6. Attention! (2) Luong Attention\n",
        "\n",
        "1. Luong Attention\n",
        "\n",
        "Luong의 Attention은 Bahdanau의 방식을 약간 발전시킨 형태입니다. Decoder의 현재 Hidden State를 구하기 위해 한 스텝 이전의 Hidden State를 활용하는 것은 연산적으로 비효율적입니다. 이는 RNN의 연산 형태 때문인데, 자세한 내용은 아래 웹페이지에서 확인하시죠! 수식적인 부분은 완벽하게 이해하지 않아도 좋으니, Luong의 아이디어에 중점을 맞추도록 합니다.\n",
        "\n",
        "[Luong Attention 개념 정리](https://hcnoh.github.io/2019-01-01-luong-attention)\n",
        "\n",
        "Q17. 저자들은 4가지 Score 함수를 제시하였습니다. 각각은 무엇인가요?\n",
        "- Dot, General, Concat, Location\n",
        "\n",
        "[참고 논문](https://arxiv.org/pdf/1508.04025.pdf)\n",
        "\n",
        "Q18. 4가지 Score 함수 중, 가장 합리적인 성능을 보이는 함수는 어떤 함수인가요?\n",
        "- General\n",
        "\n",
        "![캡처4.PNG](data:image/png;base64,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)\n",
        "\n"
      ],
      "metadata": {
        "id": "49uRosl1Pj35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LuongAttention 구현\n",
        "\n",
        "class LuongAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(LuongAttention, self).__init__()\n",
        "    self.W_combine = tf.keras.layers.Dense(units)\n",
        "\n",
        "  def call(self, H_encoder, H_decoder):\n",
        "    print('[ H_encoder ] Shape : ', H_encoder.shape)\n",
        "\n",
        "    WH = self.W_combine(H_encoder)\n",
        "    print('[ W_encoder X H_encoder ] Shape : ', WH.shape)\n",
        "\n",
        "    H_decoder = tf.expand_dims(H_decoder, 1)\n",
        "    alignment = tf.matmul(WH, tf.transpose(H_decoder, [0, 2, 1]))\n",
        "    print('[ Score_alignment ] Shape : ', alignment.shape)\n",
        "\n",
        "    attention_weights = tf.nn.softmax(alignment, axis = 1)\n",
        "    print(\"\\n최종 Weight:\\n\", attention_weights.numpy())\n",
        "\n",
        "    attention_weights = tf.squeeze(attention_weights, axis=-1)\n",
        "    context_vector = tf.matmul(attention_weights, H_encoder)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "emb_dim = 512\n",
        "\n",
        "attention = LuongAttention(emb_dim)\n",
        "\n",
        "enc_state = tf.random.uniform((1, 10, emb_dim))\n",
        "dec_state = tf.random.uniform((1, emb_dim))\n",
        "\n",
        "_ = attention(enc_state, dec_state)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1Tbzou4Dawt",
        "outputId": "16af703b-e730-49da-998b-10dda3a2a1a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ H_encoder ] Shape :  (1, 10, 512)\n",
            "[ W_encoder X H_encoder ] Shape :  (1, 10, 512)\n",
            "[ Score_alignment ] Shape :  (1, 10, 1)\n",
            "\n",
            "최종 Weight:\n",
            " [[[3.9707996e-02]\n",
            "  [8.0487800e-01]\n",
            "  [4.7589329e-04]\n",
            "  [1.5667065e-04]\n",
            "  [2.1497888e-02]\n",
            "  [1.4207237e-03]\n",
            "  [1.2390356e-01]\n",
            "  [3.7665704e-05]\n",
            "  [4.1411652e-09]\n",
            "  [7.9216221e-03]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bahdanau의 Score 함수와는 다르게 하나의 Weight만을 사용하는 것이 특징입니다. 어떤 벡터 공간에 매핑해주는 과정이 없기 때문에 Weight의 크기는 단어 Embedding 크기와 동일해야 연산이 가능합니다. 이 또한 번역에 적용해보고 성능을 비교해본다면 좋겠죠!"
      ],
      "metadata": {
        "id": "Znh8EN0aHFsi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7-7. 트랜스포머로 가기 전 징검다리?\n",
        "\n",
        "Seq2seq와 Attention이 폭풍처럼 휩쓸고 난 후, 잠잠해진 NLP 계를 다시 깨운 것은 2016년 구글의 신경망 번역 시스템이었습니다. 놀라운 구조를 제안한 것은 아니나 무려 8개 층을 쌓은 Encoder-Decoder 구조와 Residual Connection은 제법 멋졌죠. 이에 대한 정리 글을 첨부하니 참고하라.\n",
        "\n",
        "[Google's Neural Machine Translation System.](https://norman3.github.io/papers/docs/google_neural_machine_translation.html)\n",
        "\n",
        "Q19. GNMT는 Attention을 활용한 Seq2seq 모델인데요, 여기서 Attention은 누구의 방식을 채택했나요? (우리가 배운 두 명 중 한 명이겠죠?)\n",
        "- BahdanauAttention\n",
        "\n",
        "Q20. Residual Connection을 사용했을 때의 이점이 몇 가지 있습니다. 가장 인상 깊게 느껴지는 두 가지만 적어봅시다.\n",
        "- 정확도를 올릴 수 있다.\n",
        "- 기존에 LSTM을 가지는 층 문제를 해소한다.\n",
        "- 정확도가 올라간다.\n",
        "- 학습 속도가 빨라진다.\n",
        "- Gradient Exploding / Vanishing 문제를 해결한다\n",
        "\n",
        "Q21. GNMT에서 언급되는 Copy Model이란 무엇인가요?\n",
        "- 한 번도 본 적 없는 단어에 대해서 <UNK> 토큰 처리를 하지 않고 단어를 그대로 복사하여 적당한 위치에 배치하는 모델이다.\n",
        "\n",
        "GNMT(Google Neural Machine Translation)는 어쩌면 복선이었을 수도 있는데, 왜냐하면 그 후에 등장한 것이 NLP의 꽃, 트랜스포머(Transformer) 이기 때문이죠! 앞서 언급한 레이어를 쌓는 구조나 Residual Connection이 트랜스포머와 굉장히 유사하기에 그렇게 느껴지기도 합니다.\n",
        "\n",
        "![](https://d3s0tskafalll9.cloudfront.net/media/images/GN-4-L-10.max-800x600.jpg)\n",
        "<center> 트랜스포머의 구조</center>\n",
        "\n",
        "트랜스포머의 Multi-Head Attention이라는 개념을 도입해 폭넓은 문맥을 파악하게 하고, 기존의 RNN 구조를 완전히 탈피하여 연산 속도 측면에서도 혁신적인 발전이 일어났습니다! 지금까지도 트랜스포머를 기반으로 한 모델들이 각 분야에서 최고의 성능을 내고있습니다.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PxsXYfnhHJiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7-8. 마무리\n",
        "\n",
        "GD7에서 배운 내용\n",
        "\n",
        "1. seq2seq\n",
        "\n",
        "2. Attention\n",
        " - BahdanauAttention\n",
        "\n",
        " - LuongAttention\n",
        "\n",
        "[ELECTRA 논문 리뷰](https://lms.aiffel.io/course/312/node/466/step/3123)\n",
        "\n"
      ],
      "metadata": {
        "id": "rCE7YVjdJ1aM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 질문하고 싶었던 내용\n",
        "\n"
      ],
      "metadata": {
        "id": "zRDAtg4qKjqc"
      }
    }
  ]
}