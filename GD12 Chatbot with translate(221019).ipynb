{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyPLFDhS1tYg8y7Bl6aMPJ0C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"755654b1aacf436b82edc124a722f8e7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9154f8d225d64540bf770cb5111f192b","IPY_MODEL_bf0d8c2c58af489fa545bc9e595ae5d9","IPY_MODEL_51791f485b644fcf9c79d417429b13b5"],"layout":"IPY_MODEL_67eb13c104cd478580e1679d9ee7d570"}},"9154f8d225d64540bf770cb5111f192b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f5f6172ebf544b891740075176447b4","placeholder":"​","style":"IPY_MODEL_f85cd1dd80c14b83b18128e7e142df08","value":"100%"}},"bf0d8c2c58af489fa545bc9e595ae5d9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0434c1cfcaf484d830e7f147c11989e","max":118370,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e3c4ff67f393488a93967a0386cff085","value":118370}},"51791f485b644fcf9c79d417429b13b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d9f7e0db64f4a68aac44df83a6e0589","placeholder":"​","style":"IPY_MODEL_1266fcb9874243b181e56017f815c27e","value":" 118370/118370 [00:00&lt;00:00, 700735.49it/s]"}},"67eb13c104cd478580e1679d9ee7d570":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f5f6172ebf544b891740075176447b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f85cd1dd80c14b83b18128e7e142df08":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d0434c1cfcaf484d830e7f147c11989e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3c4ff67f393488a93967a0386cff085":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8d9f7e0db64f4a68aac44df83a6e0589":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1266fcb9874243b181e56017f815c27e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2d09d832042e4062a789ae121d69a40e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_47e6d3a281a045c9a77caec843eb4ae2","IPY_MODEL_67da937c3181433ea7d5c598d3877e08","IPY_MODEL_d819e2334d494636b55ed9e01ccb1833"],"layout":"IPY_MODEL_8590a2d059be4519ac634e6c7fe0f0ff"}},"47e6d3a281a045c9a77caec843eb4ae2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d1153ebb8d14659938b84a629884b3b","placeholder":"​","style":"IPY_MODEL_13bf87381c8347deaf21da8aec38dd74","value":"100%"}},"67da937c3181433ea7d5c598d3877e08":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e99e5e0bf200472d93548cd2af086028","max":594,"min":0,"orientation":"horizontal","style":"IPY_MODEL_00ce04be1a5146b0ae89b40b0fee4ddc","value":594}},"d819e2334d494636b55ed9e01ccb1833":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a4e384d90404aeda397961fa60e25b0","placeholder":"​","style":"IPY_MODEL_421f37d4caaf49039785369a225ec437","value":" 594/594 [00:00&lt;00:00, 8815.34it/s]"}},"8590a2d059be4519ac634e6c7fe0f0ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d1153ebb8d14659938b84a629884b3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13bf87381c8347deaf21da8aec38dd74":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e99e5e0bf200472d93548cd2af086028":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00ce04be1a5146b0ae89b40b0fee4ddc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9a4e384d90404aeda397961fa60e25b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"421f37d4caaf49039785369a225ec437":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"04fb89e75d0f47989e07a035a28a1c3a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aee3169330ca47aa92bb11816b6254c9","IPY_MODEL_9f0782d03e1d46cab64389f6df8600c5","IPY_MODEL_472bf828d1d840bc9ea8b3db387dd824"],"layout":"IPY_MODEL_98a9dec343cb4abca171ca0388f2118e"}},"aee3169330ca47aa92bb11816b6254c9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fa9e0971dc44e6487a51bca9e4ee795","placeholder":"​","style":"IPY_MODEL_833707fce8f54e8c8ad6239985d83478","value":"100%"}},"9f0782d03e1d46cab64389f6df8600c5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_72038146b2c84eb4824e744e771fdd3b","max":118370,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fcdff8c1efa54ba3a21066455d5dd42b","value":118370}},"472bf828d1d840bc9ea8b3db387dd824":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_499c03e6714846e09ce752afbff0ec51","placeholder":"​","style":"IPY_MODEL_c2bb22cc066b4e5a9100f4847a28ede2","value":" 118370/118370 [00:01&lt;00:00, 105137.94it/s]"}},"98a9dec343cb4abca171ca0388f2118e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fa9e0971dc44e6487a51bca9e4ee795":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"833707fce8f54e8c8ad6239985d83478":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72038146b2c84eb4824e744e771fdd3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcdff8c1efa54ba3a21066455d5dd42b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"499c03e6714846e09ce752afbff0ec51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2bb22cc066b4e5a9100f4847a28ede2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a85bfff6e5f0428880ad1eac74925376":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a7395430cfbe4c18b30b2a11bd26d0a5","IPY_MODEL_f8ab14a78a0741c4a0bef0d424a00139","IPY_MODEL_727428d5f4f5465ab03ef8961a25f7d4"],"layout":"IPY_MODEL_a188760309cd4f7281112d30fab9d214"}},"a7395430cfbe4c18b30b2a11bd26d0a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_547b746abdca48fbb4508a777575d14a","placeholder":"​","style":"IPY_MODEL_575da3ba7fcd4736960f1df67486f046","value":"100%"}},"f8ab14a78a0741c4a0bef0d424a00139":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b867d83795d4494c9aadf7586af6a474","max":118370,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c82f97a419de43fa88e9de193b5aee4b","value":118370}},"727428d5f4f5465ab03ef8961a25f7d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2eefd5ed02c04126a0a7eff58272bd4d","placeholder":"​","style":"IPY_MODEL_c60d0279dca74575ba41ef1638cf32b0","value":" 118370/118370 [00:01&lt;00:00, 103278.14it/s]"}},"a188760309cd4f7281112d30fab9d214":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"547b746abdca48fbb4508a777575d14a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"575da3ba7fcd4736960f1df67486f046":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b867d83795d4494c9aadf7586af6a474":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c82f97a419de43fa88e9de193b5aee4b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2eefd5ed02c04126a0a7eff58272bd4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c60d0279dca74575ba41ef1638cf32b0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"973e4a60d50a48db88331295573a91d4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5a8674f8c81b49d0ac69e1c9e888b551","IPY_MODEL_a00ea99269a84378863716c4ec177d00","IPY_MODEL_3915feeca33348ef915155b3e3adf7db"],"layout":"IPY_MODEL_d65e2e08342b4425ad01ab444d7a6c99"}},"5a8674f8c81b49d0ac69e1c9e888b551":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ff3d76ba6fd4388b6ccfe1d5a5b7298","placeholder":"​","style":"IPY_MODEL_e8b1ec0c32b64705bf64ea21c3ea4084","value":"Epoch  1: 100%"}},"a00ea99269a84378863716c4ec177d00":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_80d035e6940e42f18b067653518cc8ad","max":1850,"min":0,"orientation":"horizontal","style":"IPY_MODEL_874bd8a6fa3449b6831c2eee21d4c668","value":1850}},"3915feeca33348ef915155b3e3adf7db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dcd3d88988c94cc79af5350f81892bfd","placeholder":"​","style":"IPY_MODEL_94290c6a45dd42d491cfb6c2163e024b","value":" 1850/1850 [05:33&lt;00:00,  1.34it/s, Loss 3.7644]"}},"d65e2e08342b4425ad01ab444d7a6c99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ff3d76ba6fd4388b6ccfe1d5a5b7298":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8b1ec0c32b64705bf64ea21c3ea4084":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"80d035e6940e42f18b067653518cc8ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"874bd8a6fa3449b6831c2eee21d4c668":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dcd3d88988c94cc79af5350f81892bfd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94290c6a45dd42d491cfb6c2163e024b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dda7c9d0349541b78cf11460ae82deaf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae5d867fbbe749d1baaf75d9c5391566","IPY_MODEL_52b301bf38d94285808c331615349706","IPY_MODEL_0227ed00ab7443d0a68844163f5e1c17"],"layout":"IPY_MODEL_94a0bcce5b2a47ff9b0a1afddf099eb4"}},"ae5d867fbbe749d1baaf75d9c5391566":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_474f3fd74e5d4fb59eee8f66760b83d9","placeholder":"​","style":"IPY_MODEL_f3b0220faed24b61b81f30c877fcd44f","value":"Epoch  2: 100%"}},"52b301bf38d94285808c331615349706":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_2612672063da44fdbc39a640a0455e0f","max":1850,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9f221f92c5a246118bbe3eb8fcc28add","value":1850}},"0227ed00ab7443d0a68844163f5e1c17":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fc7233155cd4c889c6c026168e653ed","placeholder":"​","style":"IPY_MODEL_ac7e4ed4ec9f4c208d86e6ca3bd486d4","value":" 1850/1850 [05:24&lt;00:00,  6.55it/s, Loss 2.1399]"}},"94a0bcce5b2a47ff9b0a1afddf099eb4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"474f3fd74e5d4fb59eee8f66760b83d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3b0220faed24b61b81f30c877fcd44f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2612672063da44fdbc39a640a0455e0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f221f92c5a246118bbe3eb8fcc28add":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6fc7233155cd4c889c6c026168e653ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac7e4ed4ec9f4c208d86e6ca3bd486d4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"41435f46f1c648e3b7759d3f010f3bbd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_164e04d593e24b1fa516f071ac50c1e2","IPY_MODEL_81ce2d18ecaf42ad8c0bf9f23b14c459","IPY_MODEL_1dd76ea7d6324511a523c1a6f6925f8f"],"layout":"IPY_MODEL_211d6e6b76434938be157828c85fabfa"}},"164e04d593e24b1fa516f071ac50c1e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43a8ce98980143558a555f13089db678","placeholder":"​","style":"IPY_MODEL_8a2c3a496c9242fcb26cd09eb1ede050","value":"Epoch  3: 100%"}},"81ce2d18ecaf42ad8c0bf9f23b14c459":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_14fd1ece84db49afa9ccb5c767377cdc","max":1850,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bb742a1fb7f047d2a14fdd3b958010e0","value":1849}},"1dd76ea7d6324511a523c1a6f6925f8f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5bce96a2fa3f4b1b9adb2a5186e30a29","placeholder":"​","style":"IPY_MODEL_362faaff951c41eb8f6eb3f40c37ab26","value":" 1849/1850 [05:22&lt;00:00,  5.70it/s, Loss 1.8004]"}},"211d6e6b76434938be157828c85fabfa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43a8ce98980143558a555f13089db678":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a2c3a496c9242fcb26cd09eb1ede050":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14fd1ece84db49afa9ccb5c767377cdc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb742a1fb7f047d2a14fdd3b958010e0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5bce96a2fa3f4b1b9adb2a5186e30a29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"362faaff951c41eb8f6eb3f40c37ab26":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a90d04944c144368deb2e583b803920":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_53416c43b3e94bc6ad9081a24be697c4","IPY_MODEL_94b73b6d8f944aecaf241cadf621c615","IPY_MODEL_e2a0fa7d0ec64eb8bd810ecad4991c2e"],"layout":"IPY_MODEL_2fa7a72dd3734ee0981025158729acdb"}},"53416c43b3e94bc6ad9081a24be697c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a62f0117a14e4f96bcbec0b715fbbfe5","placeholder":"​","style":"IPY_MODEL_3bfdd863ffd84173b72c4df0c20652a7","value":"100%"}},"94b73b6d8f944aecaf241cadf621c615":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_408f6cdee36745e7a40fbb6e7735af62","max":594,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a5addf4eca3c4675ae4abc3cb0f76e6d","value":594}},"e2a0fa7d0ec64eb8bd810ecad4991c2e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8bdfc68badd340c2a5925031daaa3937","placeholder":"​","style":"IPY_MODEL_128e8d59d26a4d30b1515f16f81dd9a9","value":" 594/594 [19:26&lt;00:00,  2.71s/it]"}},"2fa7a72dd3734ee0981025158729acdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a62f0117a14e4f96bcbec0b715fbbfe5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bfdd863ffd84173b72c4df0c20652a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"408f6cdee36745e7a40fbb6e7735af62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5addf4eca3c4675ae4abc3cb0f76e6d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8bdfc68badd340c2a5925031daaa3937":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"128e8d59d26a4d30b1515f16f81dd9a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# 12-1. 들어가며\n","\n","학습 목표\n","\n","1. 번역 모델을 활용한 챗봇 만들기"],"metadata":{"id":"M6UX53j-0eF7"}},{"cell_type":"markdown","source":["# 12-2. 번역 데이터 준비\n","\n","데이터셋 : 영어 - 스페인어 데이터 셋"],"metadata":{"id":"mArdWnRR0yLG"}},{"cell_type":"code","source":["# sentencepiece 설치\n","\n","!pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CfXpdLRp1INc","executionInfo":{"status":"ok","timestamp":1666341492398,"user_tz":-540,"elapsed":4303,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"39a95fa1-1e96-494e-c784-ab7b0bb17d98"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 5.1 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n"]}]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x-0V6UU90MCP","executionInfo":{"status":"ok","timestamp":1666341501301,"user_tz":-540,"elapsed":2993,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"d0c9fda2-f7a0-4725-bc23-f68da02ca0a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.9.2\n"]}],"source":["# 라이브러리 불러오기\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import sentencepiece as spm\n","from nltk.translate.bleu_score import sentence_bleu\n","from nltk.translate.bleu_score import SmoothingFunction\n","\n","import re\n","import os\n","import random\n","import math\n","\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","\n","print(tf.__version__)"]},{"cell_type":"code","source":["# 데이터 불러오기\n","\n","zip_path = tf.keras.utils.get_file(\n","    'spa-eng.zip',\n","    origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n","    extract=True)"],"metadata":{"id":"oXkXKMB91XHv","executionInfo":{"status":"ok","timestamp":1666331528313,"user_tz":-540,"elapsed":11,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a24ef77d-ecf0-4d58-8e4a-508e74e265a8"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","2638744/2638744 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["# 데이터 출력\n","\n","file_path = os.path.dirname(zip_path)+\"/spa-eng/spa.txt\"\n","\n","with open(file_path, \"r\") as f:\n","    spa_eng_sentences = f.read().splitlines()\n","\n","spa_eng_sentences = list(set(spa_eng_sentences)) \n","total_sentence_count = len(spa_eng_sentences)\n","print(\"Example:\", total_sentence_count)\n","\n","for sen in spa_eng_sentences[0:100][::20]: \n","    print(\">>\", sen)\n","\n","# 총 118964개 데이터가 있다."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mKNXMae611MS","executionInfo":{"status":"ok","timestamp":1666331529751,"user_tz":-540,"elapsed":4,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"354a0487-1b6b-40b0-efd2-f5f65683da52"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Example: 118964\n",">> Blow the horn so that car will let us pass.\tToque la bocina para que ese auto nos deje pasar.\n",">> Would you like to be considered for the job?\t¿Le gustaría que se le considerase para el trabajo?\n",">> Women generally live longer than men.\tLas mujeres generalmente viven más que los hombres.\n",">> I won't have enough time for everything I want to do.\tNo voy a tener suficiente tiempo para todo lo que quiero hacer.\n",">> The floor was very cold.\tEl piso estaba muy frío.\n"]}]},{"cell_type":"code","source":["# 데이터 전처리\n","\n","def preprocess_sentence(sentence):\n","    sentence = sentence.lower()\n","    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n","    sentence = sentence.strip()\n","    return sentence"],"metadata":{"id":"JPQlZyaD2Ham","executionInfo":{"status":"ok","timestamp":1666331554499,"user_tz":-540,"elapsed":322,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# 전처리 적용\n","\n","spa_eng_sentences = list(map(preprocess_sentence, spa_eng_sentences))"],"metadata":{"id":"8w__DPZV2iOc","executionInfo":{"status":"ok","timestamp":1666331555745,"user_tz":-540,"elapsed":868,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# 테스트에 사용할 데이터 분리\n","\n","test_sentence_count = total_sentence_count // 200\n","print(\"Test Size: \", test_sentence_count)\n","print(\"\\n\")\n","\n","train_spa_eng_sentences = spa_eng_sentences[:-test_sentence_count]\n","test_spa_eng_sentences = spa_eng_sentences[-test_sentence_count:]\n","print(\"Train Example:\", len(train_spa_eng_sentences))\n","for sen in train_spa_eng_sentences[0:100][::20]: \n","    print(\">>\", sen)\n","print(\"\\n\")\n","print(\"Test Example:\", len(test_spa_eng_sentences))\n","for sen in test_spa_eng_sentences[0:100][::20]: \n","    print(\">>\", sen)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Nv2PX2a2sWd","executionInfo":{"status":"ok","timestamp":1666331556638,"user_tz":-540,"elapsed":6,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"6a3028c8-5e17-446e-878c-6e1261b9d960"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Size:  594\n","\n","\n","Train Example: 118370\n",">> blow the horn so that car will let us pass.\ttoque la bocina para que ese auto nos deje pasar.\n",">> would you like to be considered for the job?\t¿le gustaría que se le considerase para el trabajo?\n",">> women generally live longer than men.\tlas mujeres generalmente viven más que los hombres.\n",">> i won't have enough time for everything i want to do.\tno voy a tener suficiente tiempo para todo lo que quiero hacer.\n",">> the floor was very cold.\tel piso estaba muy frío.\n","\n","\n","Test Example: 594\n",">> he never fails to call his mother on her birthday.\tél siempre llama a su madre el día de su cumpleaños.\n",">> the sun is shining in my face.\tel sol brilla en mi cara.\n",">> i met him at a club.\tlo conocí en el club.\n",">> how much is this going to cost us?\t¿cuánto nos va a costar esto?\n",">> my father was lost in thought.\tmi padre estaba en las nubes.\n"]}]},{"cell_type":"code","source":["# 영어 문장과 스페인어 문장 구분하기\n","# tab으로 연결되어 있기에 \\t을 기준으로 분리\n","\n","def split_spa_eng_sentences(spa_eng_sentences):\n","    spa_sentences = []\n","    eng_sentences = []\n","    for spa_eng_sentence in tqdm(spa_eng_sentences):\n","        eng_sentence, spa_sentence = spa_eng_sentence.split('\\t')\n","        spa_sentences.append(spa_sentence)\n","        eng_sentences.append(eng_sentence)\n","    return eng_sentences, spa_sentences"],"metadata":{"id":"PtdXU8ct3vjq","executionInfo":{"status":"ok","timestamp":1666331560882,"user_tz":-540,"elapsed":4,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# train 데이터 나누기\n","\n","train_eng_sentences, train_spa_sentences = split_spa_eng_sentences(train_spa_eng_sentences)\n","print(len(train_eng_sentences))\n","print(train_eng_sentences[0])\n","print('\\n')\n","print(len(train_spa_sentences))\n","print(train_spa_sentences[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158,"referenced_widgets":["755654b1aacf436b82edc124a722f8e7","9154f8d225d64540bf770cb5111f192b","bf0d8c2c58af489fa545bc9e595ae5d9","51791f485b644fcf9c79d417429b13b5","67eb13c104cd478580e1679d9ee7d570","8f5f6172ebf544b891740075176447b4","f85cd1dd80c14b83b18128e7e142df08","d0434c1cfcaf484d830e7f147c11989e","e3c4ff67f393488a93967a0386cff085","8d9f7e0db64f4a68aac44df83a6e0589","1266fcb9874243b181e56017f815c27e"]},"id":"nrbTleeh4swW","executionInfo":{"status":"ok","timestamp":1666331567320,"user_tz":-540,"elapsed":351,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"0d2c2d61-0243-401e-dc1a-8100068cb5e5"},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/118370 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"755654b1aacf436b82edc124a722f8e7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["118370\n","blow the horn so that car will let us pass.\n","\n","\n","118370\n","toque la bocina para que ese auto nos deje pasar.\n"]}]},{"cell_type":"code","source":["# test 데이터 나누기\n","\n","test_eng_sentences, test_spa_sentences = split_spa_eng_sentences(test_spa_eng_sentences)\n","print(len(test_eng_sentences))\n","print(test_eng_sentences[0])\n","print('\\n')\n","print(len(test_spa_sentences))\n","print(test_spa_sentences[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158,"referenced_widgets":["2d09d832042e4062a789ae121d69a40e","47e6d3a281a045c9a77caec843eb4ae2","67da937c3181433ea7d5c598d3877e08","d819e2334d494636b55ed9e01ccb1833","8590a2d059be4519ac634e6c7fe0f0ff","8d1153ebb8d14659938b84a629884b3b","13bf87381c8347deaf21da8aec38dd74","e99e5e0bf200472d93548cd2af086028","00ce04be1a5146b0ae89b40b0fee4ddc","9a4e384d90404aeda397961fa60e25b0","421f37d4caaf49039785369a225ec437"]},"id":"Eho0ijbP5bE8","executionInfo":{"status":"ok","timestamp":1666331572846,"user_tz":-540,"elapsed":631,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"92997ef7-238f-4e95-ba35-eba0e525a716"},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/594 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d09d832042e4062a789ae121d69a40e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["594\n","he never fails to call his mother on her birthday.\n","\n","\n","594\n","él siempre llama a su madre el día de su cumpleaños.\n"]}]},{"cell_type":"markdown","source":["토큰화\n","\n","이제 문장 데이터를 토큰화를 해야 할 차례입니다. Sentencepiece 기반의 토크나이저를 생성해 주는 generate_tokenizer() 함수를 정의하여 토크나이저를 얻어보도록 하죠!"],"metadata":{"id":"aqalIbqg6G-_"}},{"cell_type":"code","source":["def generate_tokenizer(corpus,\n","                       vocab_size,\n","                       lang=\"spa-eng\",\n","                       pad_id=0,   # pad token의 일련번호\n","                       bos_id=1,  # 문장의 시작을 의미하는 bos token(<s>)의 일련번호\n","                       eos_id=2,  # 문장의 끝을 의미하는 eos token(</s>)의 일련번호\n","                       unk_id=3):   # unk token의 일련번호\n","    file = \"./%s_corpus.txt\" % lang\n","    model = \"%s_spm\" % lang\n","\n","    with open(file, 'w') as f:\n","        for row in corpus: f.write(str(row) + '\\n')\n","\n","    import sentencepiece as spm\n","    spm.SentencePieceTrainer.Train(\n","        '--input=./%s --model_prefix=%s --vocab_size=%d'\\\n","        % (file, model, vocab_size) + \\\n","        '--pad_id==%d --bos_id=%d --eos_id=%d --unk_id=%d'\\\n","        % (pad_id, bos_id, eos_id, unk_id)\n","    )\n","\n","    tokenizer = spm.SentencePieceProcessor()\n","    tokenizer.Load('%s.model' % model)\n","\n","    return tokenizer"],"metadata":{"id":"YgFCN_A76Lf1","executionInfo":{"status":"ok","timestamp":1666331574259,"user_tz":-540,"elapsed":3,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["한-영 번역 때와 다륵, 두 언어가 단어 사전을 공유하도록 한다. 영어와 스페인어는 모두 알파벳으로 이뤄지는데다가 인도유럽족이기 때문에 기대할 수 있는 효과가 많다. 후에 챗봇을 만들 때에도 질문과 답변이 모두 한글로 이루어져 있기 때문에 Embedding 층을 공유하는 것이 성능에 도움이 됩니다."],"metadata":{"id":"OUeYxzbh7u-U"}},{"cell_type":"code","source":["VOCAB_SIZE = 20000\n","tokenizer = generate_tokenizer(train_eng_sentences + train_spa_sentences, VOCAB_SIZE, 'spa-eng')\n","tokenizer.set_encode_extra_options(\"bos:eos\")  # 문장 양 끝에 <s> , </s> 추가"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mM0CTcyj7_F-","executionInfo":{"status":"ok","timestamp":1666331584815,"user_tz":-540,"elapsed":6149,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"722ad176-5f8d-4356-95f0-bc26a35b1806"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["위에서 두 언어 사이에 단어 사전을 공유하기로 하였으므로 Encoder와 Decoder의 전용 토크나이저를 만들지 않고, 방금 만들어진 토크나이저를 두 언어 사이에서 공유하게 됩니다.\n","\n","토크나이저가 준비되었으니 본격적으로 데이터를 토큰화하도록 하겠습니다. 토큰화를 해주는 함수를 만들어 줍니다."],"metadata":{"id":"eOhd08FA8SCk"}},{"cell_type":"code","source":["def make_corpus(sentences, tokenizer):\n","    corpus = []\n","    for sentence in tqdm(sentences):\n","        tokens = tokenizer.encode_as_ids(sentence)\n","        corpus.append(tokens)\n","    return corpus"],"metadata":{"id":"Dgdw6DAm8Ufw","executionInfo":{"status":"ok","timestamp":1666331588623,"user_tz":-540,"elapsed":344,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["eng_corpus = make_corpus(train_eng_sentences, tokenizer)\n","spa_corpus = make_corpus(train_spa_sentences, tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["04fb89e75d0f47989e07a035a28a1c3a","aee3169330ca47aa92bb11816b6254c9","9f0782d03e1d46cab64389f6df8600c5","472bf828d1d840bc9ea8b3db387dd824","98a9dec343cb4abca171ca0388f2118e","4fa9e0971dc44e6487a51bca9e4ee795","833707fce8f54e8c8ad6239985d83478","72038146b2c84eb4824e744e771fdd3b","fcdff8c1efa54ba3a21066455d5dd42b","499c03e6714846e09ce752afbff0ec51","c2bb22cc066b4e5a9100f4847a28ede2","a85bfff6e5f0428880ad1eac74925376","a7395430cfbe4c18b30b2a11bd26d0a5","f8ab14a78a0741c4a0bef0d424a00139","727428d5f4f5465ab03ef8961a25f7d4","a188760309cd4f7281112d30fab9d214","547b746abdca48fbb4508a777575d14a","575da3ba7fcd4736960f1df67486f046","b867d83795d4494c9aadf7586af6a474","c82f97a419de43fa88e9de193b5aee4b","2eefd5ed02c04126a0a7eff58272bd4d","c60d0279dca74575ba41ef1638cf32b0"]},"id":"MJMICCfk802g","executionInfo":{"status":"ok","timestamp":1666331599283,"user_tz":-540,"elapsed":2954,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"820a81c9-3c90-458f-fd01-10068281470c"},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/118370 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04fb89e75d0f47989e07a035a28a1c3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/118370 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a85bfff6e5f0428880ad1eac74925376"}},"metadata":{}}]},{"cell_type":"code","source":["print(train_eng_sentences[0])\n","print(eng_corpus[0])\n","print('\\n')\n","print(train_spa_sentences[0])\n","print(spa_corpus[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EqpgneZL84LC","executionInfo":{"status":"ok","timestamp":1666331601473,"user_tz":-540,"elapsed":415,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"bdc82960-1216-49e8-ce16-685f64aee9de"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["blow the horn so that car will let us pass.\n","[1, 5579, 9, 8393, 155, 32, 184, 90, 165, 182, 1432, 0, 2]\n","\n","\n","toque la bocina para que ese auto nos deje pasar.\n","[1, 6473, 16, 14679, 66, 15, 230, 468, 229, 2305, 754, 0, 2]\n"]}]},{"cell_type":"code","source":["# 토큰의 길이 50으로 제한 후 패딩 작업\n","\n","MAX_LEN = 50\n","enc_ndarray = tf.keras.preprocessing.sequence.pad_sequences(eng_corpus, maxlen=MAX_LEN, padding='post')\n","dec_ndarray = tf.keras.preprocessing.sequence.pad_sequences(spa_corpus, maxlen=MAX_LEN, padding='post')"],"metadata":{"id":"9eWlafFi9Dh4","executionInfo":{"status":"ok","timestamp":1666331603604,"user_tz":-540,"elapsed":987,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# 데이터셋\n","\n","BATCH_SIZE = 64\n","train_dataset = tf.data.Dataset.from_tensor_slices((enc_ndarray, dec_ndarray)).batch(batch_size=BATCH_SIZE)"],"metadata":{"id":"4so5WR7rD6Fa","executionInfo":{"status":"ok","timestamp":1666331607861,"user_tz":-540,"elapsed":388,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# 12-3. 번역 모델 만들기"],"metadata":{"id":"AX8qXtBSEnAz"}},{"cell_type":"markdown","source":["1. 트랜스포머 구현하기\n","\n","아래 웹페이지를 참고하여 구현한다.\n","\n","[위키독스: 트랜스포머](https://wikidocs.net/31379)\n","\n","[Trax: Transformer](https://github.com/google/trax/blob/master/trax/models/transformer.py)\n","\n","[Tensorflow: Transformer](https://www.tensorflow.org/text/tutorials/transformer)"],"metadata":{"id":"knXjF8h-EqDF"}},{"cell_type":"code","source":["# Positional Encoding 구현\n","\n","def positional_encoding(pos, d_model):\n","    def cal_angle(position, i):\n","        return position / np.power(10000, int(i) / d_model)\n","\n","    def get_posi_angle_vec(position):\n","        return [cal_angle(position, i) for i in range(d_model)]\n","\n","    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n","    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n","    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n","    return sinusoid_table"],"metadata":{"id":"rKzorDTNEpU2","executionInfo":{"status":"ok","timestamp":1666331635903,"user_tz":-540,"elapsed":2,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["tf.cast() : 배열의 dtype을 변경시켜준다\n","tf.math.equal() : \n","[텐서플로우를 이용한 논리연산](https://chan-lab.tistory.com/m/9)\n","\n","![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FcRzJTh%2FbtqwMdHnKKz%2F64aVKDH4W04DsJm2ybOSr1%2Fimg.png)\n","<center>tf.math.equal 예시/center>\n","\n"],"metadata":{"id":"4zoamWVeL_t_"}},{"cell_type":"code","source":["# Mask 생성하기\n","\n","def generate_padding_mask(seq):\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","    return seq[:, tf.newaxis, tf.newaxis, :]\n","\n","def generate_lookahead_mask(size):\n","    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","    return mask\n","\n","def generate_masks(src, tgt):\n","    enc_mask = generate_padding_mask(src)\n","    dec_enc_mask = generate_padding_mask(src)\n","\n","    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])\n","    dec_tgt_padding_mask = generate_padding_mask(tgt)\n","    dec_mask = tf.maximum(dec_tgt_padding_mask, dec_lookahead_mask)\n","\n","    return enc_mask, dec_enc_mask, dec_mask"],"metadata":{"id":"YpyvE_1mJd4h","executionInfo":{"status":"ok","timestamp":1666331638308,"user_tz":-540,"elapsed":1,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Multi Head Attention 구현\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","        \n","        self.depth = d_model // self.num_heads\n","        \n","        self.W_q = tf.keras.layers.Dense(d_model)\n","        self.W_k = tf.keras.layers.Dense(d_model)\n","        self.W_v = tf.keras.layers.Dense(d_model)\n","        \n","        self.linear = tf.keras.layers.Dense(d_model)\n","\n","    def scaled_dot_product_attention(self, Q, K, V, mask):\n","        d_k = tf.cast(K.shape[-1], tf.float32)\n","        QK = tf.matmul(Q, K, transpose_b=True)\n","\n","        scaled_qk = QK / tf.math.sqrt(d_k)\n","\n","        if mask is not None: scaled_qk += (mask * -1e9)  \n","\n","        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n","        out = tf.matmul(attentions, V)\n","\n","        return out, attentions\n","        \n","\n","    def split_heads(self, x):\n","        bsz = x.shape[0]\n","        split_x = tf.reshape(x, (bsz, -1, self.num_heads, self.depth))\n","        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n","\n","        return split_x\n","\n","    def combine_heads(self, x):\n","        bsz = x.shape[0]\n","        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n","        combined_x = tf.reshape(combined_x, (bsz, -1, self.d_model))\n","\n","        return combined_x\n","\n","    \n","    def call(self, Q, K, V, mask):\n","        WQ = self.W_q(Q)\n","        WK = self.W_k(K)\n","        WV = self.W_v(V)\n","        \n","        WQ_splits = self.split_heads(WQ)\n","        WK_splits = self.split_heads(WK)\n","        WV_splits = self.split_heads(WV)\n","        \n","        out, attention_weights = self.scaled_dot_product_attention(\n","            WQ_splits, WK_splits, WV_splits, mask)\n","                        \n","        out = self.combine_heads(out)\n","        out = self.linear(out)\n","            \n","        return out, attention_weights"],"metadata":{"id":"jD9FxcUvQ3lw","executionInfo":{"status":"ok","timestamp":1666331640271,"user_tz":-540,"elapsed":3,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# Position-wise Feed Forward Network 구현\n","class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n","    def __init__(self, d_model, d_ff):\n","        super(PoswiseFeedForwardNet, self).__init__()\n","        self.d_model = d_model\n","        self.d_ff = d_ff\n","\n","        self.fc1 = tf.keras.layers.Dense(d_ff, activation='relu')\n","        self.fc2 = tf.keras.layers.Dense(d_model)\n","\n","    def call(self, x):\n","        out = self.fc1(x)\n","        out = self.fc2(out)\n","            \n","        return out"],"metadata":{"id":"O_uM98BetfHG","executionInfo":{"status":"ok","timestamp":1666331680903,"user_tz":-540,"elapsed":4,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Encoder의 레이어 구현\n","class EncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, n_heads, d_ff, dropout):\n","        super(EncoderLayer, self).__init__()\n","\n","        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n","        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n","\n","        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.do = tf.keras.layers.Dropout(dropout)\n","        \n","    def call(self, x, mask):\n","        '''\n","        Multi-Head Attention\n","        '''\n","        residual = x\n","        out = self.norm_1(x)\n","        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n","        out = self.do(out)\n","        out += residual\n","        \n","        '''\n","        Position-Wise Feed Forward Network\n","        '''\n","        residual = out\n","        out = self.norm_2(out)\n","        out = self.ffn(out)\n","        out = self.do(out)\n","        out += residual\n","        \n","        return out, enc_attn"],"metadata":{"id":"lVt_pw_Puh2Q","executionInfo":{"status":"ok","timestamp":1666331683096,"user_tz":-540,"elapsed":3,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Decoder 레이어 구현\n","class DecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, d_ff, dropout):\n","        super(DecoderLayer, self).__init__()\n","\n","        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n","        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n","\n","        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n","\n","        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.do = tf.keras.layers.Dropout(dropout)\n","    \n","    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n","        '''\n","        Masked Multi-Head Attention\n","        '''\n","        residual = x\n","        out = self.norm_1(x)\n","        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n","        out = self.do(out)\n","        out += residual\n","\n","        '''\n","        Multi-Head Attention\n","        '''\n","        residual = out\n","        out = self.norm_2(out)\n","        # Q, K, V 순서에 주의하세요!\n","        out, dec_enc_attn = self.enc_dec_attn(Q=out, K=enc_out, V=enc_out, mask=dec_enc_mask)\n","        out = self.do(out)\n","        out += residual\n","        \n","        '''\n","        Position-Wise Feed Forward Network\n","        '''\n","        residual = out\n","        out = self.norm_3(out)\n","        out = self.ffn(out)\n","        out = self.do(out)\n","        out += residual\n","\n","        return out, dec_attn, dec_enc_attn"],"metadata":{"id":"RtQ6oaLtwTlL","executionInfo":{"status":"ok","timestamp":1666331686363,"user_tz":-540,"elapsed":322,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# Encoder 구현\n","class Encoder(tf.keras.Model):\n","    def __init__(self,\n","                    n_layers,\n","                    d_model,\n","                    n_heads,\n","                    d_ff,\n","                    dropout):\n","        super(Encoder, self).__init__()\n","        self.n_layers = n_layers\n","        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n","                        for _ in range(n_layers)]\n","    \n","        self.do = tf.keras.layers.Dropout(dropout)\n","        \n","    def call(self, x, mask):\n","        out = x\n","    \n","        enc_attns = list()\n","        for i in range(self.n_layers):\n","            out, enc_attn = self.enc_layers[i](out, mask)\n","            enc_attns.append(enc_attn)\n","        \n","        return out, enc_attns"],"metadata":{"id":"EmNQ3uq1yg6g","executionInfo":{"status":"ok","timestamp":1666331689699,"user_tz":-540,"elapsed":3,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Decoder 구현\n","class Decoder(tf.keras.Model):\n","    def __init__(self,\n","                    n_layers,\n","                    d_model,\n","                    n_heads,\n","                    d_ff,\n","                    dropout):\n","        super(Decoder, self).__init__()\n","        self.n_layers = n_layers\n","        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n","                            for _ in range(n_layers)]\n","                            \n","    def call(self, x, enc_out, dec_enc_mask, padding_mask):\n","        out = x\n","    \n","        dec_attns = list()\n","        dec_enc_attns = list()\n","        for i in range(self.n_layers):\n","            out, dec_attn, dec_enc_attn = \\\n","            self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n","\n","            dec_attns.append(dec_attn)\n","            dec_enc_attns.append(dec_enc_attn)\n","\n","        return out, dec_attns, dec_enc_attns"],"metadata":{"id":"y2godUfNzkvg","executionInfo":{"status":"ok","timestamp":1666331693414,"user_tz":-540,"elapsed":3,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# 트랜스포머 전체 모델 조립\n","\n","class Transformer(tf.keras.Model):\n","    def __init__(self,\n","                    n_layers,\n","                    d_model,\n","                    n_heads,\n","                    d_ff,\n","                    src_vocab_size,\n","                    tgt_vocab_size,\n","                    pos_len,\n","                    dropout=0.2,\n","                    shared_fc=True,\n","                    shared_emb=False):\n","        super(Transformer, self).__init__()\n","        \n","        self.d_model = tf.cast(d_model, tf.float32)\n","\n","        if shared_emb:\n","            self.enc_emb = self.dec_emb = \\\n","            tf.keras.layers.Embedding(src_vocab_size, d_model)\n","        else:\n","            self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n","            self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n","\n","        self.pos_encoding = positional_encoding(pos_len, d_model)\n","        self.do = tf.keras.layers.Dropout(dropout)\n","\n","        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n","        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n","\n","        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n","\n","        self.shared_fc = shared_fc\n","\n","        if shared_fc:\n","            self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n","\n","    def embedding(self, emb, x):\n","        seq_len = x.shape[1]\n","\n","        out = emb(x)\n","\n","        if self.shared_fc: out *= tf.math.sqrt(self.d_model)\n","\n","        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n","        out = self.do(out)\n","\n","        return out\n","\n","        \n","    def call(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n","        enc_in = self.embedding(self.enc_emb, enc_in)\n","        dec_in = self.embedding(self.dec_emb, dec_in)\n","\n","        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n","        \n","        dec_out, dec_attns, dec_enc_attns = \\\n","        self.decoder(dec_in, enc_out, dec_enc_mask, dec_mask)\n","        \n","        logits = self.fc(dec_out)\n","        \n","        return logits, enc_attns, dec_attns, dec_enc_attns"],"metadata":{"id":"CZJPvAPZ0Xta","executionInfo":{"status":"ok","timestamp":1666331695039,"user_tz":-540,"elapsed":3,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# 트랜스포머 하이퍼 파라미터 지정\n","\n","transformer = Transformer(\n","    n_layers=2,\n","    d_model=512,\n","    n_heads=8,\n","    d_ff=2048,\n","    src_vocab_size=VOCAB_SIZE,\n","    tgt_vocab_size=VOCAB_SIZE,\n","    pos_len=200,\n","    dropout=0.3,\n","    shared_fc=True,\n","    shared_emb=True)\n","\t\t\n","d_model = 512"],"metadata":{"id":"VLh-b9fD0X_2","executionInfo":{"status":"ok","timestamp":1666331698646,"user_tz":-540,"elapsed":839,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# Learning Rate Scheduler 구현\n","class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps=4000):\n","        super(LearningRateScheduler, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.warmup_steps = warmup_steps\n","    \n","    def __call__(self, step):\n","        arg1 = step ** -0.5\n","        arg2 = step * (self.warmup_steps ** -1.5)\n","        \n","        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)"],"metadata":{"id":"qP6XNusC_5FG","executionInfo":{"status":"ok","timestamp":1666331700318,"user_tz":-540,"elapsed":2,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# Learning Rate 인스턴스 선언 & Optimizer 구현\n","learning_rate = LearningRateScheduler(d_model)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate,\n","                                        beta_1=0.9,\n","                                        beta_2=0.98, \n","                                        epsilon=1e-9)"],"metadata":{"id":"Eui5XsUQ__lA","executionInfo":{"status":"ok","timestamp":1666331728803,"user_tz":-540,"elapsed":2,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["# Loss Function 정의\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"],"metadata":{"id":"bJT2SJkIAFiP","executionInfo":{"status":"ok","timestamp":1666331730943,"user_tz":-540,"elapsed":2,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["# Train Step 정의\n","@tf.function()\n","def train_step(src, tgt, model, optimizer):\n","    tgt_in = tgt[:, :-1]  # Decoder의 input\n","    gold = tgt[:, 1:]     # Decoder의 output과 비교하기 위해 right shift를 통해 생성한 최종 타겟\n","\n","    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n","\n","    with tf.GradientTape() as tape:\n","        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n","        model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n","        loss = loss_function(gold, predictions)\n","\n","    gradients = tape.gradient(loss, model.trainable_variables)    \n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","    return loss, enc_attns, dec_attns, dec_enc_attns"],"metadata":{"id":"6etXkE_0AJ0A","executionInfo":{"status":"ok","timestamp":1666331733960,"user_tz":-540,"elapsed":4,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# 훈련시키기\n","\n","EPOCHS = 3\n","\n","for epoch in range(EPOCHS):\n","    total_loss = 0\n","    \n","    dataset_count = tf.data.experimental.cardinality(train_dataset).numpy()\n","    tqdm_bar = tqdm(total=dataset_count)\n","    for step, (enc_batch, dec_batch) in enumerate(train_dataset):\n","        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n","        train_step(enc_batch,\n","                    dec_batch,\n","                    transformer,\n","                    optimizer)\n","\n","        total_loss += batch_loss\n","        \n","        tqdm_bar.set_description_str('Epoch %2d' % (epoch + 1))\n","        tqdm_bar.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (step + 1)))\n","        tqdm_bar.update()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["973e4a60d50a48db88331295573a91d4","5a8674f8c81b49d0ac69e1c9e888b551","a00ea99269a84378863716c4ec177d00","3915feeca33348ef915155b3e3adf7db","d65e2e08342b4425ad01ab444d7a6c99","0ff3d76ba6fd4388b6ccfe1d5a5b7298","e8b1ec0c32b64705bf64ea21c3ea4084","80d035e6940e42f18b067653518cc8ad","874bd8a6fa3449b6831c2eee21d4c668","dcd3d88988c94cc79af5350f81892bfd","94290c6a45dd42d491cfb6c2163e024b","dda7c9d0349541b78cf11460ae82deaf","ae5d867fbbe749d1baaf75d9c5391566","52b301bf38d94285808c331615349706","0227ed00ab7443d0a68844163f5e1c17","94a0bcce5b2a47ff9b0a1afddf099eb4","474f3fd74e5d4fb59eee8f66760b83d9","f3b0220faed24b61b81f30c877fcd44f","2612672063da44fdbc39a640a0455e0f","9f221f92c5a246118bbe3eb8fcc28add","6fc7233155cd4c889c6c026168e653ed","ac7e4ed4ec9f4c208d86e6ca3bd486d4","41435f46f1c648e3b7759d3f010f3bbd","164e04d593e24b1fa516f071ac50c1e2","81ce2d18ecaf42ad8c0bf9f23b14c459","1dd76ea7d6324511a523c1a6f6925f8f","211d6e6b76434938be157828c85fabfa","43a8ce98980143558a555f13089db678","8a2c3a496c9242fcb26cd09eb1ede050","14fd1ece84db49afa9ccb5c767377cdc","bb742a1fb7f047d2a14fdd3b958010e0","5bce96a2fa3f4b1b9adb2a5186e30a29","362faaff951c41eb8f6eb3f40c37ab26"]},"id":"rNTbw5Y4ANTU","executionInfo":{"status":"ok","timestamp":1666332717051,"user_tz":-540,"elapsed":981193,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"718ffc24-dc87-4799-8b76-c0460ac382a2"},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1850 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"973e4a60d50a48db88331295573a91d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1850 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dda7c9d0349541b78cf11460ae82deaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1850 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41435f46f1c648e3b7759d3f010f3bbd"}},"metadata":{}}]},{"cell_type":"markdown","source":["# 12-4. 번역 성능 측정하기 (1) BLEU Score\n","\n"],"metadata":{"id":"1Vh7NiwNJSe_"}},{"cell_type":"markdown","source":["1. NLTK를 활용한 BLEU Score\n","\n","NLTK는 Natural Language Tool Kit 의 준말로 이름부터 자연어 처리에 큰 도움이 될 것 같은 라이브러리입니다.😃 nltk 가 BLEU Score를 지원하니 이를 활용하도록 합시다."],"metadata":{"id":"SnSF4K1KKSH9"}},{"cell_type":"code","source":["# BLEU socre\n","\n","reference = \"많 은 자연어 처리 연구자 들 이 트랜스포머 를 선호 한다\".split()\n","candidate = \"적 은 자연어 학 개발자 들 가 트랜스포머 을 선호 한다 요\".split()\n","\n","print(\"원문:\", reference)\n","print(\"번역문:\", candidate)\n","print(\"BLEU Score:\", sentence_bleu([reference], candidate))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8H-o6hJiKgIL","executionInfo":{"status":"ok","timestamp":1666332881897,"user_tz":-540,"elapsed":434,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"897dfc2b-796d-424e-980d-2aaf98520164"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["원문: ['많', '은', '자연어', '처리', '연구자', '들', '이', '트랜스포머', '를', '선호', '한다']\n","번역문: ['적', '은', '자연어', '학', '개발자', '들', '가', '트랜스포머', '을', '선호', '한다', '요']\n","BLEU Score: 8.190757052088229e-155\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"]}]},{"cell_type":"markdown","source":["BLEU Socer는 0~1 사이의 값을 가지지만, 100을 곱한 백분율 값으로 표기하는 경우도 많다. BLEU Score의 점수대별 해석은 [여기](https://cloud.google.com/translate/automl/docs/evaluate?hl=ko#bleu)를 참고\n","\n","BLEU Socre가 50점을 넘는다면 좋은 번역을 생성했다는 의미이다. 보통 논문에서 제시하는 BLEU Socre는 20점에서 높으면 40점을 바라본다. 지금 위에서 나온 점수는 0점이다. 이건 BLEU Score의 점수 측정 방식때문에 그런 것으로 추측된다.\n","\n","BLEU Score의 정의를 다시 생각해보자\n","\n","BLEU Score가 N-gram으로 점수를 측정한다는 것을 기억할 수 있다.\n","\n","아래 수식을 보면 1-gram부터 4-gram까지의 점수(Precision)를 모두 곱한 후, 루트를 두 번 씌우면(1/4) BLEU Score가 된다. 만약 번역 결과가 정말 괜찮았다면 모든 N-gram에서 좋은 점수를 얻었을 것이다.\n","\n","아래 코드에서 각 그램마다 점수가 몇 인지 확인해보자\n","\n","![](https://miro.medium.com/max/828/1*0Zi8SI4CkOMd7avkk9BTTA.png)\n","<center></center>\n","\n"],"metadata":{"id":"Vmo4LPygKz_0"}},{"cell_type":"code","source":["print(\"1-gram:\", sentence_bleu([reference], candidate, weights=[1, 0, 0, 0]))\n","print(\"2-gram:\", sentence_bleu([reference], candidate, weights=[0, 1, 0, 0]))\n","print(\"3-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 1, 0]))\n","print(\"4-gram:\", sentence_bleu([reference], candidate, weights=[0, 0, 0, 1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0d8uDIxhN_2B","executionInfo":{"status":"ok","timestamp":1666332884607,"user_tz":-540,"elapsed":4,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"7e3c2810-c633-4115-e596-ec0232dc40bd"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["1-gram: 0.5\n","2-gram: 0.18181818181818182\n","3-gram: 2.2250738585072626e-308\n","4-gram: 2.2250738585072626e-308\n"]}]},{"cell_type":"markdown","source":["3, 4gram에 점수를보면 왜 위에서 점수가 0점에 가까웠는지 알 수 있다. 하지만 만약 nltk의 낮은 버전을 사용할 경우, 간혹 이런 경우에 3-gram, 4-gram 점수가 1이 나와서, 전체적인 BLEU 점수가 50점 이상으로 매우 높게 나오게 될 수도 있습니다.\n","\n","예전 버전에서는 위 수식에서 어떤 N-gram이 0의 값을 갖는다면 그 하위 N-gram 점수들이 곱했을 때 모두 소멸해버리기 때문에 일치하는 N-gram이 없더라도 점수를 1.0 으로 유지하여 하위 점수를 보존하게끔 구현되어 있었습니다. 하지만 1.0 은 모든 번역을 완벽히 재현했음을 의미하기 때문에 총점이 의도치 않게 높아질 수 있디. 그럴 경우에는 BLEU Score가 바람직하지 못할 것(Undesirable) 이라는 경고문이 추가되긴 합니다."],"metadata":{"id":"GUpLW4G6OCDM"}},{"cell_type":"markdown","source":["2. SmoothingFunction()으로 BLEU Score 보정하기\n","\n","그래서 BLEU 계산시 특정 N-gram이 0점이 나와서 BLEU가 너무 커지거나 작아지는 쪽으로 왜곡되는 문제를 보완하기 위해 SmoothingFunction() 을 사용하고 있습니다.\n","\n","Smoothing 함수는 모든 Precision에 아주 작은 epsilon 값을 더해주는 역할을 하는데, 이로써 0점이 부여된 Precision도 완전한 0이 되지 않으니 점수를 1.0 으로 대체할 필요가 없어지죠. 즉 우리의 의도대로 점수가 계산되는 거예요.\n","\n","진실된 BLEU Score를 확인하기 위해 어서 SmoothingFunction() 을 적용해 봅시다! 아래 코드에서는 SmoothingFunction().method1을 사용해 보겠습니다. 자신만의 Smoothing 함수를 구현해서 적용할 수도 있겠지만, nltk에서는 method0부터 method7까지를 이미 제공하고 있습니다.\n","\n","- (참고) 각 method들의 상세한 설명은 [nltk의 bleu_score 소스코드](https://www.nltk.org/_modules/nltk/translate/bleu_score.html)를 참고해 봅시다. sentence_bleu() 함수에 smoothing_function=None을 적용하면 method0가 기본 적용됨을 알 수 있습니다."],"metadata":{"id":"mF3v6_WqOhsN"}},{"cell_type":"code","source":["# SmoothingFunction을 사용한 BLEU score\n","\n","def calculate_bleu(reference, candidate, weights = [0.25,0.25,0.25,0.25]):\n","  return sentence_bleu([reference],\n","                       candidate, weights =weights,\n","                       smoothing_function = SmoothingFunction().method1) # smoothing_function 적용\n","\n","print(\"BLEU-1:\", calculate_bleu(reference, candidate, weights=[1, 0, 0, 0]))\n","print(\"BLEU-2:\", calculate_bleu(reference, candidate, weights=[0, 1, 0, 0]))\n","print(\"BLEU-3:\", calculate_bleu(reference, candidate, weights=[0, 0, 1, 0]))\n","print(\"BLEU-4:\", calculate_bleu(reference, candidate, weights=[0, 0, 0, 1]))\n","\n","print(\"\\nBLEU-Total:\", calculate_bleu(reference, candidate))         "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H7o-wQv4QnNe","executionInfo":{"status":"ok","timestamp":1666332887783,"user_tz":-540,"elapsed":3,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"aa925a84-8ce9-4613-8d85-8b44ce686197"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["BLEU-1: 0.5\n","BLEU-2: 0.18181818181818182\n","BLEU-3: 0.010000000000000004\n","BLEU-4: 0.011111111111111112\n","\n","BLEU-Total: 0.05637560315259291\n"]}]},{"cell_type":"markdown","source":["아까와 비교했을때 5점이나 더 상승한 모습을 볼 수 있다.\n","\n","여기서 BLEU-4가 BLEU-3보다 조금이나마 점수가 높은 이유는 한 문장에서 발생하는 3-gram 쌍의 개수와 4-gram 쌍의 개수를 생각해 보면 이해할 수 있습니다. 각 Precision을 N-gram 개수로 나누는 부분에서 차이가 발생하는 것이죠."],"metadata":{"id":"sZtrxL4XRWdG"}},{"cell_type":"markdown","source":["3. 트랜스포머 모델의 번역 성능 알아보기\n","\n","아까 제외시켜둔 test 데이터셋을 사용하여 트랜스포머의 번역 성능을 알아보도록하자."],"metadata":{"id":"bVC2KfZWR-bY"}},{"cell_type":"code","source":["# translate 함수 구현\n","\n","def translate(tokens, model, src_tokenizer, tgt_tokenizer):\n","  padded_tokens = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n","                                                                maxlen=MAX_LEN,\n","                                                                padding = 'post')\n","  ids = []\n","  output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n","  for i in range(MAX_LEN):\n","    enc_padding_mask, combined_mask, dec_padding_mask = \\\n","    generate_masks(padded_tokens, output)\n","\n","    predictions,_,_,_ = model(padded_tokens, output, enc_padding_mask,\n","                              combined_mask, dec_padding_mask)\n","    \n","    predicted_id = tf.argmax(tf.math.softmax(predictions, axis = -1)[0, -1]).numpy().item()\n","\n","    if tgt_tokenizer.eos_id() == predicted_id:\n","      result = tgt_tokenizer.decode_ids(ids)\n","      return result\n","\n","    ids.append(predicted_id)\n","    output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis = -1)\n","\n","  result = tgt_tokenizer.decode_ids(ids)\n","  return result"],"metadata":{"id":"esnTDQccSIf6","executionInfo":{"status":"ok","timestamp":1666332893877,"user_tz":-540,"elapsed":339,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["# eval_bleu_single 구현\n","\n","def eval_bleu_single(model, src_sentence, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n","  src_tokens = src_tokenizer.encode_as_ids(src_sentence)\n","  tgt_tokens = tgt_tokenizer.encode_as_ids(tgt_sentence)\n","\n","  if (len(src_tokens) > MAX_LEN) : return None\n","  if (len(tgt_tokens) > MAX_LEN) : return None\n","\n","  reference = tgt_sentence.split()\n","  candidate = translate(src_tokens, model, src_tokenizer, tgt_tokenizer).split()\n","\n","  score = sentence_bleu([reference], candidate,\n","                        smoothing_function=SmoothingFunction().method1)\n","  \n","  if verbose:\n","    print(\"Source Sentence: \", src_sentence)\n","    print(\"Model Prediction: \", candidate)\n","    print(\"Real: \", reference)\n","    print(f'Score: {score:.2f}')\n","\n","  return score"],"metadata":{"id":"n56rehlLT9Ei","executionInfo":{"status":"ok","timestamp":1666332896378,"user_tz":-540,"elapsed":378,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# 테스트해보기\n","\n","test_idx = range(5)\n","\n","for num in test_idx:\n","  eval_bleu_single(transformer, \n","                 test_eng_sentences[num], \n","                 test_spa_sentences[num], \n","                 tokenizer, \n","                 tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JROSjXQwU9CI","executionInfo":{"status":"ok","timestamp":1666332908577,"user_tz":-540,"elapsed":9617,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"bb83a684-d41e-4037-c552-8579cbab6b95"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Source Sentence:  he never fails to call his mother on her birthday.\n","Model Prediction:  ['él', 'nunca', 'se', 'fracasara', 'a', 'su', 'madre', 'en', 'su', 'cumpleaños', 'de', 'cumpleaños', 'de', 'cumpleaños', 'de', 'cumpleaños', 'de', 'cumpleaños', 'de', 'cumpleaños', 'de', 'cumpleaños', 'regaló', 'su', 'cumpleaños', 'cumpleaños', 'cumpleaños', 'cumpleaños', 'cumpleaños', 'cumpleaños', 'cumpleaños', 'cumpleaños', 'cumpleaños', 'cumpleaños', 'cumpleaños', 'su', 'cumpleaños', 'regaló', 'su', 'cumpleaños', 'cumpleaños', 'cumpleaños', 'cumpleaños', 'regaló', 'su', 'cumpleaños', 'cumpleaños', 'cumpleaños', 'cumpleaños']\n","Real:  ['él', 'siempre', 'llama', 'a', 'su', 'madre', 'el', 'día', 'de', 'su', 'cumpleaños.']\n","Score: 0.02\n","Source Sentence:  there was fear in her eyes.\n","Model Prediction:  ['en', 'los', 'ojos', 'le', 'dan', 'ojos', 'en', 'los', 'ojos', 'de', 'los', 'ojos', 'en', 'los', 'ojos', 'en', 'los', 'ojos', 'de', 'ellas', 'estaban', 'temor', 'estaban', 'temors', 'en', 'los', 'ojos', 'en', 'los', 'ojos', 'de', 'ella', 'estaban', 'temor', 'estaban', 'temors', 'en', 'los', 'ojos', 'de', 'ella', 'estaban', 'temor', 'estaban', 'temor', 'estaban', 'temor']\n","Real:  ['había', 'miedo', 'en', 'sus', 'ojos.']\n","Score: 0.00\n","Source Sentence:  let's sit down.\n","Model Prediction:  ['siéntate', 'bajamos', 'por', 'bajados', 'bajados', 'de', 'bajaos?']\n","Real:  ['sentémonos.']\n","Score: 0.00\n","Source Sentence:  who knows that?\n","Model Prediction:  ['¿quién', 'sabe', 'eso?']\n","Real:  ['¿quién', 'lo', 'sabe?']\n","Score: 0.11\n","Source Sentence:  tom has a wide circle of friends.\n","Model Prediction:  ['tom', 'tiene', 'un', 'círculo', 'muy', 'largas', 'en', 'sus', 'amigos', 'de', 'amigos', 'míos']\n","Real:  ['tom', 'tiene', 'un', 'amplio', 'círculo', 'de', 'amigos.']\n","Score: 0.10\n"]}]},{"cell_type":"code","source":["# 전체 문장을 평가하는 eval_bleu 구현\n","def eval_bleu(model, src_sentences, tgt_sentence, src_tokenizer, tgt_tokenizer, verbose=True):\n","  total_score = 0.0\n","  sample_size = len(src_sentences)\n","\n","  for idx in tqdm(range(sample_size)):\n","    score = eval_bleu_single(model, src_sentences[idx], tgt_sentence[idx],\n","                             src_tokenizer, tgt_tokenizer, verbose)\n","    if not score: continue\n","\n","    total_score += score\n","\n","  print('Num of Sample:', sample_size)\n","  print('Total Score : ', total_score / sample_size)"],"metadata":{"id":"c_dQ7mMmW8xz","executionInfo":{"status":"ok","timestamp":1666332938691,"user_tz":-540,"elapsed":352,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["eval_bleu(transformer, test_eng_sentences, test_spa_sentences, tokenizer, tokenizer, verbose=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":85,"referenced_widgets":["6a90d04944c144368deb2e583b803920","53416c43b3e94bc6ad9081a24be697c4","94b73b6d8f944aecaf241cadf621c615","e2a0fa7d0ec64eb8bd810ecad4991c2e","2fa7a72dd3734ee0981025158729acdb","a62f0117a14e4f96bcbec0b715fbbfe5","3bfdd863ffd84173b72c4df0c20652a7","408f6cdee36745e7a40fbb6e7735af62","a5addf4eca3c4675ae4abc3cb0f76e6d","8bdfc68badd340c2a5925031daaa3937","128e8d59d26a4d30b1515f16f81dd9a9"]},"id":"lgO_BwE8YUQE","executionInfo":{"status":"ok","timestamp":1666334108377,"user_tz":-540,"elapsed":1167036,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"78f20e33-2cb4-4717-d467-e8144ac621f1"},"execution_count":41,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/594 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a90d04944c144368deb2e583b803920"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Num of Sample: 594\n","Total Score :  0.08351059642317914\n"]}]},{"cell_type":"markdown","source":["# 12-5. 번역 성능 측정하기 (2) Beam Search Decoder\n","\n"," Greedy Decoding 대신 새로운 기법을 사용하여 모델을 더 잘 평가할 수 있을것이다."],"metadata":{"id":"8adPr4AYfSm6"}},{"cell_type":"code","source":["# Bean Sarch 코드\n","\n","def beam_search_decoder(prob, beam_size):\n","    sequences = [[[], 1.0]]  # 생성된 문장과 점수를 저장\n","\n","    for tok in prob:\n","        all_candidates = []\n","\n","        for seq, score in sequences:\n","            for idx, p in enumerate(tok): # 각 단어의 확률을 총점에 누적 곱\n","                candidate = [seq + [idx], score * -math.log(-(p-1))]\n","                all_candidates.append(candidate)\n","\n","        ordered = sorted(all_candidates,\n","                         key=lambda tup:tup[1],\n","                         reverse=True) # 총점 순 정렬\n","        sequences = ordered[:beam_size] # Beam Size에 해당하는 문장만 저장 \n","\n","    return sequences"],"metadata":{"id":"Lo8VOS06fiev","executionInfo":{"status":"ok","timestamp":1666334115486,"user_tz":-540,"elapsed":338,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["vocab = {\n","    0: \"<pad>\",\n","    1: \"까요?\",\n","    2: \"커피\",\n","    3: \"마셔\",\n","    4: \"가져\",\n","    5: \"될\",\n","    6: \"를\",\n","    7: \"한\",\n","    8: \"잔\",\n","    9: \"도\",\n","}\n","\n","prob_seq = [[0.01, 0.01, 0.60, 0.32, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n","            [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.75, 0.01, 0.01, 0.17],\n","            [0.01, 0.01, 0.01, 0.35, 0.48, 0.10, 0.01, 0.01, 0.01, 0.01],\n","            [0.24, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.68],\n","            [0.01, 0.01, 0.12, 0.01, 0.01, 0.80, 0.01, 0.01, 0.01, 0.01],\n","            [0.01, 0.81, 0.01, 0.01, 0.01, 0.01, 0.11, 0.01, 0.01, 0.01],\n","            [0.70, 0.22, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n","            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n","            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01],\n","            [0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]]\n","\n","prob_seq = np.array(prob_seq)\n","beam_size = 3\n","\n","result = beam_search_decoder(prob_seq, beam_size)\n","\n","for seq, score in result:\n","    sentence = \"\"\n","\n","    for word in seq:\n","        sentence += vocab[word] + \" \"\n","\n","    print(sentence, \"// Score: %.4f\" % score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GD4qTodzgIbf","executionInfo":{"status":"ok","timestamp":1666334117668,"user_tz":-540,"elapsed":3,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"f597d4fe-0233-486f-b727-91e75865216c"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["커피 를 가져 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 42.5243\n","커피 를 마셔 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 28.0135\n","마셔 를 가져 도 될 까요? <pad> <pad> <pad> <pad>  // Score: 17.8983\n"]}]},{"cell_type":"markdown","source":["사실 이 예시는 Beam Search를 설명하는 데에는 더 없이 적당하지만 실제로 모델이 문장을 생성하는 과정과는 거리가 멀다. 당장 모델이 문장을 생성하는 과정만 떠올려도 위의 prob_seq처럼 확률을 정의할 수 없다. 각 단어에 대한 확률은 prob_seq처럼 한 번에 정의가 되지 않고 이전 스텝까지의 단어에 따라서 결정되기 때문이다.\n","\n","간단한 예시로, Beam Size가 2이고 Time-Step이 2인 순간의 두 문장이 나는 밥을, 나는 커피를 이라고 한다면 세 번째 단어로 먹는다, 마신다 를 고려할 수 있다. 이때, 전자에서 마신다에 할당하는 확률과 후자에서 마신다 에 할당하는 확률과 후자에서 마신다 에 할당하는 확률은 각각 이전 단어들인 나는 밥을, 나는 커피를 에 따라서 결정되기 때문에 서로 독립적인 확률을 갖는다. 예컨대 후자가 마신다 에 더 높을 확률을 할당할 것을 할 수 있다. 위 소스에서 처럼 3번째 단어는 항상 [마신디:0.3, 먹는다:0.5..]의 확률을 가진다고 할 수 없다.\n","\n","따라서 Beam Search를 생성 기법으로 구현할 때에는 분기를 잘 나눠줘야 합니다. Beam Size가 5라고 가정하면 맨 첫 단어로 적합한 5개의 단어를 생성하고, 두 번째 단어로 각 첫 단어(5개 단어)에 대해 5순위까지 확률을 구하여 총 25개의 문장을 생성하죠. 그 25개의 문장들은 각 단어에 할당된 확률을 곱하여 구한 점수(존재 확률) 를 가지고 있으니 각각의 순위를 매길 수 있겠다 점수 상위 5개의 표본만 살아남아 세 번째 단어를 구할 자격을 얻게 됩니다.\n","\n","위 과정을 반복하면 최종적으로 점수가 가장 높은 5개의 문장을 얻게 됩니다. 물론 Beam Size를 조절해 주면 그 수는 유동적으로 변할 수 있다."],"metadata":{"id":"LkpM3lABKxek"}},{"cell_type":"markdown","source":["1. Beam Search Decoder 작성 및 평가하기\n","\n","각 단어의 확률값을 계산하는 calc_prob()와 Beam Search를 기반으로 동작하는 beam_search_decoder() 를 구현하고 생성된 문장에 대해 BLEU Score를 출력하는 beam_bleu() 를 구현하세요!\n","\n","편의에 따라서 두 기능을 하나의 함수에 구현해도 좋습니다!\n"],"metadata":{"id":"gI6fKGoRt6H6"}},{"cell_type":"code","source":["# calc_prob() 구현\n","def calc_prob(src_ids, tgt_ids, model):\n","  enc_padding_mask, combined_mask, dec_padding_mask = generate_masks(src_ids, tgt_ids)\n","\n","  predictions, enc_attns, dec_attns, dec_enc_attns = \\\n","  model(src_ids, tgt_ids, enc_padding_mask, combined_mask, dec_padding_mask)\n","\n","  return tf.math.softmax(predictions, axis = -1)"],"metadata":{"id":"rF44oUokMQLA","executionInfo":{"status":"ok","timestamp":1666334123229,"user_tz":-540,"elapsed":2,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["# beam_search_decoder() 구현\n","\n","def beam_search_decoder(sentence, src_len, tgt_len, model, src_tokenizer, tgt_tokenizer, beam_size):\n","  tokens = src_tokenizer.encode_as_ids(sentence)\n","\n","  src_in = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n","                                                         maxlen=src_len,\n","                                                         padding='post')\n","  pred_cache = np.zeros((beam_size * beam_size, tgt_len), dtype = np.int64)\n","  pred_tmp = np.zeros((beam_size, tgt_len), dtype = np.int64)\n","\n","  eos_flag = np.zeros((beam_size,), dtype = np.int64)\n","  scores = np.ones((beam_size,))\n","\n","  pred_tmp[:, 0] = tgt_tokenizer.bos_id()\n","\n","  dec_in = tf.expand_dims(pred_tmp[0, :1], 0)\n","  prob = calc_prob(src_in, dec_in, model)[0, -1].numpy()\n","\n","  for seq_pos in range(1, tgt_len):\n","    score_cache = np.ones((beam_size * beam_size, ))\n","\n","    # init\n","    for branch_idx in range(beam_size):\n","      cache_pos = branch_idx * beam_size\n","\n","      score_cache[cache_pos:cache_pos+beam_size] = scores[branch_idx]\n","      pred_cache[cache_pos:cache_pos+beam_size, :seq_pos] = \\\n","      pred_tmp[branch_idx, :seq_pos]\n","\n","    for branch_idx in range(beam_size):\n","      cache_pos = branch_idx * beam_size\n","\n","      if seq_pos != 1: # 모든 Branch를 로 시작하는 경우를 방지\n","        dec_in = pred_cache[branch_idx, :seq_pos]\n","        dec_in = tf.expand_dims(dec_in, 0)\n","\n","        prob = calc_prob(src_in, dec_in, model)\n","\n","      for beam_idx in range(beam_size):\n","        max_idx = np.argmax(prob)\n","\n","        score_cache[cache_pos+beam_idx] *= prob[max_idx]\n","        pred_cache[cache_pos+beam_idx, seq_pos] = max_idx\n","\n","        prob[max_idx] = -1\n","\n","    for beam_idx in range(beam_size):\n","      if eos_flag[beam_idx] == -1: continue\n","\n","      max_idx = np.argmax(score_cache)\n","      prediction = pred_cache[max_idx, :seq_pos + 1]\n","\n","      pred_tmp[beam_idx, :seq_pos + 1] = prediction\n","      scores[beam_idx] = score_cache[max_idx]\n","      score_cache[max_idx] = -1\n","\n","      if prediction[-1] == tgt_tokenizer.eos_id():\n","        eos_flag[beam_idx] = -1\n","\n","  pred = []\n","  for long_pred in pred_tmp:\n","    zero_idx = long_pred.tolist().index(tgt_tokenizer.eos_id())\n","    short_pred = long_pred[:zero_idx + 1]\n","    pred.append(short_pred)\n","  \n","  return pred"],"metadata":{"id":"KBsPFt6fPtxY","executionInfo":{"status":"ok","timestamp":1666340876355,"user_tz":-540,"elapsed":2,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["def calculate_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):\n","    return sentence_bleu([reference],\n","                            candidate,\n","                            weights=weights,\n","                            smoothing_function=SmoothingFunction().method1)"],"metadata":{"id":"UU-TSF0FqAv2","executionInfo":{"status":"ok","timestamp":1666340886331,"user_tz":-540,"elapsed":750,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# beam_bleu() 구현\n","def beam_bleu(reference, ids, tokenizer):\n","    reference = reference.split()\n","\n","    total_score = 0.0\n","    for _id in ids:\n","        candidate = tokenizer.decode_ids(_id.tolist()).split()\n","        score = calculate_bleu(reference, candidate)\n","\n","        print(\"Reference:\", reference)\n","        print(\"Candidate:\", candidate)\n","        print(\"BLEU:\", calculate_bleu(reference, candidate))\n","\n","        total_score += score"],"metadata":{"id":"E_haZi9qqDcU","executionInfo":{"status":"ok","timestamp":1666340896886,"user_tz":-540,"elapsed":558,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# 인덱스를 바꿔가며 확인해 보세요\n","test_idx = 1\n","\n","ids = \\\n","beam_search_decoder(test_eng_sentences[test_idx],\n","                    MAX_LEN,\n","                    MAX_LEN,\n","                    transformer,\n","                    tokenizer,\n","                    tokenizer,\n","                    beam_size=5)\n","\n","bleu = beam_bleu(test_spa_sentences[test_idx], ids, tokenizer)\n","print(bleu)"],"metadata":{"id":"35AHvoCHqJjU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 12-6. 데이터 부풀리기\n","\n","이번 스텝에서는 Data Augmentation, 그중에서도 Embedding을 활용한 Lexical Substitution을 구현해 볼 거예요. gensim 라이브러리를 활용하면 어렵지 않게 해낼 수 있습니다.\n","\n","gensim 에 사전 훈련된 Embedding 모델을 불러오는 것은 두 가지 방법이 있습니다.\n","\n","1. 직접 모델을 다운로드해 load 하는 방법\n","\n","2. gensim 이 자체적으로 지원하는 downloader 를 활용해 모델을 load 하는 방법\n","\n","한국어는 gensim 에서 지원하지 않으므로 두 번째 방법을 사용할 수 없지만, 영어라면 얘기가 달라지죠! 아래 웹페이지의 Available data → Model 부분에서 공개된 모델의 종류를 확인할 수 있습니다.\n","\n","[RaRe-Technologies/gensim-data](https://github.com/RaRe-Technologies/gensim-data)\n","\n","대표적으로 사용되는 Embedding 모델은 word2vec-google-news-300 이지만 용량이 커서 다운로드에 많은 시간이 소요되므로 이번 실습엔 적합하지 않습니다. 우리는 적당한 사이즈의 모델인 glove-wiki-gigaword-300 을 사용할게요! 아래 소스를 실행해 사전 훈련된 Embedding 모델을 다운로드해 주세요.\n"],"metadata":{"id":"czHp3boWqaeI"}},{"cell_type":"code","source":["import gensim.downloader as api\n","\n","wv = api.load('glove-wiki-gigaword-300')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HlVqHgmwq_eO","executionInfo":{"status":"ok","timestamp":1666341321255,"user_tz":-540,"elapsed":177185,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"4d0a2106-462f-42f6-9eef-25d04b8a3176"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 376.1/376.1MB downloaded\n"]}]},{"cell_type":"markdown","source":["불러온 모델은 아래와 같이 활용할 수 있습니다."],"metadata":{"id":"SmPaPDleraqx"}},{"cell_type":"code","source":["wv.most_similar(\"banana\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j33on-MzraJF","executionInfo":{"status":"ok","timestamp":1666341325328,"user_tz":-540,"elapsed":848,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"c70a1671-677e-49b7-c787-4d9aa747e55d"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('bananas', 0.6691170930862427),\n"," ('mango', 0.580410361289978),\n"," ('pineapple', 0.5492371916770935),\n"," ('coconut', 0.5462779402732849),\n"," ('papaya', 0.541056752204895),\n"," ('fruit', 0.5218108296394348),\n"," ('growers', 0.4877638816833496),\n"," ('nut', 0.4839959144592285),\n"," ('peanut', 0.48062020540237427),\n"," ('potato', 0.4806118607521057)]"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["주어진 데이터를 토큰 단위로 분리한 후, 랜덤하게 하나를 선정하여 해당 토큰과 가장 유사한 단어를 찾아 대치하면 그것으로 Lexical Substitution은 완성되겠죠? 가볍게 확인해 봅시다!\n","\n"],"metadata":{"id":"Nc1aU4LhreIr"}},{"cell_type":"code","source":["sample_sentence = 'you know ? all you need is attention .'\n","sample_tokens = sample_sentence.split()\n","\n","selected_tok = random.choice(sample_tokens)\n","\n","result = ''\n","\n","for tok in sample_tokens:\n","  if tok is selected_tok:\n","    result += wv.most_similar(tok)[0][0] + ' '\n","  else:\n","    result += tok + ' '\n","\n","print(\"From:\", sample_sentence)\n","print(\"To:\", result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lu8QQuTUrdUt","executionInfo":{"status":"ok","timestamp":1666341523522,"user_tz":-540,"elapsed":320,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"7744b873-6b90-4970-e1b1-a48423cd3a85"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["From: you know ? all you need is attention .\n","To: you think ? all you need is attention . \n"]}]},{"cell_type":"markdown","source":["1. Lexical Substitution 구현하기\n","- 입력된 문장을 Embedding 유사도를 기반으로 Augmentation 하여 반환하는 lexical_sub() 를 구현하세요!"],"metadata":{"id":"85mHCDfdsd-B"}},{"cell_type":"code","source":["# Lexical Substitution 구현\n","\n","def lexical_sub(sentence, word2vec):\n","  res = ''\n","  toks = sentence.split()\n","\n","  try:\n","    _from = random.choice(toks)\n","    _to = word2vec.most_similar(_from)[0][0]\n","\n","  except: # 단어장에 없는 단어\n","    return None\n","  \n","  for tok in toks:\n","    if tok is _from: res += _to + ' '\n","    else: res += tok + ' '\n","    \n","  return res"],"metadata":{"id":"kSUhVF-7sg0X","executionInfo":{"status":"ok","timestamp":1666342101678,"user_tz":-540,"elapsed":349,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["new_corpus = []\n","\n","for old_src in tqdm(test_eng_sentences):\n","  new_src = lexical_sub(old_src, wv)\n","  if new_src is not None:\n","    new_corpus.append(new_src)\n","  # Augmentation이 없더라도 원본 문장을 포함시킵니다\n","  new_corpus.append(old_src)\n","\n","print(new_corpus[:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"T2e2Fr9Kuwx1","executionInfo":{"status":"error","timestamp":1666342309387,"user_tz":-540,"elapsed":5,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"6b3c5303-c9d8-4bd0-ef5f-4211ef039889"},"execution_count":13,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-35e842796b2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnew_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mold_src\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_eng_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mnew_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlexical_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mnew_src\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'test_eng_sentences' is not defined"]}]}]}